{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.MT-GANBERT_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOjVWAIwCpdvtwkJZIFRVyt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crux82/mt-ganbert/blob/main/3_MT_GANBERT_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kROPcbt9WhWe"
      },
      "source": [
        "# MT-GANBERT model in Pytorch\n",
        "This notebook shows how to train a model that combines Multi-task learning and Generative Adversarial learning. The model is based on the transformer, Italian Bert-base model, UmBERTo (https://github.com/musixmatchresearch/umberto), and the MT-GANBERT model is trained at the same time on the six tasks considered in our work,used for the recognition of abusive linguistic behaviors. The task are:\n",
        "\n",
        "1.   HaSpeeDe: Hate Spech Recognition\n",
        "2.   AMI A: Automatic Misogyny Identification (misogyny, not mysogyny)\n",
        "3.   AMI B: Automatic Misogyny Identification (misogyny_category: stereotype, sexual_harassment, discredit)\n",
        "4.   DANKMEMEs: Hate Spech Recognition in MEMEs sentences\n",
        "5.   SENTIPOLC 1: Sentiment Polarity Classification (objective, subjective)\n",
        "6.   SENTIPOLC 2: Sentiment Polarity Classification (polarity: positive, negative, neutral)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDSXwj8-3jQt"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZs976xHuMtV"
      },
      "source": [
        "#--------------------------------\n",
        "#  Retrieve the github directory\n",
        "#--------------------------------\n",
        "!git clone https://github.com/crux82/mt-ganbert\n",
        "%cd mt-ganbert/mttransformer/\n",
        "\n",
        "#installation of necessary packages\n",
        "!pip install -r requirements.txt\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install ekphrasis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRzKsvle3oWc"
      },
      "source": [
        "##Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LULw5oINt9KW"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aS__2ilfRum"
      },
      "source": [
        "## Run training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9TaSrUauaQt"
      },
      "source": [
        "For each dataset, with a dedicated script (\"script_tsv.py\"), are created 4 files:\n",
        "\n",
        "1.   taskName_task_def.yml, a config file about the task\n",
        "2.   taskName_train.tsv, file tsv of task train set \n",
        "3.   taskName_test.tsv, file tsv of task test set \n",
        "4.   taskName_dev.tsv, file tsv of task dev set \n",
        "\n",
        "\n",
        "The number of examples of train can consist of:\n",
        "\n",
        "*   All train dataset\n",
        "*   100 examples of oringinal train dataset\n",
        "*   200 examples of oringinal train dataset\n",
        "*   500 examples of oringinal train dataset\n",
        "\n",
        "\n",
        "To access to the .tsv files and config file of each task, based on the cutting of examples of the train set you want to use, these can be the paths:\n",
        "\n",
        "*   data/0/taskName_file\n",
        "*   data/100/gan/taskName_file\n",
        "*   data/200/gan/taskName_file\n",
        "*   data/500/gan/taskName_file\n",
        "\n",
        "\"gan\" means that you want to use the model that consists of GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfGXddUX2A0U"
      },
      "source": [
        "###**Tokenization and Convert to Json**\n",
        "\n",
        "The training code reads tokenized data in json format, so \"prepro_std.py\" (modified script of work https://github.com/namisan/mt-dnn) is used to do tokenization and convert data of .tsv files into json format.\n",
        "\n",
        "The args used in the script invocation are:\n",
        "\n",
        "* --gan: it's a flag which means we want to use a model that contains adversarial learning, in this case the model is MT-GANBERT\n",
        "* --apply_balance: it's a flag which means that we want activate the balancing that the GANBERT performs\n",
        "* --model: the model used to tokenize input sentences\n",
        "* --root_dir: the folder from which to get the .tsv files\n",
        "* --task_def: the task_def file of the task, which contains useful information for converting to .json files. In this case the task_def file contains the information about all tasks\n",
        "\n",
        "The script is run for all tasks simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktE5q93Y18UV"
      },
      "source": [
        "#edit --root_dir and --task_def depending on the task and train set\n",
        "!python prepro_std.py --gan --apply_balance --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir data/\"100\"/gan/ --task_def data/100/gan/haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162_task_def.yml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3RCD0vM2MH4"
      },
      "source": [
        "###**Onboard your task into training!**\n",
        "\n",
        "To run the training is used the script \"train.py\" (modified script of work https://github.com/namisan/mt-dnn).\n",
        "The args used in the script invocation are:\n",
        "\n",
        "*   --gan: it's a flag which means we want to use a model that contains adversarial learning\n",
        "*   --noise_size: the size of the noise that the vectors in input to the Generator have. In our work is 100 \n",
        "*   --epsilon: epsilon for the training of Generator and Discriminator\n",
        "*   --encoder_type: it means which transformer is used to encode the sentences. In this case is equals to \"9\", that matches to UmBERTo\n",
        "*   --epochs: number of epochs that you want to use in the training\n",
        "*   --task_def: the task_def file of the tasks\n",
        "*   --data_dir: the folder from which to get the .json files\n",
        "*   --init_checkpoint: the name of the transformer to be loaded, in this case \"Musixmatch/umberto-commoncrawl-cased-v1\"\n",
        "*   --max_seq_len: the maximum length of a sequence that the BERT model can handle\n",
        "*   --batch_size: the number of training examples in one forward/backward pass\n",
        "*   --batch_size_eval: the batch size used for validation and test\n",
        "*   --optimizer: the name of optimizer that you want to use\n",
        "*   --train_datasets: the name of the tasks without train files extension, separated by \",\"\n",
        "*   --test_datasets: the name of the tasks without test files extension, , separated by \",\"\n",
        "*   --learning_rate: the learning rate that you want to use\n",
        "*   --multi_gpu_on: since the model is trained on multiple tasks at the same time, it is possible to train with multiple GPUs, to have a more timely training\n",
        "*   --grad_accumulation_step: you may need to use the gradient accumulation to make training stable, when you small GPUs. For example, if you use the flag \"--grad_accumulation_step 4\" during the training, the actual batch size will be batch_size * 4\n",
        "\n",
        "The script is run for all tasks simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb4mcYjw2TyA"
      },
      "source": [
        "#edit --task_def, --data_dir, --train_datasets and test_datasets  depending on the task and train set\n",
        "!python train.py --gan --noise_size 100 --epsilon 1e-8 --encoder_type 9 --epochs 25 --task_def data/100/gan/haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162_task_def.yml --data_dir data/100/gan/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 64 --batch_size_eval 64 --optimizer \"adamW\" --train_datasets haspeede-TW,AMI2018A,AMI2018B,DANKMEMES2020,SENTIPOLC20161,SENTIPOLC20162 --test_datasets haspeede-TW,AMI2018A,AMI2018B,DANKMEMES2020,SENTIPOLC20161,SENTIPOLC20162 --learning_rate \"1e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}