{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Multi-task GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Be-vdjMgVZ-Q",
        "qqKeShzDD-_t",
        "-8i48m5b3r4-",
        "qZ0_9xuB7bdu",
        "f4WGayCY9uJv",
        "trpS4S-dAGUA",
        "AMfiRst0cP6a",
        "R-qbdtFl1Ko1",
        "tBOPQSIeOeDI"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTW-xASHXVQT"
      },
      "source": [
        "# Tutorial for Run Your Own Tasks in MT-GAN\n",
        "This notebook shows how to run MT-GAN with the example tasks present in the git repository, used for the recognition of abusive linguistic behaviors.\n",
        "\n",
        "To compare the performance of the MT-DNN model, the following models are present in this notebbok:\n",
        "\n",
        "1. A model based only on the Transformer BERT (BERT-based model)\n",
        "2. A model based on the Transformer BERT (BERT-based model) and characterized by Semi-Supervised Adversarial Learning (SS-GAN), called GANBERT\n",
        "3. The MT-DNN model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeqpD3RgYBv6"
      },
      "source": [
        "## Import and device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRHzHChwKJ3L",
        "outputId": "263a311c-5564-4ca5-c6c0-234e606af272"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQdg9pLKYWm3"
      },
      "source": [
        "## Clone repository git and install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hq1HA4tHXFM3",
        "outputId": "846ba80c-c1b4-4857-fb7d-86dce284ab9a"
      },
      "source": [
        "!git clone http://breazzano:Tr4nsf0rm3r!@gitlab.revealsrl.it/croce/mttransformer.git\n",
        "%cd mttransformer/\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install ekphrasis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mttransformer'...\n",
            "remote: Counting objects: 270, done.\u001b[K\n",
            "remote: Compressing objects: 100% (266/266), done.\u001b[K\n",
            "remote: Total 270 (delta 131), reused 0 (delta 0)\u001b[K\n",
            "Receiving objects: 100% (270/270), 1.30 MiB | 1.56 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n",
            "/content/mttransformer\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.41.1)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-5.0.1-py2.py3-none-any.whl (10 kB)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.18.6-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 47.8 MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert==v0.6.0\n",
            "  Downloading pytorch_pretrained_bert-0.6.0-py3-none-any.whl (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (2019.12.20)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.22.2.post1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.13)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (3.6.4)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (2.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (0.16.0)\n",
            "Collecting seqeval==0.0.12\n",
            "  Downloading seqeval-0.0.12.tar.gz (21 kB)\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 66.0 MB/s \n",
            "\u001b[?25hCollecting transformers==4.1.1\n",
            "  Downloading transformers-4.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (2.23.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1->-r requirements.txt (line 2)) (2.11.3)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (1.9.0+cu102)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12->-r requirements.txt (line 15)) (2.4.3)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 74.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.1->-r requirements.txt (line 17)) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.1->-r requirements.txt (line 17)) (3.0.12)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 61.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras>=2.2.4->seqeval==0.0.12->-r requirements.txt (line 15)) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.2.4->seqeval==0.0.12->-r requirements.txt (line 15)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (3.7.4.3)\n",
            "Collecting botocore<1.22.0,>=1.21.6\n",
            "  Downloading botocore-1.21.6-py3-none-any.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 61.2 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.9 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.6->boto3->-r requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.6->boto3->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (21.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (57.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (8.8.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 10)) (1.10.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r requirements.txt (line 12)) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 13)) (1.34.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 13)) (1.32.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 13)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 13)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 13)) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 13)) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 13)) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 13)) (3.3.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 13)) (0.12.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 18)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 18)) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 18)) (3.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 13)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 13)) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 13)) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 13)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 13)) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 13)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 13)) (3.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->Keras>=2.2.4->seqeval==0.0.12->-r requirements.txt (line 15)) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->-r requirements.txt (line 13)) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1->-r requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.1.1->-r requirements.txt (line 17)) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.1.1->-r requirements.txt (line 17)) (7.1.2)\n",
            "Building wheels for collected packages: folium, seqeval\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=62bd31e3686b965be7abc41d323d78341b39e1fff38b8e9c5f77fbae487ae2db\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7434 sha256=fba315f1bd6b39555ad59bb82d3b29be2936f92c40df07eaf8c5397dd3a229a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/cc/62/a3b81f92d35a80e39eb9b2a9d8b31abac54c02b21b2d466edc\n",
            "Successfully built folium seqeval\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, tokenizers, sacremoses, boto3, transformers, tensorboardX, seqeval, sentencepiece, pytorch-pretrained-bert, folium, colorlog\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed boto3-1.18.6 botocore-1.21.6 colorlog-5.0.1 folium-0.2.1 jmespath-0.10.0 pytorch-pretrained-bert-0.6.0 s3transfer-0.5.0 sacremoses-0.0.45 sentencepiece-0.1.96 seqeval-0.0.12 tensorboardX-2.4 tokenizers-0.9.4 transformers-4.1.1 urllib3-1.25.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4 MB 11 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 76 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu101) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.1+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting ekphrasis\n",
            "  Downloading ekphrasis-0.5.1.tar.gz (80 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 20 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 30 kB 37.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 40 kB 39.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 51 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 61 kB 38.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 71 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 80 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting ujson\n",
            "  Downloading ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179 kB)\n",
            "\u001b[K     |████████████████████████████████| 179 kB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->ekphrasis) (1.15.0)\n",
            "Building wheels for collected packages: ekphrasis, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-py3-none-any.whl size=82843 sha256=7d07f2834c2fdfd80f3b6ca3144e4dfe3d10fcca481d4986107ce64932ee6263\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/ec/0d/12659e32faf780546945d0120f2c8410eb3efb7426731da88f\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41934 sha256=46964d4b91bca5c2e7148a7f18a824c6c344e0a2dbc60da832b1d2aaa842122e\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built ekphrasis ftfy\n",
            "Installing collected packages: ujson, ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.4 ekphrasis-0.5.1 ftfy-6.0.3 ujson-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwCwMxMfc8vH"
      },
      "source": [
        "## Set the type of model you are using and the amount of data to train the model:\n",
        "\n",
        "\n",
        "> Single-task model:\n",
        "\n",
        "\n",
        "    1. If apply_gan = False & number_labeled_examples = 0, I want to train the BERT-based model with the full amount of data available for the i-th task\n",
        "    2. If apply_gan = False & number_labeled_examples = 100 (or 200 or 500), I want to train the BERT-based model (because the model does not contain GAN) with a \"reduction\" of the i-th task data\n",
        "    3. If apply_gan = True & number_labeled_examples = 100 (or 200 or 500), I want to train the GANBERT model with the amount of data in the number_labeled_examples, labeled, of the i-th task and the rest of the dataset not labeled\n",
        "\n",
        "> Model trained simultaneously on tasks (MT model):\n",
        "\n",
        "    1. If apply_gan = False & number_labeled_examples = 0, I want to train the MT-DNN model with the full amount of data available to each task\n",
        "    2. If apply_gan = False & number_labeled_examples = 100 (or 200 or 500), I want to train the MT-DNN model with a \"reduction\" of the data of each task\n",
        "    3. If apply_gan = True & number_labeled_examples = 100 (or 200 or 500), I want to train the MT-GAN model with the amount of data in number_labeled_examples, tagged and the remainder of the untagged dataset, of each task\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZCVi3rqXYfR"
      },
      "source": [
        "apply_gan=False\n",
        "number_labeled_examples=200 #0-100-200-500\n",
        "file_loaded=False\n",
        "file_loaded2=False\n",
        "file_loaded3=False\n",
        "file_loaded4=False\n",
        "file_loaded5=False\n",
        "file_loaded6=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFB_aAQle_Kk"
      },
      "source": [
        "## **Single-Task model**\n",
        "\n",
        "\n",
        "> BERT-based model\n",
        "\n",
        "\n",
        "> *GANBERT*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Each sub-block consists of training the chosen model with the Abusive Recognition task dataset. The tasks are:\n",
        "\n",
        "\n",
        "1.   HaSpeeDe: Hate Spech Recognition\n",
        "2.   AMI A: Automatic Misogyny Identification (misogyny, not mysogyny)\n",
        "3.   AMI B: Automatic Misogyny Identification (misogyny_category: stereotype, sexual_harassment, discredit)\n",
        "4.   DANKMEMEs: Hate Spech Recognition in MEMEs sentences\n",
        "5.   SENTIPOLC 1: Sentiment Polarity Classification (objective, subjective)\n",
        "6.   SENTIPOLC 2: Sentiment Polarity Classification (polarity: positive, negative, neutral)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be-vdjMgVZ-Q"
      },
      "source": [
        "### Task HaSpeeDe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvqEs7rsBt-6",
        "outputId": "9eeb5b6e-0ee0-4a87-a734-aa622e82b9f9"
      },
      "source": [
        "%cd tsv_files/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'tsv_files/'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmDRA336R7B1"
      },
      "source": [
        "Upload the dataset as dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "MbanCieQUlB9",
        "outputId": "44278236-20f1-4a21-c2c1-714183f9b56a"
      },
      "source": [
        "file_loaded=True\n",
        "\n",
        "tsv_haspeede_train = 'haspeede_TW-train.tsv'\n",
        "tsv_haspeede_test = 'haspeede_TW-reference.tsv'\n",
        "\n",
        "df_train = pd.read_csv(tsv_haspeede_train, delimiter='\\t', names=('id','sentence','label'))\n",
        "df_train = df_train[['id']+['label']+['sentence']]\n",
        "df_test = pd.read_csv(tsv_haspeede_test, delimiter='\\t', names=('id','sentence','label'))\n",
        "df_test = df_test[['id']+['label']+['sentence']]\n",
        "\n",
        "#split train dev\n",
        "train_dataset, dev_dataset = train_test_split(df_train, test_size=0.2, shuffle = True)\n",
        "\n",
        "#reduction\n",
        "if number_labeled_examples!=0:\n",
        "  if number_labeled_examples==100:\n",
        "    labeled = train_dataset.sample(n=100)\n",
        "    unlabeled = train_dataset\n",
        "    cond = unlabeled['id'].isin(labeled['id'])\n",
        "    unlabeled.drop(unlabeled[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==200:\n",
        "    labeled = train_dataset.sample(n=200)\n",
        "    unlabeled = train_dataset\n",
        "    cond = unlabeled['id'].isin(labeled['id'])\n",
        "    unlabeled.drop(unlabeled[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==500:\n",
        "    labeled = train_dataset.sample(n=500)\n",
        "    unlabeled = train_dataset\n",
        "    cond = unlabeled['id'].isin(labeled['id'])\n",
        "    unlabeled.drop(unlabeled[cond].index, inplace = True)\n",
        "\n",
        "  #model with or without gan \n",
        "  if apply_gan == True:\n",
        "    print(\"GANBERT\")\n",
        "    #dataset unlabeled with label -1\n",
        "    unlabeled['label'] = unlabeled['label'].replace(0,-1)\n",
        "    unlabeled['label'] = unlabeled['label'].replace(1,-1)\n",
        "    train = pd.concat([labeled, unlabeled])\n",
        "    dev = dev_dataset\n",
        "    print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train),len(labeled), len(unlabeled)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev)))\n",
        "  else:\n",
        "    print(\"BERT-based model, with reduction dataset\")\n",
        "    train = labeled\n",
        "    dev = dev_dataset\n",
        "    print(\"Size of Train dataset is {} \".format(len(labeled)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev)))\n",
        "\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  train = train_dataset\n",
        "  dev = dev_dataset\n",
        "  print(\"Size of Train dataset is {} \".format(len(train)))\n",
        "  print(\"Size of Dev dataset is {} \".format(len(dev)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7a79c1ef92c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtsv_haspeede_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'haspeede_TW-reference.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsv_haspeede_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsv_haspeede_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV8X__UiXwl7"
      },
      "source": [
        "!mkdir tsv_transformed\n",
        "%cd tsv_transformed/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEge0L5DfLhx"
      },
      "source": [
        "The code is using surfix to distinguish what type of set it is (\"_train\",\"_dev\" and \"_test\"). So:\n",
        "1.   make sure your train set is named as \"TASK_train\" (replace TASK with your task name)\n",
        "\n",
        "2.   make sure your dev set and test set ends with \"_dev\" and \"_test\".\n",
        "3.   add your task into task define config (task_def file):\n",
        "\n",
        "  Here is a piece of example task define config :\n",
        "  <pre>haspeede-TW:\n",
        "    data_format: PremiseOnly\n",
        "    ensable_san: false\n",
        "    labels:\n",
        "    - contradiction\n",
        "    - neutral\n",
        "    - entailment\n",
        "    metric_meta:\n",
        "    - ACC\n",
        "    loss: CeCriterion\n",
        "    n_class: 3\n",
        "    task_type: Classification</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXmFDHRlgUFi"
      },
      "source": [
        "Choose the correct data format based on your task, in this notebook are used 2 types of data formats, coresponds to different tasks:\n",
        "  1. \"PremiseOnly\" : single text, i.e. premise. Data format is \"id\" \\t \"label\" \\t \"premise\" .\n",
        "  2. \"Gan\" : single text, i.e. premise. Data format is \"id\" \\t \"label\" \\t \"premise\" .\n",
        "\n",
        "ensable_san: Set \"true\" if you would like to use Stochastic Answer Networks(SAN) for your task.\n",
        "\n",
        "If you prefer using readable labels (text), you can specify what labels are there in your data set, under \"labels\" field.\n",
        "\n",
        "More details about metrics,please refer to [data_utils/metrics.py](../data_utils/metrics.py);\n",
        "  \n",
        "You can choose loss (for BERT-based model and MT-DNN, the GANBERT loss is in the model), from pre-defined losses in file [mt_dnn/loss.py](../mt_dnn/loss.py), and you can implement your customized losses into this file and specify it in the task config.\n",
        "\n",
        "Specify what task type it is in your own task, choose one from types in:\n",
        "    1. Classification\n",
        "    2. Regression\n",
        "    3. Ranking\n",
        "    4. Span\n",
        "    5. SeqenceLabeling\n",
        "    6. MaskLM\n",
        "  More details in [data_utils/task_def.py](../data_utils/task_def.py)\n",
        "  \n",
        "Also, specify how many classes in total in your task, under \"n_class\" field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxvZbuNAVoue"
      },
      "source": [
        "#train\n",
        "name_train = \"haspeede-TW_train.tsv\"\n",
        "id_train = train.id \n",
        "label_train = train.label\n",
        "sentence_train = train.sentence\n",
        "\n",
        "#dev\n",
        "name_dev = \"haspeede-TW_dev.tsv\"\n",
        "id_dev = dev.id\n",
        "label_dev = dev.label\n",
        "sentence_dev = dev.sentence\n",
        "\n",
        "#test\n",
        "name_test = \"haspeede-TW_test.tsv\"\n",
        "id_test = df_test.id\n",
        "label_test = df_test.label\n",
        "sentence_test = df_test.sentence\n",
        "\n",
        "#task_def\n",
        "name_file = 'haspeede-TW_task_def.yml'\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))\n",
        "\n",
        "task = \"haspeede-TW:\\n\"\n",
        "\n",
        "f = open(name_file, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    f.write(task)\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\")\n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 2\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mSHpzDoX6K-"
      },
      "source": [
        "%cd ..\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZMLmIonixHg"
      },
      "source": [
        "**Tokenization and Convert to Json**\n",
        "\n",
        "The training code reads tokenized data in json format. please use \"prepro_std.py\" to do tokenization and convert your data into json format. The tokenization can be of two types:\n",
        "\n",
        "\n",
        "*   For GANBERT, applying the balance between labeled and unlabeled data\n",
        "*   For BERT-based model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM5jy7-qWNfB"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python prepro_std.py --gan --apply_balance --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/haspeede-TW_task_def.yml\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python prepro_std.py --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/haspeede-TW_task_def.yml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1efemo3jz-N"
      },
      "source": [
        "**Onboard your task into training!**\n",
        "\n",
        "Add your piece of config into overall config for task. Again, we distinguish between:\n",
        "\n",
        "*   For GANBERT, in which the number of layers of the Discriminator and Generator, the size of the noise vector and the epsilon are specified\n",
        "*   For BERT-based model\n",
        "\n",
        "--encoder_type 9: it means which BERT is used to encode the sentences. In this case Umberto is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDufGLgLAEv6"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python train.py --gan --num_hidden_layers_g 3 --num_hidden_layers_d 0 --noise_size 100 --epsilon 1e-8 --encoder_type 9 --epochs 25 --task_def tsv_files/tsv_transformed/haspeede-TW_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 64 --batch_size_eval 64 --optimizer \"adamW\" --train_datasets haspeede-TW --test_datasets haspeede-TW --learning_rate \"1e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python train.py --encoder_type 9 --epochs 10 --task_def tsv_files/tsv_transformed/haspeede-TW_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 16 --batch_size_eval 16 --optimizer \"adamW\" --train_datasets haspeede-TW --test_datasets haspeede-TW --learning_rate \"5e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqKeShzDD-_t"
      },
      "source": [
        "### Task AMI A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MESPYayB8k6",
        "outputId": "1d1f9f89-0b34-4219-d3f5-a22df7418b8a"
      },
      "source": [
        "%cd tsv_files/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYvh6L_5EGLc",
        "outputId": "fadb67e9-3b77-4697-9240-a6f4e6775f07"
      },
      "source": [
        "file_loaded2=True\n",
        "\n",
        "tsv_AMI2018_train = 'AMI2018_it_training.tsv'\n",
        "tsv_AMI2018_test = 'AMI2018_it_testing.tsv'\n",
        "\n",
        "df_train2 = pd.read_csv(tsv_AMI2018_train, delimiter='\\t')\n",
        "df_train2 = df_train2[['id']+['misogynous']+['text']]\n",
        "df_test2 = pd.read_csv(tsv_AMI2018_test, delimiter='\\t')\n",
        "df_test2 = df_test2[['id']+['misogynous']+['text']]\n",
        "\n",
        "#split train dev\n",
        "train_dataset2, dev_dataset2 = train_test_split(df_train2, test_size=0.2, shuffle = True)\n",
        "\n",
        "#reduction\n",
        "if number_labeled_examples!=0:\n",
        "  if number_labeled_examples==100:\n",
        "      labeled2 = train_dataset2.sample(n=100)\n",
        "      unlabeled2 = train_dataset2\n",
        "      cond = unlabeled2['id'].isin(labeled2['id'])\n",
        "      unlabeled2.drop(unlabeled2[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==200:\n",
        "    labeled2 = train_dataset2.sample(n=200)\n",
        "    unlabeled2 = train_dataset2\n",
        "    cond = unlabeled2['id'].isin(labeled2['id'])\n",
        "    unlabeled2.drop(unlabeled2[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==500:\n",
        "    labeled2 = train_dataset2.sample(n=500)\n",
        "    unlabeled2 = train_dataset2\n",
        "    cond = unlabeled2['id'].isin(labeled2['id'])\n",
        "    unlabeled2.drop(unlabeled2[cond].index, inplace = True)\n",
        "  \n",
        "  #model with or without gan \n",
        "  if apply_gan == True:\n",
        "    print(\"GANBERT\")\n",
        "    #dataset unlabeled with label -1\n",
        "    unlabeled2['misogynous'] = unlabeled2['misogynous'].replace(0,-1)\n",
        "    unlabeled2['misogynous'] = unlabeled2['misogynous'].replace(1,-1)\n",
        "    train2 = pd.concat([labeled2, unlabeled2])\n",
        "    dev2 = dev_dataset2\n",
        "    print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train2),len(labeled2), len(unlabeled2)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev2)))\n",
        "  else:\n",
        "    print(\"BERT-based model, with reduction dataset\")\n",
        "    train2 = labeled2\n",
        "    dev2 = dev_dataset2\n",
        "    print(\"Size of Train dataset is {} \".format(len(labeled2)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev2)))\n",
        "\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  train2 = train_dataset2\n",
        "  dev2 = dev_dataset2\n",
        "  print(\"Size of Train dataset is {} \".format(len(train2)))\n",
        "  print(\"Size of Dev dataset is {} \".format(len(dev2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model, with reduction dataset\n",
            "Size of Train dataset is 200 \n",
            "Size of Dev dataset is 800 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUbsM6oZFeP4",
        "outputId": "17650876-8e3e-444d-8205-b7b3fa62d9a0"
      },
      "source": [
        "!mkdir tsv_transformed\n",
        "%cd tsv_transformed/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘tsv_transformed’: File exists\n",
            "/content/mttransformer/tsv_files/tsv_transformed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq4uRyF4Fluo"
      },
      "source": [
        "#train\n",
        "name_train = \"AMI2018A_train.tsv\"\n",
        "id_train = train2.id\n",
        "label_train = train2.misogynous\n",
        "sentence_train = train2.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"AMI2018A_dev.tsv\"\n",
        "id_dev = dev2.id\n",
        "label_dev = dev2.misogynous\n",
        "sentence_dev = dev2.text\n",
        "\n",
        "#test\n",
        "name_test = \"AMI2018A_test.tsv\"\n",
        "id_test = df_test2.id\n",
        "label_test = df_test2.misogynous\n",
        "sentence_test = df_test2.text\n",
        "\n",
        "#task_def\n",
        "name_file = 'AMI2018A_task_def.yml'\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))\n",
        "\n",
        "\n",
        "task = \"AMI2018A:\\n\"\n",
        "  \n",
        "f = open(name_file, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    f.write(task)\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\")\n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 2\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBgTAkIhF2bZ",
        "outputId": "a1eef39d-ff15-49ea-a508-7bd225525821"
      },
      "source": [
        "%cd ..\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n",
            "/content/mttransformer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GItxf9xYF6zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d8b8be-14d5-4cb5-af7e-fa2a7c13da0b"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python prepro_std.py --gan --apply_balance --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/AMI2018A_task_def.yml\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python prepro_std.py --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/AMI2018A_task_def.yml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "2021-07-26 08:51:15.632842: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading english - 1grams ...\n",
            "Reading english - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "07/26/2021 08:51:27 Task AMI2018A\n",
            "07/26/2021 08:51:27 tsv_files/tsv_transformed/musixmatch_cased/AMI2018A_train.json\n",
            "07/26/2021 08:51:27 tsv_files/tsv_transformed/musixmatch_cased/AMI2018A_dev.json\n",
            "07/26/2021 08:51:27 tsv_files/tsv_transformed/musixmatch_cased/AMI2018A_test.json\n",
            "\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbBTkiI_GD2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770228fb-c1fc-4e6a-aff1-ff44774a169a"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python train.py --gan --num_hidden_layers_d 0 --num_hidden_layers_g 3 --noise_size 100 --epsilon 1e-8 --encoder_type 9 --epochs 25 --task_def tsv_files/tsv_transformed/AMI2018A_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 64 --batch_size_eval 64 --optimizer \"adamW\" --train_datasets AMI2018A --test_datasets AMI2018A --learning_rate \"1e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python train.py --encoder_type 9 --epochs 10 --task_def tsv_files/tsv_transformed/AMI2018A_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 16 --batch_size_eval 16 --optimizer \"adamW\" --train_datasets AMI2018A --test_datasets AMI2018A --learning_rate \"5e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model\n",
            "2021-07-26 08:51:29.881740: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "07/26/2021 08:51:32 Launching the MT-DNN training\n",
            "07/26/2021 08:51:32 Loading tsv_files/tsv_transformed/musixmatch_cased/AMI2018A_train.json as task 0\n",
            "Loaded 200 samples out of 200\n",
            "Loaded 800 samples out of 800\n",
            "Loaded 1000 samples out of 1000\n",
            "07/26/2021 08:51:32 ####################\n",
            "07/26/2021 08:51:32 {'log_file': 'mt-dnn-train.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'Musixmatch/umberto-commoncrawl-cased-v1', 'data_dir': 'tsv_files/tsv_transformed/musixmatch_cased/', 'data_sort_on': False, 'name': 'farmer', 'task_def': 'tsv_files/tsv_transformed/AMI2018A_task_def.yml', 'train_datasets': ['AMI2018A'], 'test_datasets': ['AMI2018A'], 'glue_format_on': False, 'mkd_opt': 0, 'do_padding': False, 'gan': False, 'num_hidden_layers_d': 1, 'num_hidden_layers_g': 1, 'noise_size': 100, 'epsilon': 1e-08, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 10, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 1, 'pooler_actf': 'tanh', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 128, 'init_ratio': 1, 'encoder_type': 9, 'num_hidden_layers': -1, 'bert_model_type': 'Musixmatch/umberto-commoncrawl-cased-v1', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'bin_on': False, 'bin_size': 64, 'bin_grow_ratio': 0.5, 'local_rank': -1, 'world_size': 1, 'master_addr': 'localhost', 'master_port': '6600', 'backend': 'nccl', 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 10, 'batch_size': 16, 'batch_size_eval': 16, 'optimizer': 'adamW', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'apply_scheduler': False, 'lr_gamma': 0.5, 'scheduler_type': 'ms', 'output_dir': 'checkpoint', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': False, 'fp16_opt_level': 'O1', 'adv_train': False, 'adv_opt': 0, 'adv_norm_level': 0, 'adv_p_norm': 'inf', 'adv_alpha': 1, 'adv_k': 1, 'adv_step_size': 1e-05, 'adv_noise_var': 1e-05, 'adv_epsilon': 1e-06, 'encode_mode': False, 'debug': False, 'task_def_list': [{'self': '{}', 'label_vocab': 'None', 'n_class': '2', 'data_type': '<DataFormat.PremiseOnly: 1>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}]}\n",
            "07/26/2021 08:51:32 ####################\n",
            "07/26/2021 08:51:32 ############# Gradient Accumulation Info #############\n",
            "07/26/2021 08:51:32 number of step: 130\n",
            "07/26/2021 08:51:32 number of grad grad_accumulation step: 1\n",
            "07/26/2021 08:51:32 adjusted number of step: 130\n",
            "07/26/2021 08:51:32 ############# Gradient Accumulation Info #############\n",
            "Downloading: 100% 508/508 [00:00<00:00, 660kB/s]\n",
            "Downloading: 100% 445M/445M [00:11<00:00, 38.2MB/s]\n",
            "07/26/2021 08:51:53 \n",
            "############# Model Arch of MT-DNN #############\n",
            "SANBertNetwork(\n",
            "  (dropout_list): ModuleList(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (bert): CamembertModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): RobertaPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (scoring_list): ModuleList(\n",
            "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "07/26/2021 08:51:53 Total number of params: 110623490\n",
            "07/26/2021 08:51:53 At epoch 0\n",
            "07/26/2021 08:51:53 Task [ 0] updates[     1] train loss[0.70540] remaining[0:00:04]\n",
            "07/26/2021 08:51:56 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:52:00 Task AMI2018A -- epoch 0 -- Dev F1MAC: 39.933\n",
            "07/26/2021 08:52:00 Task AMI2018A -- epoch 0 -- Dev ACC: 54.875\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:52:00 Evaluation\n",
            "Test\n",
            "07/26/2021 08:52:05 Task AMI2018A -- epoch 0 -- Test F1MAC: 35.103\n",
            "07/26/2021 08:52:05 Task AMI2018A -- epoch 0 -- Test ACC: 49.500\n",
            "07/26/2021 08:52:05 [new test scores at 0 saved.]\n",
            "07/26/2021 08:52:10 At epoch 1\n",
            "07/26/2021 08:52:13 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:52:17 Task AMI2018A -- epoch 1 -- Dev F1MAC: 64.011\n",
            "07/26/2021 08:52:17 Task AMI2018A -- epoch 1 -- Dev ACC: 64.125\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:52:17 Evaluation\n",
            "Test\n",
            "07/26/2021 08:52:22 Task AMI2018A -- epoch 1 -- Test F1MAC: 67.335\n",
            "07/26/2021 08:52:22 Task AMI2018A -- epoch 1 -- Test ACC: 67.500\n",
            "07/26/2021 08:52:22 [new test scores at 1 saved.]\n",
            "07/26/2021 08:52:38 At epoch 2\n",
            "07/26/2021 08:52:42 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:52:46 Task AMI2018A -- epoch 2 -- Dev F1MAC: 68.467\n",
            "07/26/2021 08:52:46 Task AMI2018A -- epoch 2 -- Dev ACC: 68.500\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:52:46 Evaluation\n",
            "Test\n",
            "07/26/2021 08:52:51 Task AMI2018A -- epoch 2 -- Test F1MAC: 71.175\n",
            "07/26/2021 08:52:51 Task AMI2018A -- epoch 2 -- Test ACC: 71.700\n",
            "07/26/2021 08:52:51 [new test scores at 2 saved.]\n",
            "07/26/2021 08:53:13 At epoch 3\n",
            "07/26/2021 08:53:17 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:53:21 Task AMI2018A -- epoch 3 -- Dev F1MAC: 72.461\n",
            "07/26/2021 08:53:21 Task AMI2018A -- epoch 3 -- Dev ACC: 72.500\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:53:21 Evaluation\n",
            "Test\n",
            "07/26/2021 08:53:26 Task AMI2018A -- epoch 3 -- Test F1MAC: 74.891\n",
            "07/26/2021 08:53:26 Task AMI2018A -- epoch 3 -- Test ACC: 75.600\n",
            "07/26/2021 08:53:26 [new test scores at 3 saved.]\n",
            "07/26/2021 08:53:45 At epoch 4\n",
            "07/26/2021 08:53:49 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:53:53 Task AMI2018A -- epoch 4 -- Dev F1MAC: 70.741\n",
            "07/26/2021 08:53:53 Task AMI2018A -- epoch 4 -- Dev ACC: 72.625\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:53:53 Evaluation\n",
            "Test\n",
            "07/26/2021 08:53:58 Task AMI2018A -- epoch 4 -- Test F1MAC: 67.413\n",
            "07/26/2021 08:53:58 Task AMI2018A -- epoch 4 -- Test ACC: 68.300\n",
            "07/26/2021 08:53:58 [new test scores at 4 saved.]\n",
            "07/26/2021 08:54:18 At epoch 5\n",
            "07/26/2021 08:54:22 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:54:26 Task AMI2018A -- epoch 5 -- Dev F1MAC: 73.207\n",
            "07/26/2021 08:54:26 Task AMI2018A -- epoch 5 -- Dev ACC: 74.750\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:54:26 Evaluation\n",
            "Test\n",
            "07/26/2021 08:54:31 Task AMI2018A -- epoch 5 -- Test F1MAC: 73.682\n",
            "07/26/2021 08:54:31 Task AMI2018A -- epoch 5 -- Test ACC: 73.900\n",
            "07/26/2021 08:54:31 [new test scores at 5 saved.]\n",
            "07/26/2021 08:54:50 At epoch 6\n",
            "07/26/2021 08:54:54 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:54:58 Task AMI2018A -- epoch 6 -- Dev F1MAC: 74.482\n",
            "07/26/2021 08:54:58 Task AMI2018A -- epoch 6 -- Dev ACC: 76.125\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:54:58 Evaluation\n",
            "Test\n",
            "07/26/2021 08:55:03 Task AMI2018A -- epoch 6 -- Test F1MAC: 79.725\n",
            "07/26/2021 08:55:03 Task AMI2018A -- epoch 6 -- Test ACC: 79.900\n",
            "07/26/2021 08:55:03 [new test scores at 6 saved.]\n",
            "07/26/2021 08:55:23 At epoch 7\n",
            "07/26/2021 08:55:27 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:55:31 Task AMI2018A -- epoch 7 -- Dev F1MAC: 73.853\n",
            "07/26/2021 08:55:31 Task AMI2018A -- epoch 7 -- Dev ACC: 73.875\n",
            "07/26/2021 08:55:31 Evaluation\n",
            "Test\n",
            "07/26/2021 08:55:36 Task AMI2018A -- epoch 7 -- Test F1MAC: 72.407\n",
            "07/26/2021 08:55:36 Task AMI2018A -- epoch 7 -- Test ACC: 73.500\n",
            "07/26/2021 08:55:36 [new test scores at 7 saved.]\n",
            "07/26/2021 08:55:57 At epoch 8\n",
            "07/26/2021 08:56:00 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:56:04 Task AMI2018A -- epoch 8 -- Dev F1MAC: 77.622\n",
            "07/26/2021 08:56:04 Task AMI2018A -- epoch 8 -- Dev ACC: 77.625\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:56:04 Evaluation\n",
            "Test\n",
            "07/26/2021 08:56:09 Task AMI2018A -- epoch 8 -- Test F1MAC: 75.070\n",
            "07/26/2021 08:56:09 Task AMI2018A -- epoch 8 -- Test ACC: 75.700\n",
            "07/26/2021 08:56:09 [new test scores at 8 saved.]\n",
            "07/26/2021 08:56:29 At epoch 9\n",
            "07/26/2021 08:56:32 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:56:36 Task AMI2018A -- epoch 9 -- Dev F1MAC: 78.319\n",
            "07/26/2021 08:56:36 Task AMI2018A -- epoch 9 -- Dev ACC: 78.625\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:56:36 Evaluation\n",
            "Test\n",
            "07/26/2021 08:56:41 Task AMI2018A -- epoch 9 -- Test F1MAC: 79.834\n",
            "07/26/2021 08:56:41 Task AMI2018A -- epoch 9 -- Test ACC: 79.900\n",
            "07/26/2021 08:56:41 [new test scores at 9 saved.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8i48m5b3r4-"
      },
      "source": [
        "### Task AMI B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPgAd8un3yc2",
        "outputId": "1b0d85ee-422c-46ee-ecb4-d0874b099fa1"
      },
      "source": [
        "%cd tsv_files/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRl49CR14B9l",
        "outputId": "901adf92-424f-49eb-9883-73cc4d71dc34"
      },
      "source": [
        "file_loaded3=True\n",
        "\n",
        "tsv_AMI2018_train = 'AMI2018_it_training.tsv'\n",
        "tsv_AMI2018_test = 'AMI2018_it_testing.tsv'\n",
        "\n",
        "df_train3 = pd.read_csv(tsv_AMI2018_train, delimiter='\\t')\n",
        "df = pd.DataFrame(columns=['id', 'misogyny_category', 'text'])\n",
        "for ind in df_train3.index:\n",
        "  if df_train3.misogynous[ind]==1:\n",
        "    if df_train3.misogyny_category[ind] == 'stereotype':\n",
        "      df = df.append({'id' : df_train3['id'][ind], 'misogyny_category' : 0, 'text' : df_train3['text'][ind] }, ignore_index=True)\n",
        "    #elif df_train3.misogyny_category[ind] == 'dominance':\n",
        "      #df = df.append({'id' : df_train3['id'][ind], 'misogyny_category' : 1, 'text' : df_train3['text'][ind] }, ignore_index=True)\n",
        "    #elif df_train3.misogyny_category[ind] == 'derailing':\n",
        "      #df = df.append({'id' : df_train3['id'][ind], 'misogyny_category' : 2, 'text' : df_train3['text'][ind] }, ignore_index=True)\n",
        "    elif df_train3.misogyny_category[ind] == 'sexual_harassment':\n",
        "      df = df.append({'id' : df_train3['id'][ind], 'misogyny_category' : 1, 'text' : df_train3['text'][ind] }, ignore_index=True)\n",
        "    elif df_train3.misogyny_category[ind] == 'discredit':\n",
        "      df = df.append({'id' : df_train3['id'][ind], 'misogyny_category' : 2, 'text' : df_train3['text'][ind] }, ignore_index=True)\n",
        "\n",
        "df_train3 = df\n",
        "\n",
        "df_test3 = pd.read_csv(tsv_AMI2018_test, delimiter='\\t')\n",
        "df = pd.DataFrame(columns=['id', 'misogyny_category', 'text'])\n",
        "for ind in df_test3.index:\n",
        "  if df_test3.misogynous[ind]==1:\n",
        "    if df_test3.misogyny_category[ind] == 'stereotype':\n",
        "      df = df.append({'id' : df_test3['id'][ind], 'misogyny_category' : 0, 'text' : df_test3['text'][ind] }, ignore_index=True)\n",
        "    #elif df_test3.misogyny_category[ind] == 'dominance':\n",
        "      #df = df.append({'id' : df_test3['id'][ind], 'misogyny_category' : 1, 'text' : df_test3['text'][ind] }, ignore_index=True)\n",
        "    #elif df_test3.misogyny_category[ind] == 'derailing':\n",
        "      #df = df.append({'id' : df_test3['id'][ind], 'misogyny_category' : 2, 'text' : df_test3['text'][ind] }, ignore_index=True)\n",
        "    elif df_test3.misogyny_category[ind] == 'sexual_harassment':\n",
        "      df = df.append({'id' : df_test3['id'][ind], 'misogyny_category' : 1, 'text' : df_test3['text'][ind] }, ignore_index=True)\n",
        "    elif df_test3.misogyny_category[ind] == 'discredit':\n",
        "      df = df.append({'id' : df_test3['id'][ind], 'misogyny_category' : 2, 'text' : df_test3['text'][ind] }, ignore_index=True)\n",
        "\n",
        "df_test3 = df\n",
        "\n",
        "#split train dev\n",
        "train_dataset3, dev_dataset3 = train_test_split(df_train3, test_size=0.2, shuffle = True)\n",
        "\n",
        "#reduction\n",
        "if number_labeled_examples!=0:\n",
        "  if number_labeled_examples==100:\n",
        "    labeled3 = train_dataset3.sample(n=100)\n",
        "    unlabeled3 = train_dataset3\n",
        "    cond = unlabeled3['id'].isin(labeled3['id'])\n",
        "    unlabeled3.drop(unlabeled3[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==200:\n",
        "    labeled3 = train_dataset3.sample(n=200)\n",
        "    unlabeled3 = train_dataset3\n",
        "    cond = unlabeled3['id'].isin(labeled3['id'])\n",
        "    unlabeled3.drop(unlabeled3[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==500:\n",
        "    labeled3 = train_dataset3.sample(n=500)\n",
        "    unlabeled3 = train_dataset3\n",
        "    cond = unlabeled3['id'].isin(labeled3['id'])\n",
        "    unlabeled3.drop(unlabeled3[cond].index, inplace = True)\n",
        "\n",
        "  #model with or without gan \n",
        "  if apply_gan == True:\n",
        "    print(\"GANBERT\")\n",
        "    #dataset unlabeled with label -1\n",
        "    unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(0,-1)\n",
        "    unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(1,-1)\n",
        "    unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(2,-1)\n",
        "    unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(3,-1)\n",
        "    unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(4,-1)\n",
        "    train3 = pd.concat([labeled3, unlabeled3])\n",
        "    dev3 = dev_dataset3\n",
        "    print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train3),len(labeled3), len(unlabeled3)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev3)))\n",
        "  else:\n",
        "    print(\"BERT-based model, with reduction dataset\")\n",
        "    train3 = labeled3\n",
        "    dev3 = dev_dataset3\n",
        "    print(\"Size of Train dataset is {} \".format(len(labeled3)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev3)))\n",
        "\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  train3 = train_dataset3\n",
        "  dev3=dev_dataset3\n",
        "  print(\"Size of Train dataset is {} \".format(len(train3)))\n",
        "  print(\"Size of Dev dataset is {} \".format(len(dev3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model, with reduction dataset\n",
            "Size of Train dataset is 200 \n",
            "Size of Dev dataset is 347 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJmXmQEu6CI4",
        "outputId": "28cc2826-bf2b-4079-baa0-d3d68bcd61cb"
      },
      "source": [
        "!mkdir tsv_transformed\n",
        "%cd tsv_transformed/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘tsv_transformed’: File exists\n",
            "/content/mttransformer/tsv_files/tsv_transformed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V167Yaii6EEm"
      },
      "source": [
        "#train\n",
        "name_train = \"AMI2018B_train.tsv\"\n",
        "id_train = train3.id\n",
        "label_train = train3.misogyny_category\n",
        "sentence_train = train3.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"AMI2018B_dev.tsv\"\n",
        "id_dev = dev3.id\n",
        "label_dev = dev3.misogyny_category\n",
        "sentence_dev = dev3.text\n",
        "\n",
        "#test\n",
        "name_test = \"AMI2018B_test.tsv\"\n",
        "id_test = df_test3.id\n",
        "label_test = df_test3.misogyny_category\n",
        "sentence_test = df_test3.text\n",
        "\n",
        "#task_def\n",
        "name_file = 'AMI2018B_task_def.yml'\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))\n",
        "\n",
        "\n",
        "task = \"AMI2018B:\\n\"\n",
        "  \n",
        "f = open(name_file, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    f.write(task)\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\")\n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 3\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvA_RjKr6y_n",
        "outputId": "71bb338a-1477-4e13-c99f-dadbc07f4405"
      },
      "source": [
        "%cd ..\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n",
            "/content/mttransformer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz9_Igaq6zmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05728738-67b8-4c9c-abbb-c871d64df321"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python prepro_std.py --gan --apply_balance --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/AMI2018B_task_def.yml\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python prepro_std.py --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/AMI2018B_task_def.yml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "2021-07-26 08:57:18.973721: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading english - 1grams ...\n",
            "Reading english - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "07/26/2021 08:57:36 Task AMI2018B\n",
            "07/26/2021 08:57:36 tsv_files/tsv_transformed/musixmatch_cased/AMI2018B_train.json\n",
            "07/26/2021 08:57:36 tsv_files/tsv_transformed/musixmatch_cased/AMI2018B_dev.json\n",
            "07/26/2021 08:57:36 tsv_files/tsv_transformed/musixmatch_cased/AMI2018B_test.json\n",
            "\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUvisdOm64Rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ffc52c7-38dc-4d0c-de11-1cffaf095b8c"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python train.py --gan --num_hidden_layers_d 0 --num_hidden_layers_g 3 --noise_size 100 --epsilon 1e-8 --encoder_type 9 --epochs 25 --task_def tsv_files/tsv_transformed/AMI2018B_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 64 --batch_size_eval 64 --optimizer \"adamW\" --train_datasets AMI2018B --test_datasets AMI2018B --learning_rate \"1e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python train.py --encoder_type 9 --epochs 10 --task_def tsv_files/tsv_transformed/AMI2018B_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 16 --batch_size_eval 16 --optimizer \"adamW\" --train_datasets AMI2018B --test_datasets AMI2018B --learning_rate \"5e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model\n",
            "2021-07-26 08:57:38.774832: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "07/26/2021 08:57:40 Launching the MT-DNN training\n",
            "07/26/2021 08:57:40 Loading tsv_files/tsv_transformed/musixmatch_cased/AMI2018B_train.json as task 0\n",
            "Loaded 200 samples out of 200\n",
            "Loaded 347 samples out of 347\n",
            "Loaded 446 samples out of 446\n",
            "07/26/2021 08:57:40 ####################\n",
            "07/26/2021 08:57:40 {'log_file': 'mt-dnn-train.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'Musixmatch/umberto-commoncrawl-cased-v1', 'data_dir': 'tsv_files/tsv_transformed/musixmatch_cased/', 'data_sort_on': False, 'name': 'farmer', 'task_def': 'tsv_files/tsv_transformed/AMI2018B_task_def.yml', 'train_datasets': ['AMI2018B'], 'test_datasets': ['AMI2018B'], 'glue_format_on': False, 'mkd_opt': 0, 'do_padding': False, 'gan': False, 'num_hidden_layers_d': 1, 'num_hidden_layers_g': 1, 'noise_size': 100, 'epsilon': 1e-08, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 10, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 1, 'pooler_actf': 'tanh', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 128, 'init_ratio': 1, 'encoder_type': 9, 'num_hidden_layers': -1, 'bert_model_type': 'Musixmatch/umberto-commoncrawl-cased-v1', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'bin_on': False, 'bin_size': 64, 'bin_grow_ratio': 0.5, 'local_rank': -1, 'world_size': 1, 'master_addr': 'localhost', 'master_port': '6600', 'backend': 'nccl', 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 10, 'batch_size': 16, 'batch_size_eval': 16, 'optimizer': 'adamW', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'apply_scheduler': False, 'lr_gamma': 0.5, 'scheduler_type': 'ms', 'output_dir': 'checkpoint', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': False, 'fp16_opt_level': 'O1', 'adv_train': False, 'adv_opt': 0, 'adv_norm_level': 0, 'adv_p_norm': 'inf', 'adv_alpha': 1, 'adv_k': 1, 'adv_step_size': 1e-05, 'adv_noise_var': 1e-05, 'adv_epsilon': 1e-06, 'encode_mode': False, 'debug': False, 'task_def_list': [{'self': '{}', 'label_vocab': 'None', 'n_class': '3', 'data_type': '<DataFormat.PremiseOnly: 1>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}]}\n",
            "07/26/2021 08:57:40 ####################\n",
            "07/26/2021 08:57:40 ############# Gradient Accumulation Info #############\n",
            "07/26/2021 08:57:40 number of step: 130\n",
            "07/26/2021 08:57:40 number of grad grad_accumulation step: 1\n",
            "07/26/2021 08:57:40 adjusted number of step: 130\n",
            "07/26/2021 08:57:40 ############# Gradient Accumulation Info #############\n",
            "07/26/2021 08:58:01 \n",
            "############# Model Arch of MT-DNN #############\n",
            "SANBertNetwork(\n",
            "  (dropout_list): ModuleList(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (bert): CamembertModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): RobertaPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (scoring_list): ModuleList(\n",
            "    (0): Linear(in_features=768, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "07/26/2021 08:58:01 Total number of params: 110624259\n",
            "07/26/2021 08:58:01 At epoch 0\n",
            "07/26/2021 08:58:01 Task [ 0] updates[     1] train loss[1.12177] remaining[0:00:02]\n",
            "07/26/2021 08:58:04 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:58:06 Task AMI2018B -- epoch 0 -- Dev F1MAC: 28.651\n",
            "07/26/2021 08:58:06 Task AMI2018B -- epoch 0 -- Dev ACC: 39.769\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:58:06 Evaluation\n",
            "Test\n",
            "07/26/2021 08:58:08 Task AMI2018B -- epoch 0 -- Test F1MAC: 26.818\n",
            "07/26/2021 08:58:08 Task AMI2018B -- epoch 0 -- Test ACC: 32.511\n",
            "07/26/2021 08:58:08 [new test scores at 0 saved.]\n",
            "07/26/2021 08:58:14 At epoch 1\n",
            "07/26/2021 08:58:17 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:58:19 Task AMI2018B -- epoch 1 -- Dev F1MAC: 24.613\n",
            "07/26/2021 08:58:19 Task AMI2018B -- epoch 1 -- Dev ACC: 38.329\n",
            "07/26/2021 08:58:19 Evaluation\n",
            "Test\n",
            "07/26/2021 08:58:21 Task AMI2018B -- epoch 1 -- Test F1MAC: 25.872\n",
            "07/26/2021 08:58:21 Task AMI2018B -- epoch 1 -- Test ACC: 39.910\n",
            "07/26/2021 08:58:21 [new test scores at 1 saved.]\n",
            "07/26/2021 08:58:45 At epoch 2\n",
            "07/26/2021 08:58:48 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:58:50 Task AMI2018B -- epoch 2 -- Dev F1MAC: 54.908\n",
            "07/26/2021 08:58:50 Task AMI2018B -- epoch 2 -- Dev ACC: 54.467\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:58:50 Evaluation\n",
            "Test\n",
            "07/26/2021 08:58:52 Task AMI2018B -- epoch 2 -- Test F1MAC: 48.659\n",
            "07/26/2021 08:58:52 Task AMI2018B -- epoch 2 -- Test ACC: 51.570\n",
            "07/26/2021 08:58:52 [new test scores at 2 saved.]\n",
            "07/26/2021 08:59:17 At epoch 3\n",
            "07/26/2021 08:59:22 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:59:24 Task AMI2018B -- epoch 3 -- Dev F1MAC: 65.591\n",
            "07/26/2021 08:59:24 Task AMI2018B -- epoch 3 -- Dev ACC: 65.706\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 08:59:24 Evaluation\n",
            "Test\n",
            "07/26/2021 08:59:26 Task AMI2018B -- epoch 3 -- Test F1MAC: 54.096\n",
            "07/26/2021 08:59:26 Task AMI2018B -- epoch 3 -- Test ACC: 58.744\n",
            "07/26/2021 08:59:26 [new test scores at 3 saved.]\n",
            "07/26/2021 08:59:52 At epoch 4\n",
            "07/26/2021 08:59:55 Evaluation\n",
            "Dev\n",
            "07/26/2021 08:59:57 Task AMI2018B -- epoch 4 -- Dev F1MAC: 65.555\n",
            "07/26/2021 08:59:57 Task AMI2018B -- epoch 4 -- Dev ACC: 65.706\n",
            "07/26/2021 08:59:57 Evaluation\n",
            "Test\n",
            "07/26/2021 08:59:59 Task AMI2018B -- epoch 4 -- Test F1MAC: 59.477\n",
            "07/26/2021 08:59:59 Task AMI2018B -- epoch 4 -- Test ACC: 61.435\n",
            "07/26/2021 08:59:59 [new test scores at 4 saved.]\n",
            "07/26/2021 09:00:21 At epoch 5\n",
            "07/26/2021 09:00:25 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:00:27 Task AMI2018B -- epoch 5 -- Dev F1MAC: 76.803\n",
            "07/26/2021 09:00:27 Task AMI2018B -- epoch 5 -- Dev ACC: 76.369\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:00:27 Evaluation\n",
            "Test\n",
            "07/26/2021 09:00:29 Task AMI2018B -- epoch 5 -- Test F1MAC: 70.942\n",
            "07/26/2021 09:00:29 Task AMI2018B -- epoch 5 -- Test ACC: 72.646\n",
            "07/26/2021 09:00:29 [new test scores at 5 saved.]\n",
            "07/26/2021 09:00:53 At epoch 6\n",
            "07/26/2021 09:00:56 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:00:58 Task AMI2018B -- epoch 6 -- Dev F1MAC: 77.998\n",
            "07/26/2021 09:00:58 Task AMI2018B -- epoch 6 -- Dev ACC: 77.522\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:00:58 Evaluation\n",
            "Test\n",
            "07/26/2021 09:01:01 Task AMI2018B -- epoch 6 -- Test F1MAC: 71.857\n",
            "07/26/2021 09:01:01 Task AMI2018B -- epoch 6 -- Test ACC: 72.422\n",
            "07/26/2021 09:01:01 [new test scores at 6 saved.]\n",
            "07/26/2021 09:01:28 At epoch 7\n",
            "07/26/2021 09:01:31 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:01:33 Task AMI2018B -- epoch 7 -- Dev F1MAC: 79.065\n",
            "07/26/2021 09:01:33 Task AMI2018B -- epoch 7 -- Dev ACC: 78.674\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:01:33 Evaluation\n",
            "Test\n",
            "07/26/2021 09:01:35 Task AMI2018B -- epoch 7 -- Test F1MAC: 70.513\n",
            "07/26/2021 09:01:35 Task AMI2018B -- epoch 7 -- Test ACC: 70.852\n",
            "07/26/2021 09:01:35 [new test scores at 7 saved.]\n",
            "07/26/2021 09:01:57 At epoch 8\n",
            "07/26/2021 09:02:01 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:02:03 Task AMI2018B -- epoch 8 -- Dev F1MAC: 82.401\n",
            "07/26/2021 09:02:03 Task AMI2018B -- epoch 8 -- Dev ACC: 82.421\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:02:03 Evaluation\n",
            "Test\n",
            "07/26/2021 09:02:05 Task AMI2018B -- epoch 8 -- Test F1MAC: 77.089\n",
            "07/26/2021 09:02:05 Task AMI2018B -- epoch 8 -- Test ACC: 76.906\n",
            "07/26/2021 09:02:05 [new test scores at 8 saved.]\n",
            "07/26/2021 09:02:30 At epoch 9\n",
            "07/26/2021 09:02:34 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:02:36 Task AMI2018B -- epoch 9 -- Dev F1MAC: 82.087\n",
            "07/26/2021 09:02:36 Task AMI2018B -- epoch 9 -- Dev ACC: 81.844\n",
            "07/26/2021 09:02:36 Evaluation\n",
            "Test\n",
            "07/26/2021 09:02:38 Task AMI2018B -- epoch 9 -- Test F1MAC: 74.823\n",
            "07/26/2021 09:02:38 Task AMI2018B -- epoch 9 -- Test ACC: 75.112\n",
            "07/26/2021 09:02:38 [new test scores at 9 saved.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ0_9xuB7bdu"
      },
      "source": [
        "### Task DANKMEMEs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-Dmli_w7efq",
        "outputId": "0c6b3293-b4b3-4c1e-c3e7-93cc04f702d6"
      },
      "source": [
        "%cd tsv_files/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-3cuSEh7rgY",
        "outputId": "86376d67-6c1b-4a3d-c0bc-d497d53db389"
      },
      "source": [
        "file_loaded4=True\n",
        "\n",
        "tsv_DANKMEMES2020_train = 'dankmemes_task2_train.csv'\n",
        "tsv_DANKMEMES2020_test = 'hate_test.csv'\n",
        "\n",
        "df_train4 = pd.read_csv(tsv_DANKMEMES2020_train, delimiter=',')\n",
        "df_train4 = df_train4[['File']+['Hate Speech']+['Text']]\n",
        "df_test4 = pd.read_csv(tsv_DANKMEMES2020_test, delimiter=',')\n",
        "df_test4 = df_test4[['File']+['Hate Speech']+['Text']]\n",
        "\n",
        "\n",
        "#split train dev\n",
        "train_dataset4, dev_dataset4 = train_test_split(df_train4, test_size=0.2, shuffle = True)\n",
        "\n",
        "#reduction\n",
        "if number_labeled_examples!=0:\n",
        "\n",
        "  if number_labeled_examples==100:\n",
        "    labeled4 = train_dataset4.sample(n=100)\n",
        "    unlabeled4 = train_dataset4\n",
        "    cond = unlabeled4['File'].isin(labeled4['File'])\n",
        "    unlabeled4.drop(unlabeled4[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==200:\n",
        "    labeled4 = train_dataset4.sample(n=200)\n",
        "    unlabeled4 = train_dataset4\n",
        "    cond = unlabeled4['File'].isin(labeled4['File'])\n",
        "    unlabeled4.drop(unlabeled4[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==500:\n",
        "    labeled4 = train_dataset4.sample(n=500)\n",
        "    unlabeled4 = train_dataset4\n",
        "    cond = unlabeled4['File'].isin(labeled4['File'])\n",
        "    unlabeled4.drop(unlabeled4[cond].index, inplace = True)\n",
        "\n",
        "  #model with or without gan \n",
        "  if apply_gan == True:\n",
        "    print(\"GANBERT\")\n",
        "    #dataset unlabeled with label -1\n",
        "    unlabeled4['Hate Speech'] = unlabeled4['Hate Speech'].replace(0,-1)\n",
        "    unlabeled4['Hate Speech'] = unlabeled4['Hate Speech'].replace(1,-1)\n",
        "    train4 = pd.concat([labeled4, unlabeled4])\n",
        "    dev4 = dev_dataset4\n",
        "    print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train4),len(labeled4), len(unlabeled4)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev4)))\n",
        "  else:\n",
        "    print(\"BERT-based model, with reduction dataset\")\n",
        "    train4 = labeled4\n",
        "    dev4 = dev_dataset4\n",
        "    print(\"Size of Train dataset is {} \".format(len(labeled4)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev4)))\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  train4 = train_dataset4\n",
        "  dev4=dev_dataset4\n",
        "  print(\"Size of Train dataset is {} \".format(len(train4)))\n",
        "  print(\"Size of Dev dataset is {} \".format(len(dev4)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model, with reduction dataset\n",
            "Size of Train dataset is 200 \n",
            "Size of Dev dataset is 160 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9orDLf898wQd",
        "outputId": "efbecd77-9564-42fc-a181-f2dad9a3363a"
      },
      "source": [
        "!mkdir tsv_transformed\n",
        "%cd tsv_transformed/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘tsv_transformed’: File exists\n",
            "/content/mttransformer/tsv_files/tsv_transformed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISMXEotm838t"
      },
      "source": [
        "#train\n",
        "name_train = \"DANKMEMES2020_train.tsv\"\n",
        "id_train = train4.File\n",
        "label_train = train4[\"Hate Speech\"]\n",
        "sentence_train = train4.Text\n",
        "\n",
        "#dev\n",
        "name_dev = \"DANKMEMES2020_dev.tsv\"\n",
        "id_dev = dev4.File\n",
        "label_dev = dev4[\"Hate Speech\"]\n",
        "sentence_dev = dev4.Text\n",
        "\n",
        "#test\n",
        "name_test = \"DANKMEMES2020_test.tsv\"\n",
        "id_test = df_test4.File\n",
        "label_test = df_test4[\"Hate Speech\"]\n",
        "sentence_test = df_test4.Text\n",
        "\n",
        "#task_def\n",
        "name_file = 'DANKMEMES2020_task_def.yml'\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))\n",
        "\n",
        "\n",
        "task = \"DANKMEMES2020:\\n\"\n",
        "  \n",
        "f = open(name_file, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    f.write(task)\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\") \n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 2\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iMRNj4W8-OA",
        "outputId": "61cf49e2-813b-4604-a74b-3bdee614ec2e"
      },
      "source": [
        "%cd ..\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n",
            "/content/mttransformer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWeMI_RK8_hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ebe2bb-f1bb-4e95-da09-a9383ffb62a7"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python prepro_std.py --gan --apply_balance --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/DANKMEMES2020_task_def.yml\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python prepro_std.py --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/DANKMEMES2020_task_def.yml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "2021-07-26 09:03:11.695673: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading english - 1grams ...\n",
            "Reading english - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "07/26/2021 09:03:28 Task DANKMEMES2020\n",
            "07/26/2021 09:03:28 tsv_files/tsv_transformed/musixmatch_cased/DANKMEMES2020_train.json\n",
            "07/26/2021 09:03:28 tsv_files/tsv_transformed/musixmatch_cased/DANKMEMES2020_dev.json\n",
            "07/26/2021 09:03:28 tsv_files/tsv_transformed/musixmatch_cased/DANKMEMES2020_test.json\n",
            "\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vXniYso9BUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b691508-6403-47ba-c313-241174904200"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python train.py --gan --num_hidden_layers_d 0 --num_hidden_layers_g 3 --noise_size 100 --epsilon 1e-8 --encoder_type 9 --epochs 25 --task_def tsv_files/tsv_transformed/DANKMEMES2020_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 64 --batch_size_eval 64 --optimizer \"adamW\" --train_datasets DANKMEMES2020 --test_datasets DANKMEMES2020 --learning_rate \"1e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python train.py --encoder_type 9 --epochs 10 --task_def tsv_files/tsv_transformed/DANKMEMES2020_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 16 --batch_size_eval 16 --optimizer \"adamW\" --train_datasets DANKMEMES2020 --test_datasets DANKMEMES2020 --learning_rate \"5e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model\n",
            "2021-07-26 09:03:30.841804: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "07/26/2021 09:03:33 Launching the MT-DNN training\n",
            "07/26/2021 09:03:33 Loading tsv_files/tsv_transformed/musixmatch_cased/DANKMEMES2020_train.json as task 0\n",
            "Loaded 200 samples out of 200\n",
            "Loaded 160 samples out of 160\n",
            "Loaded 200 samples out of 200\n",
            "07/26/2021 09:03:33 ####################\n",
            "07/26/2021 09:03:33 {'log_file': 'mt-dnn-train.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'Musixmatch/umberto-commoncrawl-cased-v1', 'data_dir': 'tsv_files/tsv_transformed/musixmatch_cased/', 'data_sort_on': False, 'name': 'farmer', 'task_def': 'tsv_files/tsv_transformed/DANKMEMES2020_task_def.yml', 'train_datasets': ['DANKMEMES2020'], 'test_datasets': ['DANKMEMES2020'], 'glue_format_on': False, 'mkd_opt': 0, 'do_padding': False, 'gan': False, 'num_hidden_layers_d': 1, 'num_hidden_layers_g': 1, 'noise_size': 100, 'epsilon': 1e-08, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 10, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 1, 'pooler_actf': 'tanh', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 128, 'init_ratio': 1, 'encoder_type': 9, 'num_hidden_layers': -1, 'bert_model_type': 'Musixmatch/umberto-commoncrawl-cased-v1', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'bin_on': False, 'bin_size': 64, 'bin_grow_ratio': 0.5, 'local_rank': -1, 'world_size': 1, 'master_addr': 'localhost', 'master_port': '6600', 'backend': 'nccl', 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 10, 'batch_size': 16, 'batch_size_eval': 16, 'optimizer': 'adamW', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'apply_scheduler': False, 'lr_gamma': 0.5, 'scheduler_type': 'ms', 'output_dir': 'checkpoint', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': False, 'fp16_opt_level': 'O1', 'adv_train': False, 'adv_opt': 0, 'adv_norm_level': 0, 'adv_p_norm': 'inf', 'adv_alpha': 1, 'adv_k': 1, 'adv_step_size': 1e-05, 'adv_noise_var': 1e-05, 'adv_epsilon': 1e-06, 'encode_mode': False, 'debug': False, 'task_def_list': [{'self': '{}', 'label_vocab': 'None', 'n_class': '2', 'data_type': '<DataFormat.PremiseOnly: 1>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}]}\n",
            "07/26/2021 09:03:33 ####################\n",
            "07/26/2021 09:03:33 ############# Gradient Accumulation Info #############\n",
            "07/26/2021 09:03:33 number of step: 130\n",
            "07/26/2021 09:03:33 number of grad grad_accumulation step: 1\n",
            "07/26/2021 09:03:33 adjusted number of step: 130\n",
            "07/26/2021 09:03:33 ############# Gradient Accumulation Info #############\n",
            "07/26/2021 09:03:53 \n",
            "############# Model Arch of MT-DNN #############\n",
            "SANBertNetwork(\n",
            "  (dropout_list): ModuleList(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (bert): CamembertModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): RobertaPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (scoring_list): ModuleList(\n",
            "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "07/26/2021 09:03:53 Total number of params: 110623490\n",
            "07/26/2021 09:03:53 At epoch 0\n",
            "07/26/2021 09:03:54 Task [ 0] updates[     1] train loss[0.68217] remaining[0:00:05]\n",
            "07/26/2021 09:03:57 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:03:58 Task DANKMEMES2020 -- epoch 0 -- Dev F1MAC: 39.532\n",
            "07/26/2021 09:03:58 Task DANKMEMES2020 -- epoch 0 -- Dev ACC: 43.125\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:03:58 Evaluation\n",
            "Test\n",
            "07/26/2021 09:03:58 Task DANKMEMES2020 -- epoch 0 -- Test F1MAC: 52.154\n",
            "07/26/2021 09:03:58 Task DANKMEMES2020 -- epoch 0 -- Test ACC: 52.500\n",
            "07/26/2021 09:03:58 [new test scores at 0 saved.]\n",
            "07/26/2021 09:04:04 At epoch 1\n",
            "07/26/2021 09:04:07 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:04:08 Task DANKMEMES2020 -- epoch 1 -- Dev F1MAC: 40.476\n",
            "07/26/2021 09:04:08 Task DANKMEMES2020 -- epoch 1 -- Dev ACC: 50.000\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:04:08 Evaluation\n",
            "Test\n",
            "07/26/2021 09:04:09 Task DANKMEMES2020 -- epoch 1 -- Test F1MAC: 48.814\n",
            "07/26/2021 09:04:09 Task DANKMEMES2020 -- epoch 1 -- Test ACC: 58.500\n",
            "07/26/2021 09:04:09 [new test scores at 1 saved.]\n",
            "07/26/2021 09:04:35 At epoch 2\n",
            "07/26/2021 09:04:38 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:04:39 Task DANKMEMES2020 -- epoch 2 -- Dev F1MAC: 47.383\n",
            "07/26/2021 09:04:39 Task DANKMEMES2020 -- epoch 2 -- Dev ACC: 48.125\n",
            "BEST F1MAC EPOCH\n",
            "07/26/2021 09:04:39 Evaluation\n",
            "Test\n",
            "07/26/2021 09:04:40 Task DANKMEMES2020 -- epoch 2 -- Test F1MAC: 52.000\n",
            "07/26/2021 09:04:40 Task DANKMEMES2020 -- epoch 2 -- Test ACC: 55.000\n",
            "07/26/2021 09:04:40 [new test scores at 2 saved.]\n",
            "07/26/2021 09:05:05 At epoch 3\n",
            "07/26/2021 09:05:12 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:05:13 Task DANKMEMES2020 -- epoch 3 -- Dev F1MAC: 55.063\n",
            "07/26/2021 09:05:13 Task DANKMEMES2020 -- epoch 3 -- Dev ACC: 56.250\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:05:13 Evaluation\n",
            "Test\n",
            "07/26/2021 09:05:14 Task DANKMEMES2020 -- epoch 3 -- Test F1MAC: 57.473\n",
            "07/26/2021 09:05:14 Task DANKMEMES2020 -- epoch 3 -- Test ACC: 57.500\n",
            "07/26/2021 09:05:14 [new test scores at 3 saved.]\n",
            "07/26/2021 09:05:36 At epoch 4\n",
            "07/26/2021 09:05:39 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:05:41 Task DANKMEMES2020 -- epoch 4 -- Dev F1MAC: 55.407\n",
            "07/26/2021 09:05:41 Task DANKMEMES2020 -- epoch 4 -- Dev ACC: 56.250\n",
            "BEST F1MAC EPOCH\n",
            "07/26/2021 09:05:41 Evaluation\n",
            "Test\n",
            "07/26/2021 09:05:42 Task DANKMEMES2020 -- epoch 4 -- Test F1MAC: 49.723\n",
            "07/26/2021 09:05:42 Task DANKMEMES2020 -- epoch 4 -- Test ACC: 52.500\n",
            "07/26/2021 09:05:42 [new test scores at 4 saved.]\n",
            "07/26/2021 09:06:10 At epoch 5\n",
            "07/26/2021 09:06:13 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:06:14 Task DANKMEMES2020 -- epoch 5 -- Dev F1MAC: 62.908\n",
            "07/26/2021 09:06:14 Task DANKMEMES2020 -- epoch 5 -- Dev ACC: 65.000\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:06:14 Evaluation\n",
            "Test\n",
            "07/26/2021 09:06:15 Task DANKMEMES2020 -- epoch 5 -- Test F1MAC: 51.188\n",
            "07/26/2021 09:06:15 Task DANKMEMES2020 -- epoch 5 -- Test ACC: 54.000\n",
            "07/26/2021 09:06:15 [new test scores at 5 saved.]\n",
            "07/26/2021 09:06:41 At epoch 6\n",
            "07/26/2021 09:06:45 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:06:45 Task DANKMEMES2020 -- epoch 6 -- Dev F1MAC: 53.578\n",
            "07/26/2021 09:06:45 Task DANKMEMES2020 -- epoch 6 -- Dev ACC: 55.000\n",
            "07/26/2021 09:06:45 Evaluation\n",
            "Test\n",
            "07/26/2021 09:06:46 Task DANKMEMES2020 -- epoch 6 -- Test F1MAC: 57.933\n",
            "07/26/2021 09:06:46 Task DANKMEMES2020 -- epoch 6 -- Test ACC: 61.000\n",
            "07/26/2021 09:06:46 [new test scores at 6 saved.]\n",
            "07/26/2021 09:07:14 At epoch 7\n",
            "07/26/2021 09:07:17 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:07:18 Task DANKMEMES2020 -- epoch 7 -- Dev F1MAC: 61.862\n",
            "07/26/2021 09:07:18 Task DANKMEMES2020 -- epoch 7 -- Dev ACC: 61.875\n",
            "07/26/2021 09:07:18 Evaluation\n",
            "Test\n",
            "07/26/2021 09:07:19 Task DANKMEMES2020 -- epoch 7 -- Test F1MAC: 58.198\n",
            "07/26/2021 09:07:19 Task DANKMEMES2020 -- epoch 7 -- Test ACC: 58.500\n",
            "07/26/2021 09:07:19 [new test scores at 7 saved.]\n",
            "07/26/2021 09:07:45 At epoch 8\n",
            "07/26/2021 09:07:49 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:07:49 Task DANKMEMES2020 -- epoch 8 -- Dev F1MAC: 65.909\n",
            "07/26/2021 09:07:49 Task DANKMEMES2020 -- epoch 8 -- Dev ACC: 66.250\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:07:49 Evaluation\n",
            "Test\n",
            "07/26/2021 09:07:50 Task DANKMEMES2020 -- epoch 8 -- Test F1MAC: 62.762\n",
            "07/26/2021 09:07:50 Task DANKMEMES2020 -- epoch 8 -- Test ACC: 63.000\n",
            "07/26/2021 09:07:50 [new test scores at 8 saved.]\n",
            "07/26/2021 09:08:17 At epoch 9\n",
            "07/26/2021 09:08:23 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:08:23 Task DANKMEMES2020 -- epoch 9 -- Dev F1MAC: 61.315\n",
            "07/26/2021 09:08:23 Task DANKMEMES2020 -- epoch 9 -- Dev ACC: 62.500\n",
            "07/26/2021 09:08:23 Evaluation\n",
            "Test\n",
            "07/26/2021 09:08:24 Task DANKMEMES2020 -- epoch 9 -- Test F1MAC: 56.897\n",
            "07/26/2021 09:08:24 Task DANKMEMES2020 -- epoch 9 -- Test ACC: 58.000\n",
            "07/26/2021 09:08:24 [new test scores at 9 saved.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4WGayCY9uJv"
      },
      "source": [
        "### Task SENTIPOLC 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvvxjHIm90zu",
        "outputId": "a5d592ce-77ff-44e8-f0bb-5bec267bbbb3"
      },
      "source": [
        "%cd tsv_files/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SZhdVGf96h0",
        "outputId": "51bdca61-a31c-45c4-e4c8-a495dcb0b4c1"
      },
      "source": [
        "file_loaded5=True\n",
        "\n",
        "tsv_SENTIPOLC2016_train = 'training_set_sentipolc16.csv'\n",
        "tsv_SENTIPOLC2016_test = 'test_set_sentipolc16_gold2000.csv'\n",
        "\n",
        "df_train5 = pd.read_csv(tsv_SENTIPOLC2016_train, delimiter=',')\n",
        "df_train5 = df_train5[['idtwitter']+['subj']+['text']]\n",
        "\n",
        "df_test5 = pd.read_csv(tsv_SENTIPOLC2016_test, delimiter=',')\n",
        "df_test5 = df_test5[['idtwitter']+['subj']+['text']]\n",
        "\n",
        "for ind in df_train5.index:\n",
        "  if \"\\t\" in df_train5.text[ind]:\n",
        "    df_train5 = df_train5.replace(to_replace='\\t', value='', regex=True)\n",
        "\n",
        "\n",
        "#split train dev \n",
        "train_dataset5, dev_dataset5 = train_test_split(df_train5, test_size=0.2, shuffle = True)\n",
        "\n",
        "if number_labeled_examples!=0:\n",
        "\n",
        "  if number_labeled_examples==100:\n",
        "    labeled5 = train_dataset5.sample(n=100)\n",
        "    unlabeled5 = train_dataset5\n",
        "    cond = unlabeled5['idtwitter'].isin(labeled5['idtwitter'])\n",
        "    unlabeled5.drop(unlabeled5[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==200:\n",
        "    labeled5 = train_dataset5.sample(n=200)\n",
        "    unlabeled5 = train_dataset5\n",
        "    cond = unlabeled5['idtwitter'].isin(labeled5['idtwitter'])\n",
        "    unlabeled5.drop(unlabeled5[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==500:\n",
        "    labeled5 = train_dataset5.sample(n=500)\n",
        "    unlabeled5 = train_dataset5\n",
        "    cond = unlabeled5['idtwitter'].isin(labeled5['idtwitter'])\n",
        "    unlabeled5.drop(unlabeled5[cond].index, inplace = True)\n",
        "  \n",
        "  #model with or without gan \n",
        "  if apply_gan == True:\n",
        "    print(\"GANBERT\")\n",
        "    #dataset unlabeled with label -1\n",
        "    unlabeled5['subj'] = unlabeled5['subj'].replace(0,-1)\n",
        "    unlabeled5['subj'] = unlabeled5['subj'].replace(1,-1)\n",
        "    train5 = pd.concat([labeled5, unlabeled5])\n",
        "    dev5 = dev_dataset5\n",
        "    print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train5),len(labeled5), len(unlabeled5)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev5)))\n",
        "  else:\n",
        "    print(\"BERT-based model, with reduction dataset\")\n",
        "    train5 = labeled5\n",
        "    dev5 = dev_dataset5\n",
        "    print(\"Size of Train dataset is {} \".format(len(labeled5)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev5)))\n",
        "\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  train5 = train_dataset5\n",
        "  dev5=dev_dataset5\n",
        "  print(\"Size of Train dataset is {} \".format(len(train5)))\n",
        "  print(\"Size of Dev dataset is {} \".format(len(dev5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model, with reduction dataset\n",
            "Size of Train dataset is 200 \n",
            "Size of Dev dataset is 1482 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgrG-93z_E1p",
        "outputId": "05b40c23-6de0-4e6e-87e5-30e83a5786c7"
      },
      "source": [
        "!mkdir tsv_transformed\n",
        "%cd tsv_transformed/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘tsv_transformed’: File exists\n",
            "/content/mttransformer/tsv_files/tsv_transformed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_ICP4yv_Ozq"
      },
      "source": [
        "#train\n",
        "name_train = \"SENTIPOLC20161_train.tsv\"\n",
        "id_train = train5.idtwitter\n",
        "label_train = train5.subj\n",
        "sentence_train = train5.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"SENTIPOLC20161_dev.tsv\"\n",
        "id_dev = dev5.idtwitter\n",
        "label_dev = dev5.subj\n",
        "sentence_dev = dev5.text\n",
        "\n",
        "#test\n",
        "name_test = \"SENTIPOLC20161_test.tsv\"\n",
        "id_test = df_test5.idtwitter\n",
        "label_test = df_test5.subj\n",
        "sentence_test = df_test5.text\n",
        "\n",
        "#task_def\n",
        "name_file = 'SENTIPOLC20161_task_def.yml'\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))\n",
        "\n",
        "\n",
        "task = \"SENTIPOLC20161:\\n\"\n",
        "  \n",
        "f = open(name_file, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    f.write(task)\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\")\n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 2\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUbOVALe_Yl2",
        "outputId": "190da0f0-1add-4404-d9d1-b96d9c5f6807"
      },
      "source": [
        "%cd ..\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n",
            "/content/mttransformer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvEWbzUK_Zjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aedea1f-1185-4f7f-84cd-ab8dc0a6dccc"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python prepro_std.py --gan --apply_balance --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/SENTIPOLC20161_task_def.yml\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python prepro_std.py --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/SENTIPOLC20161_task_def.yml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "2021-07-26 09:08:56.169779: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading english - 1grams ...\n",
            "Reading english - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "07/26/2021 09:09:11 Task SENTIPOLC20161\n",
            "07/26/2021 09:09:11 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20161_train.json\n",
            "07/26/2021 09:09:11 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20161_dev.json\n",
            "07/26/2021 09:09:11 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20161_test.json\n",
            "\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83D4t1Ey_bcl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446f0762-9683-4335-ee28-cff4ca907429"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python train.py --gan --num_hidden_layers_d 0 --num_hidden_layers_g 3 --noise_size 100 --epsilon 1e-8 --encoder_type 9 --epochs 25 --task_def tsv_files/tsv_transformed/SENTIPOLC20161_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 64 --batch_size_eval 64 --optimizer \"adamW\" --train_datasets SENTIPOLC20161 --test_datasets SENTIPOLC20161 --learning_rate \"1e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python train.py --encoder_type 9 --epochs 10 --task_def tsv_files/tsv_transformed/SENTIPOLC20161_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 16 --batch_size_eval 16 --optimizer \"adamW\" --train_datasets SENTIPOLC20161 --test_datasets SENTIPOLC20161 --learning_rate \"5e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model\n",
            "2021-07-26 09:09:14.512586: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "07/26/2021 09:09:16 Launching the MT-DNN training\n",
            "07/26/2021 09:09:16 Loading tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20161_train.json as task 0\n",
            "Loaded 200 samples out of 200\n",
            "Loaded 1482 samples out of 1482\n",
            "Loaded 2000 samples out of 2000\n",
            "07/26/2021 09:09:16 ####################\n",
            "07/26/2021 09:09:16 {'log_file': 'mt-dnn-train.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'Musixmatch/umberto-commoncrawl-cased-v1', 'data_dir': 'tsv_files/tsv_transformed/musixmatch_cased/', 'data_sort_on': False, 'name': 'farmer', 'task_def': 'tsv_files/tsv_transformed/SENTIPOLC20161_task_def.yml', 'train_datasets': ['SENTIPOLC20161'], 'test_datasets': ['SENTIPOLC20161'], 'glue_format_on': False, 'mkd_opt': 0, 'do_padding': False, 'gan': False, 'num_hidden_layers_d': 1, 'num_hidden_layers_g': 1, 'noise_size': 100, 'epsilon': 1e-08, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 10, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 1, 'pooler_actf': 'tanh', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 128, 'init_ratio': 1, 'encoder_type': 9, 'num_hidden_layers': -1, 'bert_model_type': 'Musixmatch/umberto-commoncrawl-cased-v1', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'bin_on': False, 'bin_size': 64, 'bin_grow_ratio': 0.5, 'local_rank': -1, 'world_size': 1, 'master_addr': 'localhost', 'master_port': '6600', 'backend': 'nccl', 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 10, 'batch_size': 16, 'batch_size_eval': 16, 'optimizer': 'adamW', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'apply_scheduler': False, 'lr_gamma': 0.5, 'scheduler_type': 'ms', 'output_dir': 'checkpoint', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': False, 'fp16_opt_level': 'O1', 'adv_train': False, 'adv_opt': 0, 'adv_norm_level': 0, 'adv_p_norm': 'inf', 'adv_alpha': 1, 'adv_k': 1, 'adv_step_size': 1e-05, 'adv_noise_var': 1e-05, 'adv_epsilon': 1e-06, 'encode_mode': False, 'debug': False, 'task_def_list': [{'self': '{}', 'label_vocab': 'None', 'n_class': '2', 'data_type': '<DataFormat.PremiseOnly: 1>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}]}\n",
            "07/26/2021 09:09:16 ####################\n",
            "07/26/2021 09:09:16 ############# Gradient Accumulation Info #############\n",
            "07/26/2021 09:09:16 number of step: 130\n",
            "07/26/2021 09:09:16 number of grad grad_accumulation step: 1\n",
            "07/26/2021 09:09:16 adjusted number of step: 130\n",
            "07/26/2021 09:09:16 ############# Gradient Accumulation Info #############\n",
            "07/26/2021 09:09:37 \n",
            "############# Model Arch of MT-DNN #############\n",
            "SANBertNetwork(\n",
            "  (dropout_list): ModuleList(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (bert): CamembertModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): RobertaPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (scoring_list): ModuleList(\n",
            "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "07/26/2021 09:09:37 Total number of params: 110623490\n",
            "07/26/2021 09:09:37 At epoch 0\n",
            "07/26/2021 09:09:38 Task [ 0] updates[     1] train loss[0.67434] remaining[0:00:04]\n",
            "07/26/2021 09:09:40 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:09:45 Task SENTIPOLC20161 -- epoch 0 -- Dev F1MAC: 65.023\n",
            "07/26/2021 09:09:45 Task SENTIPOLC20161 -- epoch 0 -- Dev ACC: 68.893\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:09:45 Evaluation\n",
            "Test\n",
            "07/26/2021 09:09:53 Task SENTIPOLC20161 -- epoch 0 -- Test F1MAC: 67.906\n",
            "07/26/2021 09:09:53 Task SENTIPOLC20161 -- epoch 0 -- Test ACC: 68.900\n",
            "07/26/2021 09:09:53 [new test scores at 0 saved.]\n",
            "07/26/2021 09:09:58 At epoch 1\n",
            "07/26/2021 09:10:00 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:10:06 Task SENTIPOLC20161 -- epoch 1 -- Dev F1MAC: 66.781\n",
            "07/26/2021 09:10:06 Task SENTIPOLC20161 -- epoch 1 -- Dev ACC: 70.918\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:10:06 Evaluation\n",
            "Test\n",
            "07/26/2021 09:10:13 Task SENTIPOLC20161 -- epoch 1 -- Test F1MAC: 71.586\n",
            "07/26/2021 09:10:13 Task SENTIPOLC20161 -- epoch 1 -- Test ACC: 72.700\n",
            "07/26/2021 09:10:13 [new test scores at 1 saved.]\n",
            "07/26/2021 09:10:27 At epoch 2\n",
            "07/26/2021 09:10:29 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:10:34 Task SENTIPOLC20161 -- epoch 2 -- Dev F1MAC: 67.199\n",
            "07/26/2021 09:10:34 Task SENTIPOLC20161 -- epoch 2 -- Dev ACC: 68.421\n",
            "BEST F1MAC EPOCH\n",
            "07/26/2021 09:10:34 Evaluation\n",
            "Test\n",
            "07/26/2021 09:10:41 Task SENTIPOLC20161 -- epoch 2 -- Test F1MAC: 61.178\n",
            "07/26/2021 09:10:41 Task SENTIPOLC20161 -- epoch 2 -- Test ACC: 61.200\n",
            "07/26/2021 09:10:41 [new test scores at 2 saved.]\n",
            "07/26/2021 09:10:59 At epoch 3\n",
            "07/26/2021 09:11:06 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:11:11 Task SENTIPOLC20161 -- epoch 3 -- Dev F1MAC: 66.503\n",
            "07/26/2021 09:11:11 Task SENTIPOLC20161 -- epoch 3 -- Dev ACC: 72.537\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:11:11 Evaluation\n",
            "Test\n",
            "07/26/2021 09:11:18 Task SENTIPOLC20161 -- epoch 3 -- Test F1MAC: 74.290\n",
            "07/26/2021 09:11:18 Task SENTIPOLC20161 -- epoch 3 -- Test ACC: 76.550\n",
            "07/26/2021 09:11:18 [new test scores at 3 saved.]\n",
            "07/26/2021 09:11:34 At epoch 4\n",
            "07/26/2021 09:11:36 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:11:41 Task SENTIPOLC20161 -- epoch 4 -- Dev F1MAC: 70.495\n",
            "07/26/2021 09:11:41 Task SENTIPOLC20161 -- epoch 4 -- Dev ACC: 74.426\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:11:41 Evaluation\n",
            "Test\n",
            "07/26/2021 09:11:48 Task SENTIPOLC20161 -- epoch 4 -- Test F1MAC: 72.651\n",
            "07/26/2021 09:11:48 Task SENTIPOLC20161 -- epoch 4 -- Test ACC: 73.800\n",
            "07/26/2021 09:11:48 [new test scores at 4 saved.]\n",
            "07/26/2021 09:12:05 At epoch 5\n",
            "07/26/2021 09:12:07 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:12:12 Task SENTIPOLC20161 -- epoch 5 -- Dev F1MAC: 63.756\n",
            "07/26/2021 09:12:12 Task SENTIPOLC20161 -- epoch 5 -- Dev ACC: 73.617\n",
            "07/26/2021 09:12:12 Evaluation\n",
            "Test\n",
            "07/26/2021 09:12:19 Task SENTIPOLC20161 -- epoch 5 -- Test F1MAC: 74.112\n",
            "07/26/2021 09:12:19 Task SENTIPOLC20161 -- epoch 5 -- Test ACC: 78.400\n",
            "07/26/2021 09:12:19 [new test scores at 5 saved.]\n",
            "07/26/2021 09:12:39 At epoch 6\n",
            "07/26/2021 09:12:42 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:12:47 Task SENTIPOLC20161 -- epoch 6 -- Dev F1MAC: 71.068\n",
            "07/26/2021 09:12:47 Task SENTIPOLC20161 -- epoch 6 -- Dev ACC: 75.439\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:12:47 Evaluation\n",
            "Test\n",
            "07/26/2021 09:12:54 Task SENTIPOLC20161 -- epoch 6 -- Test F1MAC: 75.488\n",
            "07/26/2021 09:12:54 Task SENTIPOLC20161 -- epoch 6 -- Test ACC: 76.900\n",
            "07/26/2021 09:12:54 [new test scores at 6 saved.]\n",
            "07/26/2021 09:13:11 At epoch 7\n",
            "07/26/2021 09:13:13 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:13:18 Task SENTIPOLC20161 -- epoch 7 -- Dev F1MAC: 70.953\n",
            "07/26/2021 09:13:18 Task SENTIPOLC20161 -- epoch 7 -- Dev ACC: 73.887\n",
            "07/26/2021 09:13:18 Evaluation\n",
            "Test\n",
            "07/26/2021 09:13:26 Task SENTIPOLC20161 -- epoch 7 -- Test F1MAC: 71.981\n",
            "07/26/2021 09:13:26 Task SENTIPOLC20161 -- epoch 7 -- Test ACC: 72.550\n",
            "07/26/2021 09:13:26 [new test scores at 7 saved.]\n",
            "07/26/2021 09:13:42 At epoch 8\n",
            "07/26/2021 09:13:45 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:13:50 Task SENTIPOLC20161 -- epoch 8 -- Dev F1MAC: 59.999\n",
            "07/26/2021 09:13:50 Task SENTIPOLC20161 -- epoch 8 -- Dev ACC: 72.942\n",
            "07/26/2021 09:13:50 Evaluation\n",
            "Test\n",
            "07/26/2021 09:13:57 Task SENTIPOLC20161 -- epoch 8 -- Test F1MAC: 67.113\n",
            "07/26/2021 09:13:57 Task SENTIPOLC20161 -- epoch 8 -- Test ACC: 75.550\n",
            "07/26/2021 09:13:57 [new test scores at 8 saved.]\n",
            "07/26/2021 09:14:15 At epoch 9\n",
            "07/26/2021 09:14:19 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:14:24 Task SENTIPOLC20161 -- epoch 9 -- Dev F1MAC: 70.919\n",
            "07/26/2021 09:14:24 Task SENTIPOLC20161 -- epoch 9 -- Dev ACC: 73.144\n",
            "07/26/2021 09:14:24 Evaluation\n",
            "Test\n",
            "07/26/2021 09:14:32 Task SENTIPOLC20161 -- epoch 9 -- Test F1MAC: 70.361\n",
            "07/26/2021 09:14:32 Task SENTIPOLC20161 -- epoch 9 -- Test ACC: 70.750\n",
            "07/26/2021 09:14:32 [new test scores at 9 saved.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpS4S-dAGUA"
      },
      "source": [
        "### Task SENTIPOLC 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU0gYaJ2AKPJ",
        "outputId": "a480028f-db69-4b93-8b88-12af6909142d"
      },
      "source": [
        "%cd tsv_files/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXCDywIFAOZB",
        "outputId": "900ff139-045a-433e-cc35-aac59453e465"
      },
      "source": [
        "file_loaded6=True\n",
        "\n",
        "tsv_SENTIPOLC2016_train = 'training_set_sentipolc16.csv'\n",
        "tsv_SENTIPOLC2016_test = 'test_set_sentipolc16_gold2000.csv'\n",
        "\n",
        "df_train6 = pd.read_csv(tsv_SENTIPOLC2016_train, delimiter=',')\n",
        "\n",
        "df = pd.DataFrame(columns=['idtwitter', 'polarity', 'text'])\n",
        "for ind in df_train6.index:\n",
        "  if df_train6['subj'][ind] == 1:\n",
        "    if df_train6['opos'][ind] == 1 and df_train6['oneg'][ind] == 0:\n",
        "      df = df.append({'idtwitter' : df_train6['idtwitter'][ind], 'polarity' : 0, 'text' : df_train6['text'][ind] }, ignore_index=True)\n",
        "    elif df_train6['opos'][ind] == 0 and df_train6['oneg'][ind] == 1:\n",
        "      df = df.append({'idtwitter' : df_train6['idtwitter'][ind], 'polarity' : 1, 'text' : df_train6['text'][ind] }, ignore_index=True)\n",
        "    elif df_train6['opos'][ind] == 0 and df_train6['oneg'][ind] == 0:\n",
        "      df = df.append({'idtwitter' : df_train6['idtwitter'][ind], 'polarity' : 2, 'text' : df_train6['text'][ind] }, ignore_index=True)\n",
        "  else:\n",
        "    if df_train6['opos'][ind] == 0 and df_train6['oneg'][ind] == 0:\n",
        "      df = df.append({'idtwitter' : df_train6['idtwitter'][ind], 'polarity' : 2, 'text' : df_train6['text'][ind] }, ignore_index=True)\n",
        "\n",
        "df_train6 = df\n",
        "\n",
        "for ind in df_train6.index:\n",
        "  if \"\\t\" in df_train6.text[ind]:\n",
        "    df_train6 = df_train6.replace(to_replace='\\t', value='', regex=True)\n",
        "\n",
        "df_test6 = pd.read_csv(tsv_SENTIPOLC2016_test, delimiter=',')\n",
        "\n",
        "df = pd.DataFrame(columns=['idtwitter', 'polarity', 'text'])\n",
        "for ind in df_test6.index:\n",
        "  if df_test6['subj'][ind] == 1:\n",
        "    if df_test6['opos'][ind] == 1 and df_test6['oneg'][ind] == 0:\n",
        "      df = df.append({'idtwitter' : df_test6['idtwitter'][ind], 'polarity' : 0, 'text' : df_test6['text'][ind] }, ignore_index=True)\n",
        "    elif df_test6['opos'][ind] == 0 and df_test6['oneg'][ind] == 1:\n",
        "      df = df.append({'idtwitter' : df_test6['idtwitter'][ind], 'polarity' : 1, 'text' : df_test6['text'][ind] }, ignore_index=True)\n",
        "    elif df_test6['opos'][ind] == 0 and df_test6['oneg'][ind] == 0:\n",
        "      df = df.append({'idtwitter' : df_test6['idtwitter'][ind], 'polarity' : 2, 'text' : df_test6['text'][ind] }, ignore_index=True)\n",
        "  else:\n",
        "    if df_test6['opos'][ind] == 0 and df_test6['oneg'][ind] == 0:\n",
        "      df = df.append({'idtwitter' : df_test6['idtwitter'][ind], 'polarity' : 2, 'text' : df_test6['text'][ind] }, ignore_index=True)\n",
        "\n",
        "df_test6 = df\n",
        "\n",
        "#split train dev\n",
        "train_dataset6, dev_dataset6 = train_test_split(df_train6, test_size=0.2, shuffle = True)\n",
        "\n",
        "#reduction\n",
        "if number_labeled_examples!=0:\n",
        "\n",
        "  if number_labeled_examples==100:\n",
        "    labeled6 = train_dataset6.sample(n=100)\n",
        "    unlabeled6 = train_dataset6\n",
        "    cond = unlabeled6['idtwitter'].isin(labeled6['idtwitter'])\n",
        "    unlabeled6.drop(unlabeled6[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==200:\n",
        "    labeled6 = train_dataset6.sample(n=200)\n",
        "    unlabeled6 = train_dataset6\n",
        "    cond = unlabeled6['idtwitter'].isin(labeled6['idtwitter'])\n",
        "    unlabeled6.drop(unlabeled6[cond].index, inplace = True)\n",
        "\n",
        "  elif number_labeled_examples==500:\n",
        "    labeled6 = train_dataset6.sample(n=500)\n",
        "    unlabeled6 = train_dataset6\n",
        "    cond = unlabeled6['idtwitter'].isin(labeled6['idtwitter'])\n",
        "    unlabeled6.drop(unlabeled6[cond].index, inplace = True)\n",
        "  \n",
        "  #model with or without gan \n",
        "  if apply_gan == True:\n",
        "    print(\"GANBERT\")\n",
        "    #dataset unlabeled with label -1\n",
        "    unlabeled6['polarity'] = unlabeled6['polarity'].replace(0,-1)\n",
        "    unlabeled6['polarity'] = unlabeled6['polarity'].replace(1,-1)\n",
        "    unlabeled6['polarity'] = unlabeled6['polarity'].replace(2,-1)\n",
        "    train6 = pd.concat([labeled6, unlabeled6])\n",
        "    dev6 = dev_dataset6\n",
        "    print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train6),len(labeled6), len(unlabeled6)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev6)))\n",
        "  else:\n",
        "    print(\"BERT-based model, with reduction dataset\")\n",
        "    train6 = labeled6\n",
        "    dev6 = dev_dataset6\n",
        "    print(\"Size of Train dataset is {} \".format(len(labeled6)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev6)))\n",
        "\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  train6 = train_dataset6\n",
        "  dev6=dev_dataset6\n",
        "  print(\"Size of Train dataset is {} \".format(len(train6)))\n",
        "  print(\"Size of Dev dataset is {} \".format(len(dev6)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model, with reduction dataset\n",
            "Size of Train dataset is 200 \n",
            "Size of Dev dataset is 1394 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHsybcK4A3Hv",
        "outputId": "5d90521d-3709-4c7d-c45b-62abe5324c8e"
      },
      "source": [
        "!mkdir tsv_transformed\n",
        "%cd tsv_transformed/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘tsv_transformed’: File exists\n",
            "/content/mttransformer/tsv_files/tsv_transformed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6cdeuMFBuHy"
      },
      "source": [
        "#train\n",
        "name_train = \"SENTIPOLC20162_train.tsv\"\n",
        "id_train = train6.idtwitter\n",
        "label_train = train6.polarity\n",
        "sentence_train = train6.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"SENTIPOLC20162_dev.tsv\"\n",
        "id_dev = dev6.idtwitter\n",
        "label_dev = dev6.polarity\n",
        "sentence_dev = dev6.text\n",
        "\n",
        "#test\n",
        "name_test = \"SENTIPOLC20162_test.tsv\"\n",
        "id_test = df_test6.idtwitter\n",
        "label_test = df_test6.polarity\n",
        "sentence_test = df_test6.text\n",
        "\n",
        "#task_def\n",
        "name_file = 'SENTIPOLC20162_task_def.yml'\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))\n",
        "\n",
        "\n",
        "task = \"SENTIPOLC20162:\\n\"\n",
        "  \n",
        "f = open(name_file, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    f.write(task)\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\")\n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 3\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNmRBn0-BycO",
        "outputId": "f88a45ea-c0da-496f-9e45-137fbda9e259"
      },
      "source": [
        "%cd ..\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n",
            "/content/mttransformer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGYUG_QfCBUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e064bac-78d7-4a22-86ad-08d6c973c085"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python prepro_std.py --gan --apply_balance --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/SENTIPOLC20162_task_def.yml\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python prepro_std.py --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/SENTIPOLC20162_task_def.yml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "2021-07-26 09:15:17.563850: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading english - 1grams ...\n",
            "Reading english - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "07/26/2021 09:15:33 Task SENTIPOLC20162\n",
            "07/26/2021 09:15:33 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20162_train.json\n",
            "07/26/2021 09:15:33 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20162_dev.json\n",
            "07/26/2021 09:15:34 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20162_test.json\n",
            "\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nAi0Gw_GHcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bae26e-f143-4965-b048-439e63ebf218"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"GANBERT\")\n",
        "  !python train.py --gan --num_hidden_layers_d 0 --num_hidden_layers_g 3 --noise_size 100 --epsilon 1e-8 --encoder_type 9 --epochs 25 --task_def tsv_files/tsv_transformed/SENTIPOLC20162_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 64 --batch_size_eval 64 --optimizer \"adamW\" --train_datasets SENTIPOLC20162 --test_datasets SENTIPOLC20162 --learning_rate \"1e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1\n",
        "else:\n",
        "  print(\"BERT-based model\")\n",
        "  !python train.py --encoder_type 9 --epochs 10 --task_def tsv_files/tsv_transformed/SENTIPOLC20162_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 16 --batch_size_eval 16 --optimizer \"adamW\" --train_datasets SENTIPOLC20162 --test_datasets SENTIPOLC20162 --learning_rate \"5e-5\" #--multi_gpu_on --grad_accumulation_step 4 --fp16 --grad_clipping 0 --global_grad_clipping 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-based model\n",
            "2021-07-26 09:15:37.055273: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "07/26/2021 09:15:39 Launching the MT-DNN training\n",
            "07/26/2021 09:15:39 Loading tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20162_train.json as task 0\n",
            "Loaded 200 samples out of 200\n",
            "Loaded 1394 samples out of 1394\n",
            "Loaded 1964 samples out of 1964\n",
            "07/26/2021 09:15:39 ####################\n",
            "07/26/2021 09:15:39 {'log_file': 'mt-dnn-train.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'Musixmatch/umberto-commoncrawl-cased-v1', 'data_dir': 'tsv_files/tsv_transformed/musixmatch_cased/', 'data_sort_on': False, 'name': 'farmer', 'task_def': 'tsv_files/tsv_transformed/SENTIPOLC20162_task_def.yml', 'train_datasets': ['SENTIPOLC20162'], 'test_datasets': ['SENTIPOLC20162'], 'glue_format_on': False, 'mkd_opt': 0, 'do_padding': False, 'gan': False, 'num_hidden_layers_d': 1, 'num_hidden_layers_g': 1, 'noise_size': 100, 'epsilon': 1e-08, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 10, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 1, 'pooler_actf': 'tanh', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 128, 'init_ratio': 1, 'encoder_type': 9, 'num_hidden_layers': -1, 'bert_model_type': 'Musixmatch/umberto-commoncrawl-cased-v1', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'bin_on': False, 'bin_size': 64, 'bin_grow_ratio': 0.5, 'local_rank': -1, 'world_size': 1, 'master_addr': 'localhost', 'master_port': '6600', 'backend': 'nccl', 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 10, 'batch_size': 16, 'batch_size_eval': 16, 'optimizer': 'adamW', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'apply_scheduler': False, 'lr_gamma': 0.5, 'scheduler_type': 'ms', 'output_dir': 'checkpoint', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': False, 'fp16_opt_level': 'O1', 'adv_train': False, 'adv_opt': 0, 'adv_norm_level': 0, 'adv_p_norm': 'inf', 'adv_alpha': 1, 'adv_k': 1, 'adv_step_size': 1e-05, 'adv_noise_var': 1e-05, 'adv_epsilon': 1e-06, 'encode_mode': False, 'debug': False, 'task_def_list': [{'self': '{}', 'label_vocab': 'None', 'n_class': '3', 'data_type': '<DataFormat.PremiseOnly: 1>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}]}\n",
            "07/26/2021 09:15:39 ####################\n",
            "07/26/2021 09:15:39 ############# Gradient Accumulation Info #############\n",
            "07/26/2021 09:15:39 number of step: 130\n",
            "07/26/2021 09:15:39 number of grad grad_accumulation step: 1\n",
            "07/26/2021 09:15:39 adjusted number of step: 130\n",
            "07/26/2021 09:15:39 ############# Gradient Accumulation Info #############\n",
            "07/26/2021 09:16:00 \n",
            "############# Model Arch of MT-DNN #############\n",
            "SANBertNetwork(\n",
            "  (dropout_list): ModuleList(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (bert): CamembertModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): RobertaPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (scoring_list): ModuleList(\n",
            "    (0): Linear(in_features=768, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "07/26/2021 09:16:00 Total number of params: 110624259\n",
            "07/26/2021 09:16:00 At epoch 0\n",
            "07/26/2021 09:16:00 Task [ 0] updates[     1] train loss[1.10467] remaining[0:00:04]\n",
            "07/26/2021 09:16:03 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:16:07 Task SENTIPOLC20162 -- epoch 0 -- Dev F1MAC: 23.504\n",
            "07/26/2021 09:16:07 Task SENTIPOLC20162 -- epoch 0 -- Dev ACC: 38.379\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:16:07 Evaluation\n",
            "Test\n",
            "07/26/2021 09:16:15 Task SENTIPOLC20162 -- epoch 0 -- Test F1MAC: 21.144\n",
            "07/26/2021 09:16:15 Task SENTIPOLC20162 -- epoch 0 -- Test ACC: 38.544\n",
            "07/26/2021 09:16:15 [new test scores at 0 saved.]\n",
            "07/26/2021 09:16:20 At epoch 1\n",
            "07/26/2021 09:16:23 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:16:28 Task SENTIPOLC20162 -- epoch 1 -- Dev F1MAC: 19.393\n",
            "07/26/2021 09:16:28 Task SENTIPOLC20162 -- epoch 1 -- Dev ACC: 36.155\n",
            "07/26/2021 09:16:28 Evaluation\n",
            "Test\n",
            "07/26/2021 09:16:35 Task SENTIPOLC20162 -- epoch 1 -- Test F1MAC: 19.202\n",
            "07/26/2021 09:16:35 Task SENTIPOLC20162 -- epoch 1 -- Test ACC: 37.525\n",
            "07/26/2021 09:16:35 [new test scores at 1 saved.]\n",
            "07/26/2021 09:16:50 At epoch 2\n",
            "07/26/2021 09:16:52 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:16:56 Task SENTIPOLC20162 -- epoch 2 -- Dev F1MAC: 34.274\n",
            "07/26/2021 09:16:56 Task SENTIPOLC20162 -- epoch 2 -- Dev ACC: 44.333\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:16:56 Evaluation\n",
            "Test\n",
            "07/26/2021 09:17:03 Task SENTIPOLC20162 -- epoch 2 -- Test F1MAC: 34.361\n",
            "07/26/2021 09:17:03 Task SENTIPOLC20162 -- epoch 2 -- Test ACC: 46.640\n",
            "07/26/2021 09:17:03 [new test scores at 2 saved.]\n",
            "07/26/2021 09:17:26 At epoch 3\n",
            "07/26/2021 09:17:28 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:17:33 Task SENTIPOLC20162 -- epoch 3 -- Dev F1MAC: 30.861\n",
            "07/26/2021 09:17:33 Task SENTIPOLC20162 -- epoch 3 -- Dev ACC: 39.813\n",
            "07/26/2021 09:17:33 Evaluation\n",
            "Test\n",
            "07/26/2021 09:17:41 Task SENTIPOLC20162 -- epoch 3 -- Test F1MAC: 31.646\n",
            "07/26/2021 09:17:41 Task SENTIPOLC20162 -- epoch 3 -- Test ACC: 42.057\n",
            "07/26/2021 09:17:41 [new test scores at 3 saved.]\n",
            "07/26/2021 09:17:56 At epoch 4\n",
            "07/26/2021 09:17:59 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:18:03 Task SENTIPOLC20162 -- epoch 4 -- Dev F1MAC: 27.244\n",
            "07/26/2021 09:18:03 Task SENTIPOLC20162 -- epoch 4 -- Dev ACC: 39.527\n",
            "07/26/2021 09:18:04 Evaluation\n",
            "Test\n",
            "07/26/2021 09:18:11 Task SENTIPOLC20162 -- epoch 4 -- Test F1MAC: 24.974\n",
            "07/26/2021 09:18:11 Task SENTIPOLC20162 -- epoch 4 -- Test ACC: 39.664\n",
            "07/26/2021 09:18:11 [new test scores at 4 saved.]\n",
            "07/26/2021 09:18:26 At epoch 5\n",
            "07/26/2021 09:18:28 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:18:33 Task SENTIPOLC20162 -- epoch 5 -- Dev F1MAC: 41.495\n",
            "07/26/2021 09:18:33 Task SENTIPOLC20162 -- epoch 5 -- Dev ACC: 44.978\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:18:33 Evaluation\n",
            "Test\n",
            "07/26/2021 09:18:40 Task SENTIPOLC20162 -- epoch 5 -- Test F1MAC: 42.738\n",
            "07/26/2021 09:18:40 Task SENTIPOLC20162 -- epoch 5 -- Test ACC: 47.963\n",
            "07/26/2021 09:18:40 [new test scores at 5 saved.]\n",
            "07/26/2021 09:18:59 At epoch 6\n",
            "07/26/2021 09:19:05 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:19:10 Task SENTIPOLC20162 -- epoch 6 -- Dev F1MAC: 38.505\n",
            "07/26/2021 09:19:10 Task SENTIPOLC20162 -- epoch 6 -- Dev ACC: 46.557\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:19:10 Evaluation\n",
            "Test\n",
            "07/26/2021 09:19:17 Task SENTIPOLC20162 -- epoch 6 -- Test F1MAC: 45.047\n",
            "07/26/2021 09:19:17 Task SENTIPOLC20162 -- epoch 6 -- Test ACC: 53.768\n",
            "07/26/2021 09:19:17 [new test scores at 6 saved.]\n",
            "07/26/2021 09:19:30 At epoch 7\n",
            "07/26/2021 09:19:33 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:19:38 Task SENTIPOLC20162 -- epoch 7 -- Dev F1MAC: 49.051\n",
            "07/26/2021 09:19:38 Task SENTIPOLC20162 -- epoch 7 -- Dev ACC: 51.506\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 09:19:38 Evaluation\n",
            "Test\n",
            "07/26/2021 09:19:45 Task SENTIPOLC20162 -- epoch 7 -- Test F1MAC: 51.424\n",
            "07/26/2021 09:19:45 Task SENTIPOLC20162 -- epoch 7 -- Test ACC: 57.637\n",
            "07/26/2021 09:19:45 [new test scores at 7 saved.]\n",
            "07/26/2021 09:20:03 At epoch 8\n",
            "07/26/2021 09:20:06 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:20:10 Task SENTIPOLC20162 -- epoch 8 -- Dev F1MAC: 45.461\n",
            "07/26/2021 09:20:10 Task SENTIPOLC20162 -- epoch 8 -- Dev ACC: 46.485\n",
            "07/26/2021 09:20:10 Evaluation\n",
            "Test\n",
            "07/26/2021 09:20:17 Task SENTIPOLC20162 -- epoch 8 -- Test F1MAC: 43.107\n",
            "07/26/2021 09:20:17 Task SENTIPOLC20162 -- epoch 8 -- Test ACC: 43.992\n",
            "07/26/2021 09:20:17 [new test scores at 8 saved.]\n",
            "07/26/2021 09:20:38 At epoch 9\n",
            "07/26/2021 09:20:41 Evaluation\n",
            "Dev\n",
            "07/26/2021 09:20:46 Task SENTIPOLC20162 -- epoch 9 -- Dev F1MAC: 43.802\n",
            "07/26/2021 09:20:46 Task SENTIPOLC20162 -- epoch 9 -- Dev ACC: 45.552\n",
            "07/26/2021 09:20:46 Evaluation\n",
            "Test\n",
            "07/26/2021 09:20:53 Task SENTIPOLC20162 -- epoch 9 -- Test F1MAC: 44.930\n",
            "07/26/2021 09:20:53 Task SENTIPOLC20162 -- epoch 9 -- Test ACC: 46.640\n",
            "07/26/2021 09:20:53 [new test scores at 9 saved.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEfMeT8Nmk9R"
      },
      "source": [
        "## **MT model**\n",
        "\n",
        "\n",
        "> MTDNN\n",
        "\n",
        "\n",
        "> MT-GAN\n",
        "\n",
        "\n",
        "\n",
        "* In the first block the balancing technique is performed to balance the labeled data in the case of MT-DNN, and to balance the unlabeled data in the case of MT-GAN\n",
        "* In the second there is the multi-task training of the chosen model. This block has the same structure as the sub-blocks of the Single-Task model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ7BnGpqJgtd"
      },
      "source": [
        "### Loading File:\n",
        "\n",
        "\n",
        "> if the single-model is not used, the files of dataset have to load here or \n",
        "\n",
        "\n",
        "> if these files are loaded, but here we have to use the model with GAN (while in the single-model not), or here we have to use without GAN (while in the single-model not)\n",
        "\n",
        "if the data are the same of the single model, this block is not needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M57dvV6MJRH7"
      },
      "source": [
        "#if apply_gan is different from single-model\n",
        "apply_gan=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7whPha6oOckj",
        "outputId": "fccedabb-fa4e-4b87-8889-f9995296a3e8"
      },
      "source": [
        "%cd tsv_files/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FPfyWZ-QiI-"
      },
      "source": [
        "tsv_haspeede_train = 'haspeede_TW-train.tsv'\n",
        "tsv_haspeede_test = 'haspeede_TW-reference.tsv'\n",
        "tsv_AMI2018_train = 'AMI2018_it_training.tsv'\n",
        "tsv_AMI2018_test = 'AMI2018_it_testing.tsv'\n",
        "tsv_AMI2018_train = 'AMI2018_it_training.tsv'\n",
        "tsv_AMI2018_test = 'AMI2018_it_testing.tsv'\n",
        "tsv_DANKMEMES2020_train = 'dankmemes_task2_train.csv'\n",
        "tsv_DANKMEMES2020_test = 'hate_test.csv'\n",
        "tsv_SENTIPOLC2016_train = 'training_set_sentipolc16.csv'\n",
        "tsv_SENTIPOLC2016_test = 'test_set_sentipolc16_gold2000.csv'\n",
        "tsv_SENTIPOLC2016_train = 'training_set_sentipolc16.csv'\n",
        "tsv_SENTIPOLC2016_test = 'test_set_sentipolc16_gold2000.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TADQKc95RXR_",
        "outputId": "4552ab18-878f-46ab-94ab-29dcc66336c3"
      },
      "source": [
        "%cd tsv_transformed/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files/tsv_transformed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPD3CUjLBu8g",
        "outputId": "4d612dbb-37d8-495d-d53e-8211e9d1c93d"
      },
      "source": [
        "if file_loaded==False:\n",
        "  df_train = pd.read_csv(tsv_haspeede_train, delimiter='\\t', names=('id','sentence','label'))\n",
        "  df_train = df_train[['id']+['label']+['sentence']]\n",
        "  df_test = pd.read_csv(tsv_haspeede_test, delimiter='\\t', names=('id','sentence','label'))\n",
        "  df_test = df_test[['id']+['label']+['sentence']]\n",
        "\n",
        "  #split train dev\n",
        "  train_dataset, dev_dataset = train_test_split(df_train, test_size=0.2, shuffle = True)\n",
        "\n",
        "  #reduction\n",
        "  if number_labeled_examples!=0:\n",
        "    if number_labeled_examples==100:\n",
        "      labeled = train_dataset.sample(n=100)\n",
        "      unlabeled = train_dataset\n",
        "      cond = unlabeled['id'].isin(labeled['id'])\n",
        "      unlabeled.drop(unlabeled[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==200:\n",
        "      labeled = train_dataset.sample(n=200)\n",
        "      unlabeled = train_dataset\n",
        "      cond = unlabeled['id'].isin(labeled['id'])\n",
        "      unlabeled.drop(unlabeled[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==500:\n",
        "      labeled = train_dataset.sample(n=500)\n",
        "      unlabeled = train_dataset\n",
        "      cond = unlabeled['id'].isin(labeled['id'])\n",
        "      unlabeled.drop(unlabeled[cond].index, inplace = True)\n",
        "    \n",
        "    #model with or without gan \n",
        "    if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled['label'] = unlabeled['label'].replace(0,-1)\n",
        "      unlabeled['label'] = unlabeled['label'].replace(1,-1)\n",
        "      train = pd.concat([labeled, unlabeled])\n",
        "      dev = dev_dataset\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train),len(labeled), len(unlabeled)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev)))\n",
        "    else:\n",
        "      print(\"MT-DNN, with reduction dataset\")\n",
        "      train = labeled\n",
        "      dev = dev_dataset\n",
        "      print(\"Size of Train dataset is {} \".format(len(labeled)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev)))\n",
        "\n",
        "  else:\n",
        "    print(\"MT-DNN\")\n",
        "    train = train_dataset\n",
        "    dev = dev_dataset\n",
        "    print(\"Size of Train dataset is {} \".format(len(train)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev)))\n",
        "else:\n",
        "    print(\"no file loaded\")\n",
        "    if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled['label'] = unlabeled['label'].replace(0,-1)\n",
        "      unlabeled['label'] = unlabeled['label'].replace(1,-1)\n",
        "      train = pd.concat([labeled, unlabeled])\n",
        "      dev = dev_dataset\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train),len(labeled), len(unlabeled)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev)))\n",
        "    else:\n",
        "      print(\"MT-DNN, with reduction dataset\")\n",
        "      train = labeled\n",
        "      dev = dev_dataset\n",
        "      print(\"Size of Train dataset is {} \".format(len(labeled)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no file loaded\n",
            "MT-GAN\n",
            "Size of Train dataset is 2400, with 200 labeled and 2200 not labeled \n",
            "Size of Dev dataset is 600 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBa3lP-VPs5c"
      },
      "source": [
        "#train\n",
        "name_train = \"haspeede-TW_train.tsv\"\n",
        "id_train = train.id \n",
        "label_train = train.label\n",
        "sentence_train = train.sentence\n",
        "\n",
        "#dev\n",
        "name_dev = \"haspeede-TW_dev.tsv\"\n",
        "id_dev = dev.id\n",
        "label_dev = dev.label\n",
        "sentence_dev = dev.sentence\n",
        "\n",
        "#test\n",
        "name_test = \"haspeede-TW_test.tsv\"\n",
        "id_test = df_test.id\n",
        "label_test = df_test.label\n",
        "sentence_test = df_test.sentence\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO66Mnx_GE7j",
        "outputId": "8ab4ac95-761e-4b24-a248-af08e5d666c4"
      },
      "source": [
        "if file_loaded2==False:\n",
        "  df_train2 = pd.read_csv(tsv_AMI2018_train, delimiter='\\t')\n",
        "  df_train2 = df_train2[['id']+['misogynous']+['text']]\n",
        "  df_test2 = pd.read_csv(tsv_AMI2018_test, delimiter='\\t')\n",
        "  df_test2 = df_test2[['id']+['misogynous']+['text']]\n",
        "\n",
        "  #split train dev\n",
        "  train_dataset2, dev_dataset2 = train_test_split(df_train2, test_size=0.2, shuffle = True)\n",
        "\n",
        "  #reduction\n",
        "  if number_labeled_examples!=0:\n",
        "    if number_labeled_examples==100:\n",
        "        labeled2 = train_dataset2.sample(n=100)\n",
        "        unlabeled2 = train_dataset2\n",
        "        cond = unlabeled2['id'].isin(labeled2['id'])\n",
        "        unlabeled2.drop(unlabeled2[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==200:\n",
        "      labeled2 = train_dataset2.sample(n=200)\n",
        "      unlabeled2 = train_dataset2\n",
        "      cond = unlabeled2['id'].isin(labeled2['id'])\n",
        "      unlabeled2.drop(unlabeled2[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==500:\n",
        "      labeled2 = train_dataset2.sample(n=500)\n",
        "      unlabeled2 = train_dataset2\n",
        "      cond = unlabeled2['id'].isin(labeled2['id'])\n",
        "      unlabeled2.drop(unlabeled2[cond].index, inplace = True)\n",
        "    \n",
        "    #model with or without gan \n",
        "    if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled2['misogynous'] = unlabeled2['misogynous'].replace(0,-1)\n",
        "      unlabeled2['misogynous'] = unlabeled2['misogynous'].replace(1,-1)\n",
        "      train2 = pd.concat([labeled2, unlabeled2])\n",
        "      dev2 = dev_dataset2\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train2),len(labeled2), len(unlabeled2)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev2)))\n",
        "    else:\n",
        "      print(\"MT-DNN, with reduction dataset\")\n",
        "      train2 = labeled2\n",
        "      dev2 = dev_dataset2\n",
        "      print(\"Size of Train dataset is {} \".format(len(labeled2)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev2)))\n",
        "\n",
        "  else:\n",
        "    print(\"MT-DNN\")\n",
        "    train2 = train_dataset2\n",
        "    dev2 = dev_dataset2\n",
        "    print(\"Size of Train dataset is {} \".format(len(train2)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev2)))\n",
        "else:\n",
        "  print(\"no file loaded\")\n",
        "  if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled2['misogynous'] = unlabeled2['misogynous'].replace(0,-1)\n",
        "      unlabeled2['misogynous'] = unlabeled2['misogynous'].replace(1,-1)\n",
        "      train2 = pd.concat([labeled2, unlabeled2])\n",
        "      dev2 = dev_dataset2\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train2),len(labeled2), len(unlabeled2)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev2)))\n",
        "  else:\n",
        "    print(\"MT-DNN, with reduction dataset\")\n",
        "    train2 = labeled2\n",
        "    dev2 = dev_dataset2\n",
        "    print(\"Size of Train dataset is {} \".format(len(labeled2)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no file loaded\n",
            "MT-GAN\n",
            "Size of Train dataset is 3200, with 200 labeled and 3000 not labeled \n",
            "Size of Dev dataset is 800 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlIIVNBlP0c6"
      },
      "source": [
        "#train\n",
        "name_train = \"AMI2018A_train.tsv\"\n",
        "id_train = train2.id\n",
        "label_train = train2.misogynous\n",
        "sentence_train = train2.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"AMI2018A_dev.tsv\"\n",
        "id_dev = dev2.id\n",
        "label_dev = dev2.misogynous\n",
        "sentence_dev = dev2.text\n",
        "\n",
        "#test\n",
        "name_test = \"AMI2018A_test.tsv\"\n",
        "id_test = df_test2.id\n",
        "label_test = df_test2.misogynous\n",
        "sentence_test = df_test2.text\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvlSGts8FjIB",
        "outputId": "fa84fe22-3619-45dc-be7a-f712c8df42d1"
      },
      "source": [
        "if file_loaded3==False:\n",
        "  df_train3 = pd.read_csv(tsv_AMI2018_train, delimiter='\\t')\n",
        "  df = pd.DataFrame(columns=['id', 'misogyny_category', 'text'])\n",
        "  for ind in df_train3.index:\n",
        "    if df_train3.misogynous[ind]==1:\n",
        "      if df_train3.misogyny_category[ind] == 'stereotype':\n",
        "        df = df.append({'id' : df_train3['id'][ind], 'misogyny_category' : 0, 'text' : df_train3['text'][ind] }, ignore_index=True)\n",
        "      #elif df_train3.misogyny_category[ind] == 'dominance':\n",
        "        #df = df.append({'id' : df_train3['id'][ind], 'misogyny_category' : 1, 'text' : df_train3['text'][ind] }, ignore_index=True)\n",
        "      #elif df_train3.misogyny_category[ind] == 'derailing':\n",
        "        #df = df.append({'id' : df_train3['id'][ind], 'misogyny_category' : 2, 'text' : df_train3['text'][ind] }, ignore_index=True)\n",
        "      elif df_train3.misogyny_category[ind] == 'sexual_harassment':\n",
        "        df = df.append({'id' : df_train3['id'][ind], 'misogyny_category' : 1, 'text' : df_train3['text'][ind] }, ignore_index=True)\n",
        "      elif df_train3.misogyny_category[ind] == 'discredit':\n",
        "        df = df.append({'id' : df_train3['id'][ind], 'misogyny_category' : 2, 'text' : df_train3['text'][ind] }, ignore_index=True)\n",
        "\n",
        "  df_train3 = df\n",
        "\n",
        "  df_test3 = pd.read_csv(tsv_AMI2018_test, delimiter='\\t')\n",
        "  df = pd.DataFrame(columns=['id', 'misogyny_category', 'text'])\n",
        "  for ind in df_test3.index:\n",
        "    if df_test3.misogynous[ind]==1:\n",
        "      if df_test3.misogyny_category[ind] == 'stereotype':\n",
        "        df = df.append({'id' : df_test3['id'][ind], 'misogyny_category' : 0, 'text' : df_test3['text'][ind] }, ignore_index=True)\n",
        "      #elif df_test3.misogyny_category[ind] == 'dominance':\n",
        "        #df = df.append({'id' : df_test3['id'][ind], 'misogyny_category' : 1, 'text' : df_test3['text'][ind] }, ignore_index=True)\n",
        "      #elif df_test3.misogyny_category[ind] == 'derailing':\n",
        "        #df = df.append({'id' : df_test3['id'][ind], 'misogyny_category' : 2, 'text' : df_test3['text'][ind] }, ignore_index=True)\n",
        "      elif df_test3.misogyny_category[ind] == 'sexual_harassment':\n",
        "        df = df.append({'id' : df_test3['id'][ind], 'misogyny_category' : 1, 'text' : df_test3['text'][ind] }, ignore_index=True)\n",
        "      elif df_test3.misogyny_category[ind] == 'discredit':\n",
        "        df = df.append({'id' : df_test3['id'][ind], 'misogyny_category' : 2, 'text' : df_test3['text'][ind] }, ignore_index=True)\n",
        "\n",
        "  df_test3 = df\n",
        "\n",
        "  #split train dev\n",
        "  train_dataset3, dev_dataset3 = train_test_split(df_train3, test_size=0.2, shuffle = True)\n",
        "\n",
        "  #reduction\n",
        "  if number_labeled_examples!=0:\n",
        "    if number_labeled_examples==100:\n",
        "      labeled3 = train_dataset3.sample(n=100)\n",
        "      unlabeled3 = train_dataset3\n",
        "      cond = unlabeled3['id'].isin(labeled3['id'])\n",
        "      unlabeled3.drop(unlabeled3[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==200:\n",
        "      labeled3 = train_dataset3.sample(n=200)\n",
        "      unlabeled3 = train_dataset3\n",
        "      cond = unlabeled3['id'].isin(labeled3['id'])\n",
        "      unlabeled3.drop(unlabeled3[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==500:\n",
        "      labeled3 = train_dataset3.sample(n=500)\n",
        "      unlabeled3 = train_dataset3\n",
        "      cond = unlabeled3['id'].isin(labeled3['id'])\n",
        "      unlabeled3.drop(unlabeled3[cond].index, inplace = True)\n",
        "\n",
        "    #model with or without gan \n",
        "    if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(0,-1)\n",
        "      unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(1,-1)\n",
        "      unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(2,-1)\n",
        "      unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(3,-1)\n",
        "      unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(4,-1)\n",
        "      train3 = pd.concat([labeled3, unlabeled3])\n",
        "      dev3 = dev_dataset3\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train3),len(labeled3), len(unlabeled3)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev3)))\n",
        "    else:\n",
        "      print(\"MT-DNN, with reduction dataset\")\n",
        "      train3 = labeled3\n",
        "      dev3 = dev_dataset3\n",
        "      print(\"Size of Train dataset is {} \".format(len(labeled3)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev3)))\n",
        "\n",
        "  else:\n",
        "    print(\"MT-DNN\")\n",
        "    train3 = train_dataset3\n",
        "    dev3=dev_dataset3\n",
        "    print(\"Size of Train dataset is {} \".format(len(train3)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev3)))\n",
        "else:\n",
        "  print(\"no file loaded\")\n",
        "  if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(0,-1)\n",
        "      unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(1,-1)\n",
        "      unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(2,-1)\n",
        "      unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(3,-1)\n",
        "      unlabeled3['misogyny_category'] = unlabeled3['misogyny_category'].replace(4,-1)\n",
        "      train3 = pd.concat([labeled3, unlabeled3])\n",
        "      dev3 = dev_dataset3\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train3),len(labeled3), len(unlabeled3)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev3)))\n",
        "  else:\n",
        "    print(\"MT-DNN, with reduction dataset\")\n",
        "    train3 = labeled3\n",
        "    dev3 = dev_dataset3\n",
        "    print(\"Size of Train dataset is {} \".format(len(labeled3)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no file loaded\n",
            "MT-GAN\n",
            "Size of Train dataset is 1386, with 200 labeled and 1186 not labeled \n",
            "Size of Dev dataset is 347 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:90: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:91: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:92: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:94: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiqgRuTIP5oM"
      },
      "source": [
        "#train\n",
        "name_train = \"AMI2018B_train.tsv\"\n",
        "id_train = train3.id\n",
        "label_train = train3.misogyny_category\n",
        "sentence_train = train3.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"AMI2018B_dev.tsv\"\n",
        "id_dev = dev3.id\n",
        "label_dev = dev3.misogyny_category\n",
        "sentence_dev = dev3.text\n",
        "\n",
        "#test\n",
        "name_test = \"AMI2018B_test.tsv\"\n",
        "id_test = df_test3.id\n",
        "label_test = df_test3.misogyny_category\n",
        "sentence_test = df_test3.text\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWWeT6G2GvBj",
        "outputId": "7ad6e8a8-4b26-4746-f463-d2b555404553"
      },
      "source": [
        "if file_loaded4==False:\n",
        "  df_train4 = pd.read_csv(tsv_DANKMEMES2020_train, delimiter=',')\n",
        "  df_train4 = df_train4[['File']+['Hate Speech']+['Text']]\n",
        "  df_test4 = pd.read_csv(tsv_DANKMEMES2020_test, delimiter=',')\n",
        "  df_test4 = df_test4[['File']+['Hate Speech']+['Text']]\n",
        "\n",
        "\n",
        "  #split train dev\n",
        "  train_dataset4, dev_dataset4 = train_test_split(df_train4, test_size=0.2, shuffle = True)\n",
        "\n",
        "  #reduction\n",
        "  if number_labeled_examples!=0:\n",
        "\n",
        "    if number_labeled_examples==100:\n",
        "      labeled4 = train_dataset4.sample(n=100)\n",
        "      unlabeled4 = train_dataset4\n",
        "      cond = unlabeled4['File'].isin(labeled4['File'])\n",
        "      unlabeled4.drop(unlabeled4[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==200:\n",
        "      labeled4 = train_dataset4.sample(n=200)\n",
        "      unlabeled4 = train_dataset4\n",
        "      cond = unlabeled4['File'].isin(labeled4['File'])\n",
        "      unlabeled4.drop(unlabeled4[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==500:\n",
        "      labeled4 = train_dataset4.sample(n=500)\n",
        "      unlabeled4 = train_dataset4\n",
        "      cond = unlabeled4['File'].isin(labeled4['File'])\n",
        "      unlabeled4.drop(unlabeled4[cond].index, inplace = True)\n",
        "\n",
        "    #model with or without gan \n",
        "    if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled4['Hate Speech'] = unlabeled4['Hate Speech'].replace(0,-1)\n",
        "      unlabeled4['Hate Speech'] = unlabeled4['Hate Speech'].replace(1,-1)\n",
        "      train4 = pd.concat([labeled4, unlabeled4])\n",
        "      dev4 = dev_dataset4\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train4),len(labeled4), len(unlabeled4)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev4)))\n",
        "    else:\n",
        "      print(\"MT-DNN, with reduction dataset\")\n",
        "      train4 = labeled4\n",
        "      dev4 = dev_dataset4\n",
        "      print(\"Size of Train dataset is {} \".format(len(labeled4)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev4)))\n",
        "\n",
        "\n",
        "  else:\n",
        "    print(\"MT-DNN\")\n",
        "    train4 = train_dataset4\n",
        "    dev4=dev_dataset4\n",
        "    print(\"Size of Train dataset is {} \".format(len(train4)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev4)))\n",
        "else:\n",
        "  print(\"no file loaded\")\n",
        "  if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled4['Hate Speech'] = unlabeled4['Hate Speech'].replace(0,-1)\n",
        "      unlabeled4['Hate Speech'] = unlabeled4['Hate Speech'].replace(1,-1)\n",
        "      train4 = pd.concat([labeled4, unlabeled4])\n",
        "      dev4 = dev_dataset4\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train4),len(labeled4), len(unlabeled4)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev4)))\n",
        "  else:\n",
        "    print(\"MT-DNN, with reduction dataset\")\n",
        "    train4 = labeled4\n",
        "    dev4 = dev_dataset4\n",
        "    print(\"Size of Train dataset is {} \".format(len(labeled4)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev4)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no file loaded\n",
            "MT-GAN\n",
            "Size of Train dataset is 640, with 200 labeled and 440 not labeled \n",
            "Size of Dev dataset is 160 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8YpJPtYP-to"
      },
      "source": [
        "#train\n",
        "name_train = \"DANKMEMES2020_train.tsv\"\n",
        "id_train = train4.File\n",
        "label_train = train4[\"Hate Speech\"]\n",
        "sentence_train = train4.Text\n",
        "\n",
        "#dev\n",
        "name_dev = \"DANKMEMES2020_dev.tsv\"\n",
        "id_dev = dev4.File\n",
        "label_dev = dev4[\"Hate Speech\"]\n",
        "sentence_dev = dev4.Text\n",
        "\n",
        "#test\n",
        "name_test = \"DANKMEMES2020_test.tsv\"\n",
        "id_test = df_test4.File\n",
        "label_test = df_test4[\"Hate Speech\"]\n",
        "sentence_test = df_test4.Text\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iSz4W9OHhJ1",
        "outputId": "6aff61cd-315f-432d-fde5-d8146d0dfdce"
      },
      "source": [
        "if file_loaded5==False:\n",
        "  df_train5 = pd.read_csv(tsv_SENTIPOLC2016_train, delimiter=',')\n",
        "  df_train5 = df_train5[['idtwitter']+['subj']+['text']]\n",
        "\n",
        "  df_test5 = pd.read_csv(tsv_SENTIPOLC2016_test, delimiter=',')\n",
        "  df_test5 = df_test5[['idtwitter']+['subj']+['text']]\n",
        "\n",
        "  for ind in df_train5.index:\n",
        "    if \"\\t\" in df_train5.text[ind]:\n",
        "      df_train5 = df_train5.replace(to_replace='\\t', value='', regex=True)\n",
        "\n",
        "\n",
        "  #split train dev \n",
        "  train_dataset5, dev_dataset5 = train_test_split(df_train5, test_size=0.2, shuffle = True)\n",
        "\n",
        "  if number_labeled_examples!=0:\n",
        "\n",
        "    if number_labeled_examples==100:\n",
        "      labeled5 = train_dataset5.sample(n=100)\n",
        "      unlabeled5 = train_dataset5\n",
        "      cond = unlabeled5['idtwitter'].isin(labeled5['idtwitter'])\n",
        "      unlabeled5.drop(unlabeled5[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==200:\n",
        "      labeled5 = train_dataset5.sample(n=200)\n",
        "      unlabeled5 = train_dataset5\n",
        "      cond = unlabeled5['idtwitter'].isin(labeled5['idtwitter'])\n",
        "      unlabeled5.drop(unlabeled5[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==500:\n",
        "      labeled5 = train_dataset5.sample(n=500)\n",
        "      unlabeled5 = train_dataset5\n",
        "      cond = unlabeled5['idtwitter'].isin(labeled5['idtwitter'])\n",
        "      unlabeled5.drop(unlabeled5[cond].index, inplace = True)\n",
        "    \n",
        "    #model with or without gan \n",
        "    if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled5['subj'] = unlabeled5['subj'].replace(0,-1)\n",
        "      unlabeled5['subj'] = unlabeled5['subj'].replace(1,-1)\n",
        "      train5 = pd.concat([labeled5, unlabeled5])\n",
        "      dev5 = dev_dataset5\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train5),len(labeled5), len(unlabeled5)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev5)))\n",
        "    else:\n",
        "      print(\"MT-DNN, with reduction dataset\")\n",
        "      train5 = labeled5\n",
        "      dev5 = dev_dataset5\n",
        "      print(\"Size of Train dataset is {} \".format(len(labeled5)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev5)))\n",
        "\n",
        "  else:\n",
        "    print(\"MT-DNN\")\n",
        "    train5 = train_dataset5\n",
        "    dev5=dev_dataset5\n",
        "    print(\"Size of Train dataset is {} \".format(len(train5)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev5)))\n",
        "else:\n",
        "  print(\"no file loaded\")\n",
        "  if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled5['subj'] = unlabeled5['subj'].replace(0,-1)\n",
        "      unlabeled5['subj'] = unlabeled5['subj'].replace(1,-1)\n",
        "      train5 = pd.concat([labeled5, unlabeled5])\n",
        "      dev5 = dev_dataset5\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train5),len(labeled5), len(unlabeled5)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev5)))\n",
        "  else:\n",
        "    print(\"MT-DNN, with reduction dataset\")\n",
        "    train5 = labeled5\n",
        "    dev5 = dev_dataset5\n",
        "    print(\"Size of Train dataset is {} \".format(len(labeled5)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "no file loaded\n",
            "MT-GAN\n",
            "Size of Train dataset is 5928, with 200 labeled and 5728 not labeled \n",
            "Size of Dev dataset is 1482 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DCq42dMQC8U"
      },
      "source": [
        "#train\n",
        "name_train = \"SENTIPOLC20161_train.tsv\"\n",
        "id_train = train5.idtwitter\n",
        "label_train = train5.subj\n",
        "sentence_train = train5.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"SENTIPOLC20161_dev.tsv\"\n",
        "id_dev = dev5.idtwitter\n",
        "label_dev = dev5.subj\n",
        "sentence_dev = dev5.text\n",
        "\n",
        "#test\n",
        "name_test = \"SENTIPOLC20161_test.tsv\"\n",
        "id_test = df_test5.idtwitter\n",
        "label_test = df_test5.subj\n",
        "sentence_test = df_test5.text\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU3tV85nICoX",
        "outputId": "63846be6-de8c-41a3-fab1-fc4d6779b29d"
      },
      "source": [
        "if file_loaded6==False:\n",
        "  df_train6 = pd.read_csv(tsv_SENTIPOLC2016_train, delimiter=',')\n",
        "\n",
        "  df = pd.DataFrame(columns=['idtwitter', 'polarity', 'text'])\n",
        "  for ind in df_train6.index:\n",
        "    if df_train6['subj'][ind] == 1:\n",
        "      if df_train6['opos'][ind] == 1 and df_train6['oneg'][ind] == 0:\n",
        "        df = df.append({'idtwitter' : df_train6['idtwitter'][ind], 'polarity' : 0, 'text' : df_train6['text'][ind] }, ignore_index=True)\n",
        "      elif df_train6['opos'][ind] == 0 and df_train6['oneg'][ind] == 1:\n",
        "        df = df.append({'idtwitter' : df_train6['idtwitter'][ind], 'polarity' : 1, 'text' : df_train6['text'][ind] }, ignore_index=True)\n",
        "      elif df_train6['opos'][ind] == 0 and df_train6['oneg'][ind] == 0:\n",
        "        df = df.append({'idtwitter' : df_train6['idtwitter'][ind], 'polarity' : 2, 'text' : df_train6['text'][ind] }, ignore_index=True)\n",
        "    else:\n",
        "      if df_train6['opos'][ind] == 0 and df_train6['oneg'][ind] == 0:\n",
        "        df = df.append({'idtwitter' : df_train6['idtwitter'][ind], 'polarity' : 2, 'text' : df_train6['text'][ind] }, ignore_index=True)\n",
        "\n",
        "  df_train6 = df\n",
        "\n",
        "  for ind in df_train6.index:\n",
        "    if \"\\t\" in df_train6.text[ind]:\n",
        "      df_train6 = df_train6.replace(to_replace='\\t', value='', regex=True)\n",
        "\n",
        "  df_test6 = pd.read_csv(tsv_SENTIPOLC2016_test, delimiter=',')\n",
        "\n",
        "  df = pd.DataFrame(columns=['idtwitter', 'polarity', 'text'])\n",
        "  for ind in df_test6.index:\n",
        "    if df_test6['subj'][ind] == 1:\n",
        "      if df_test6['opos'][ind] == 1 and df_test6['oneg'][ind] == 0:\n",
        "        df = df.append({'idtwitter' : df_test6['idtwitter'][ind], 'polarity' : 0, 'text' : df_test6['text'][ind] }, ignore_index=True)\n",
        "      elif df_test6['opos'][ind] == 0 and df_test6['oneg'][ind] == 1:\n",
        "        df = df.append({'idtwitter' : df_test6['idtwitter'][ind], 'polarity' : 1, 'text' : df_test6['text'][ind] }, ignore_index=True)\n",
        "      elif df_test6['opos'][ind] == 0 and df_test6['oneg'][ind] == 0:\n",
        "        df = df.append({'idtwitter' : df_test6['idtwitter'][ind], 'polarity' : 2, 'text' : df_test6['text'][ind] }, ignore_index=True)\n",
        "    else:\n",
        "      if df_test6['opos'][ind] == 0 and df_test6['oneg'][ind] == 0:\n",
        "        df = df.append({'idtwitter' : df_test6['idtwitter'][ind], 'polarity' : 2, 'text' : df_test6['text'][ind] }, ignore_index=True)\n",
        "\n",
        "  df_test6 = df\n",
        "\n",
        "  #split train dev\n",
        "  train_dataset6, dev_dataset6 = train_test_split(df_train6, test_size=0.2, shuffle = True)\n",
        "\n",
        "  #reduction\n",
        "  if number_labeled_examples!=0:\n",
        "\n",
        "    if number_labeled_examples==100:\n",
        "      labeled6 = train_dataset6.sample(n=100)\n",
        "      unlabeled6 = train_dataset6\n",
        "      cond = unlabeled6['idtwitter'].isin(labeled6['idtwitter'])\n",
        "      unlabeled6.drop(unlabeled6[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==200:\n",
        "      labeled6 = train_dataset6.sample(n=200)\n",
        "      unlabeled6 = train_dataset6\n",
        "      cond = unlabeled6['idtwitter'].isin(labeled6['idtwitter'])\n",
        "      unlabeled6.drop(unlabeled6[cond].index, inplace = True)\n",
        "\n",
        "    elif number_labeled_examples==500:\n",
        "      labeled6 = train_dataset6.sample(n=500)\n",
        "      unlabeled6 = train_dataset6\n",
        "      cond = unlabeled6['idtwitter'].isin(labeled6['idtwitter'])\n",
        "      unlabeled6.drop(unlabeled6[cond].index, inplace = True)\n",
        "    \n",
        "    #model with or without gan \n",
        "    if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled6['polarity'] = unlabeled6['polarity'].replace(0,-1)\n",
        "      unlabeled6['polarity'] = unlabeled6['polarity'].replace(1,-1)\n",
        "      unlabeled6['polarity'] = unlabeled6['polarity'].replace(2,-1)\n",
        "      train6 = pd.concat([labeled6, unlabeled6])\n",
        "      dev6 = dev_dataset6\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train6),len(labeled6), len(unlabeled6)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev6)))\n",
        "    else:\n",
        "      print(\"MT-DNN, with reduction dataset\")\n",
        "      train6 = labeled6\n",
        "      dev6 = dev_dataset6\n",
        "      print(\"Size of Train dataset is {} \".format(len(labeled6)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev6)))\n",
        "\n",
        "  else:\n",
        "    print(\"MT-DNN\")\n",
        "    train6 = train_dataset6\n",
        "    dev6=dev_dataset6\n",
        "    print(\"Size of Train dataset is {} \".format(len(train6)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev6)))\n",
        "else:\n",
        "  print(\"no file loaded\")\n",
        "  if apply_gan == True:\n",
        "      print(\"MT-GAN\")\n",
        "      #dataset unlabeled with label -1\n",
        "      unlabeled6['polarity'] = unlabeled6['polarity'].replace(0,-1)\n",
        "      unlabeled6['polarity'] = unlabeled6['polarity'].replace(1,-1)\n",
        "      unlabeled6['polarity'] = unlabeled6['polarity'].replace(2,-1)\n",
        "      train6 = pd.concat([labeled6, unlabeled6])\n",
        "      dev6 = dev_dataset6\n",
        "      print(\"Size of Train dataset is {}, with {} labeled and {} not labeled \".format(len(train6),len(labeled6), len(unlabeled6)))\n",
        "      print(\"Size of Dev dataset is {} \".format(len(dev6)))\n",
        "  else:\n",
        "    print(\"MT-DNN, with reduction dataset\")\n",
        "    train6 = labeled6\n",
        "    dev6 = dev_dataset6\n",
        "    print(\"Size of Train dataset is {} \".format(len(labeled6)))\n",
        "    print(\"Size of Dev dataset is {} \".format(len(dev6)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no file loaded\n",
            "MT-GAN\n",
            "Size of Train dataset is 5576, with 200 labeled and 5376 not labeled \n",
            "Size of Dev dataset is 1394 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:94: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQHzWVQUQHOT"
      },
      "source": [
        "#train\n",
        "name_train = \"SENTIPOLC20162_train.tsv\"\n",
        "id_train = train6.idtwitter\n",
        "label_train = train6.polarity\n",
        "sentence_train = train6.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"SENTIPOLC20162_dev.tsv\"\n",
        "id_dev = dev6.idtwitter\n",
        "label_dev = dev6.polarity\n",
        "sentence_dev = dev6.text\n",
        "\n",
        "#test\n",
        "name_test = \"SENTIPOLC20162_test.tsv\"\n",
        "id_test = df_test6.idtwitter\n",
        "label_test = df_test6.polarity\n",
        "sentence_test = df_test6.text\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nff9jrOAPLot",
        "outputId": "79c19c00-88c5-4f40-9fb8-76f1eb01b5c2"
      },
      "source": [
        "%cd ..\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n",
            "/content/mttransformer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMfiRst0cP6a"
      },
      "source": [
        "### Balancing for:\n",
        "\n",
        "* MT-DNN, trained on the total dataset of each task\n",
        "* MT-GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNXMhkv2cUq9",
        "outputId": "c80742ec-f32b-4594-b289-4753fa2a0783"
      },
      "source": [
        "if apply_gan== True:\n",
        "  print(\"MT-GAN\")\n",
        "  max_train_un = max(len(unlabeled), len(unlabeled2), len(unlabeled3), len(unlabeled4), len(unlabeled5), len(unlabeled6))\n",
        "  print(max_train_un)\n",
        "else:\n",
        "  print(\"MT-DNN\")\n",
        "  unlabeled=train\n",
        "  unlabeled2=train2\n",
        "  unlabeled3=train3\n",
        "  unlabeled4=train4\n",
        "  unlabeled5=train5\n",
        "  unlabeled6=train6\n",
        "  max_train_un = max(len(unlabeled), len(unlabeled2), len(unlabeled3), len(unlabeled4), len(unlabeled5), len(unlabeled6))\n",
        "  print(max_train_un)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "W6Xzc2BVcfQs",
        "outputId": "47e65754-61c9-46b4-cfa6-fee86e669859"
      },
      "source": [
        "#double dataset\n",
        "\n",
        "df = pd.DataFrame(columns=['id', 'label', 'sentence'])\n",
        "count=0\n",
        "\n",
        "if len(unlabeled)<max_train_un:\n",
        "  for i in range(max_train_un):\n",
        "    if i < len(unlabeled):\n",
        "      df = df.append({'id' : unlabeled.iloc[i, 0], 'label' : unlabeled.iloc[i, 1], 'sentence' : unlabeled.iloc[i, 2] }, ignore_index=True)\n",
        "    else:\n",
        "      if count < len(unlabeled):\n",
        "        df = df.append({'id' : unlabeled.iloc[count, 0], 'label' : unlabeled.iloc[count, 1], 'sentence' : unlabeled.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "      else:\n",
        "        count = 0\n",
        "        df = df.append({'id' : unlabeled.iloc[count, 0], 'label' : unlabeled.iloc[count, 1], 'sentence' : unlabeled.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "\n",
        "  unlabeled = df\n",
        "\n",
        "if apply_gan== True:\n",
        "  train = pd.concat([labeled, unlabeled])\n",
        "else:\n",
        "  train=unlabeled\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(columns=['id', 'misogynous', 'text'])\n",
        "count=0\n",
        "\n",
        "if len(unlabeled2)<max_train_un:\n",
        "  for i in range(max_train_un):\n",
        "    if i < len(unlabeled2):\n",
        "      df = df.append({'id' : unlabeled2.iloc[i, 0], 'misogynous' : unlabeled2.iloc[i, 1], 'text' : unlabeled2.iloc[i, 2] }, ignore_index=True)\n",
        "    else:\n",
        "      if count < len(unlabeled2):\n",
        "        df = df.append({'id' : unlabeled2.iloc[count, 0], 'misogynous' : unlabeled2.iloc[count, 1], 'text' : unlabeled2.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "      else:\n",
        "        count = 0\n",
        "        df = df.append({'id' : unlabeled2.iloc[count, 0], 'misogynous' : unlabeled2.iloc[count, 1], 'text' : unlabeled2.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "\n",
        "  unlabeled2 = df\n",
        "  \n",
        "if apply_gan==True:\n",
        "  train2 = pd.concat([labeled2, unlabeled2])\n",
        "else:\n",
        "  train2=unlabeled2\n",
        "\n",
        "\n",
        "df = pd.DataFrame(columns=['id', 'misogyny_category', 'text'])\n",
        "count=0\n",
        "\n",
        "if len(unlabeled3)<max_train_un:\n",
        "  for i in range(max_train_un):\n",
        "    if i < len(unlabeled3):\n",
        "      df = df.append({'id' : unlabeled3.iloc[i, 0], 'misogyny_category' : unlabeled3.iloc[i, 1], 'text' : unlabeled3.iloc[i, 2] }, ignore_index=True)\n",
        "    else:\n",
        "      if count < len(unlabeled3):\n",
        "        df = df.append({'id' : unlabeled3.iloc[count, 0], 'misogyny_category' : unlabeled3.iloc[count, 1], 'text' : unlabeled3.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "      else:\n",
        "        count = 0\n",
        "        df = df.append({'id' : unlabeled3.iloc[count, 0], 'misogyny_category' : unlabeled3.iloc[count, 1], 'text' : unlabeled3.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "\n",
        "  unlabeled3 = df\n",
        "\n",
        "if apply_gan==True:\n",
        "  train3 = pd.concat([labeled3, unlabeled3])\n",
        "else:\n",
        "  train3=unlabeled3\n",
        "\n",
        "\n",
        "df = pd.DataFrame(columns=['File', 'Hate Speech', 'Text'])\n",
        "count=0\n",
        "\n",
        "if len(unlabeled4)<max_train_un:\n",
        "  for i in range(max_train_un):\n",
        "    if i < len(unlabeled4):\n",
        "      df = df.append({'File' : unlabeled4.iloc[i, 0], 'Hate Speech' : unlabeled4.iloc[i, 1], 'Text' : unlabeled4.iloc[i, 2] }, ignore_index=True)\n",
        "    else:\n",
        "      if count < len(unlabeled4):\n",
        "        df = df.append({'File' : unlabeled4.iloc[count, 0], 'Hate Speech' : unlabeled4.iloc[count, 1], 'Text' : unlabeled4.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "      else:\n",
        "        count = 0\n",
        "        df = df.append({'File' : unlabeled4.iloc[count, 0], 'Hate Speech' : unlabeled4.iloc[count, 1], 'Text' : unlabeled4.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "\n",
        "  unlabeled4 = df\n",
        "\n",
        "if apply_gan==True:\n",
        "  train4 = pd.concat([labeled4, unlabeled4])\n",
        "else:\n",
        "  train4=unlabeled4\n",
        "\n",
        "\n",
        "df = pd.DataFrame(columns=['idtwitter', 'subj', 'text'])\n",
        "count=0\n",
        "\n",
        "if len(unlabeled5)<max_train_un:\n",
        "  for i in range(max_train_un):\n",
        "    if i < len(unlabeled5):\n",
        "      df = df.append({'idtwitter' : unlabeled5.iloc[i, 0], 'subj' : unlabeled5.iloc[i, 1], 'text' : unlabeled5.iloc[i, 2] }, ignore_index=True)\n",
        "    else:\n",
        "      if count < len(unlabeled5):\n",
        "        df = df.append({'idtwitter' : unlabeled5.iloc[count, 0], 'subj' : unlabeled5.iloc[count, 1], 'text' : unlabeled5.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "      else:\n",
        "        count = 0\n",
        "        df = df.append({'idtwitter' : unlabeled5.iloc[count, 0], 'subj' : unlabeled5.iloc[count, 1], 'text' : unlabeled5.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "\n",
        "  unlabeled5 = df\n",
        "\n",
        "if apply_gan==True:\n",
        "  train5 = pd.concat([labeled5, unlabeled5])\n",
        "else:\n",
        "  train5=unlabeled5\n",
        "\n",
        "\n",
        "df = pd.DataFrame(columns=['idtwitter', 'polarity', 'text'])\n",
        "count=0\n",
        "\n",
        "if len(unlabeled6)<max_train_un:\n",
        "  for i in range(max_train_un):\n",
        "    if i < len(unlabeled6):\n",
        "      df = df.append({'idtwitter' : unlabeled6.iloc[i, 0], 'polarity' : unlabeled6.iloc[i, 1], 'text' : unlabeled6.iloc[i, 2] }, ignore_index=True)\n",
        "    else:\n",
        "      if count < len(unlabeled6):\n",
        "        df = df.append({'idtwitter' : unlabeled6.iloc[count, 0], 'polarity' : unlabeled6.iloc[count, 1], 'text' : unlabeled6.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "      else:\n",
        "        count = 0\n",
        "        df = df.append({'idtwitter' : unlabeled6.iloc[count, 0], 'polarity' : unlabeled6.iloc[count, 1], 'text' : unlabeled6.iloc[count, 2] }, ignore_index=True)\n",
        "        count = count+1\n",
        "\n",
        "  unlabeled6 = df\n",
        "\n",
        "if apply_gan==True:\n",
        "  train6 = pd.concat([labeled6, unlabeled6])\n",
        "else:\n",
        "  train6=unlabeled6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-9811bcb7c09c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'File'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0munlabeled4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hate Speech'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0munlabeled4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Text'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0munlabeled4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   7749\u001b[0m             \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7750\u001b[0m             \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7751\u001b[0;31m             \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7752\u001b[0m         )\n\u001b[1;32m   7753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(self, inplace)\u001b[0m\n\u001b[1;32m   5232\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5235\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5236\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5214\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5216\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5203\u001b[0m         \"\"\"\n\u001b[1;32m   5204\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5205\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5207\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5213\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5214\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    986\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   1907\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         merged_blocks = _merge_blocks(\n\u001b[0;32m-> 1909\u001b[0;31m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m         )\n\u001b[1;32m   1911\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1901\u001b[0m     \"\"\"\n\u001b[1;32m   1902\u001b[0m     \u001b[0;31m# sort by _can_consolidate, dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1903\u001b[0;31m     \u001b[0mgkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1904\u001b[0m     \u001b[0mgrouper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_consolidate_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_name_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;31m# provides dtype.name.__get__, documented as returning a \"bit name\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRPI1tMzdopb"
      },
      "source": [
        "%cd tsv_files/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlt7TmOveILw"
      },
      "source": [
        "!mkdir tsv_transformed\n",
        "%cd tsv_transformed/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYiQD4UPd1mT"
      },
      "source": [
        "#train\n",
        "name_train = \"haspeede-TW_train.tsv\"\n",
        "id_train = train.id\n",
        "label_train = train.label\n",
        "sentence_train = train.sentence\n",
        "\n",
        "#dev\n",
        "name_dev = \"haspeede-TW_dev.tsv\"\n",
        "id_dev = dev.id\n",
        "label_dev = dev.label\n",
        "sentence_dev = dev.sentence\n",
        "\n",
        "#test\n",
        "name_test = \"haspeede-TW_test.tsv\"\n",
        "id_test = df_test.id\n",
        "label_test = df_test.label\n",
        "sentence_test = df_test.sentence\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii6Ln30WeLA_"
      },
      "source": [
        "#train\n",
        "name_train = \"AMI2018A_train.tsv\"\n",
        "id_train = train2.id\n",
        "label_train = train2.misogynous\n",
        "sentence_train = train2.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"AMI2018A_dev.tsv\"\n",
        "id_dev = dev2.id\n",
        "label_dev = dev2.misogynous\n",
        "sentence_dev = dev2.text\n",
        "\n",
        "#test\n",
        "name_test = \"AMI2018A_test.tsv\"\n",
        "id_test = df_test2.id\n",
        "label_test = df_test2.misogynous\n",
        "sentence_test = df_test2.text\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlFrjdC1eLi1"
      },
      "source": [
        "#train\n",
        "name_train = \"AMI2018B_train.tsv\"\n",
        "id_train = train3.id\n",
        "label_train = train3.misogyny_category\n",
        "sentence_train = train3.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"AMI2018B_dev.tsv\"\n",
        "id_dev = dev3.id\n",
        "label_dev = dev3.misogyny_category\n",
        "sentence_dev = dev3.text\n",
        "\n",
        "#test\n",
        "name_test = \"AMI2018B_test.tsv\"\n",
        "id_test = df_test3.id\n",
        "label_test = df_test3.misogyny_category\n",
        "sentence_test = df_test3.text\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJPZW8zweNSC"
      },
      "source": [
        "#train\n",
        "name_train = \"DANKMEMES2020_train.tsv\"\n",
        "id_train = train4.File\n",
        "label_train = train4[\"Hate Speech\"]\n",
        "sentence_train = train4.Text\n",
        "\n",
        "#dev\n",
        "name_dev = \"DANKMEMES2020_dev.tsv\"\n",
        "id_dev = dev4.File\n",
        "label_dev = dev4[\"Hate Speech\"]\n",
        "sentence_dev = dev4.Text\n",
        "\n",
        "#test\n",
        "name_test = \"DANKMEMES2020_test.tsv\"\n",
        "id_test = df_test4.File\n",
        "label_test = df_test4[\"Hate Speech\"]\n",
        "sentence_test = df_test4.Text\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whX11VQNeP52"
      },
      "source": [
        "#train\n",
        "name_train = \"SENTIPOLC20161_train.tsv\"\n",
        "id_train = train5.idtwitter\n",
        "label_train = train5.subj\n",
        "sentence_train = train5.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"SENTIPOLC20161_dev.tsv\"\n",
        "id_dev = dev5.idtwitter\n",
        "label_dev = dev5.subj\n",
        "sentence_dev = dev5.text\n",
        "\n",
        "#test\n",
        "name_test = \"SENTIPOLC20161_test.tsv\"\n",
        "id_test = df_test5.idtwitter\n",
        "label_test = df_test5.subj\n",
        "sentence_test = df_test5.text\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV2frMumeRVC"
      },
      "source": [
        "#train\n",
        "name_train = \"SENTIPOLC20162_train.tsv\"\n",
        "id_train = train6.idtwitter\n",
        "label_train = train6.polarity\n",
        "sentence_train = train6.text\n",
        "\n",
        "#dev\n",
        "name_dev = \"SENTIPOLC20162_dev.tsv\"\n",
        "id_dev = dev6.idtwitter\n",
        "label_dev = dev6.polarity\n",
        "sentence_dev = dev6.text\n",
        "\n",
        "#test\n",
        "name_test = \"SENTIPOLC20162_test.tsv\"\n",
        "id_test = df_test6.idtwitter\n",
        "label_test = df_test6.polarity\n",
        "sentence_test = df_test6.text\n",
        "\n",
        "\n",
        "f = open(name_train, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_train,label_train,sentence_train))\n",
        "\n",
        "\n",
        "f = open(name_dev, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_dev,label_dev,sentence_dev))\n",
        "\n",
        "\n",
        "f = open(name_test, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(id_test,label_test,sentence_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7C-yfSLeRxn"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-qbdtFl1Ko1"
      },
      "source": [
        "### MTL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGOCDw0X1T6b",
        "outputId": "ae798c97-2f4e-4a10-a56f-7afaa94cf964"
      },
      "source": [
        "%cd tsv_files/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2akxXmH1XKu",
        "outputId": "c3de42d4-4e36-45a0-8e07-caf29605526f"
      },
      "source": [
        "!mkdir tsv_transformed\n",
        "%cd tsv_transformed/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘tsv_transformed’: File exists\n",
            "/content/mttransformer/tsv_files/tsv_transformed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7znI36Y3k0k"
      },
      "source": [
        "**Onboard your task into training!**\n",
        "1. Add your piece of config into overall config for all tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgMlB4C_12mT"
      },
      "source": [
        "#task_def\n",
        "name_file = \"haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162_task_def.yml\"\n",
        "  \n",
        "f = open(name_file, 'w')\n",
        "\n",
        "with f:\n",
        "\n",
        "    f.write(\"haspeede-TW:\\n\")\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\")\n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 2\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")\n",
        "    f.write(\"AMI2018A:\\n\")\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\")\n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 2\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")\n",
        "    f.write(\"AMI2018B:\\n\")\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\")\n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 3\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")\n",
        "    f.write(\"DANKMEMES2020:\\n\")\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\")\n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 2\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")\n",
        "    f.write(\"SENTIPOLC20161:\\n\")\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\")\n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 2\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")\n",
        "    f.write(\"SENTIPOLC20162:\\n\")\n",
        "    if apply_gan == True:\n",
        "      f.write(\"  data_format: Gan\\n\")\n",
        "    else:\n",
        "      f.write(\"  data_format: PremiseOnly\\n\")\n",
        "    f.write(\"  enable_san: false\\n\")\n",
        "    f.write(\"  metric_meta:\\n\")\n",
        "    f.write(\"  - F1MAC\\n\")\n",
        "    f.write(\"  - ACC\\n\")\n",
        "    f.write(\"  loss: CeCriterion\\n\")\n",
        "    f.write(\"  n_class: 3\\n\")\n",
        "    f.write(\"  task_type: Classification\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDgLJZgm3bg5",
        "outputId": "8e02d4e6-d46f-4aa7-8b56-f5f34cdc3b16"
      },
      "source": [
        "%cd ..\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mttransformer/tsv_files\n",
            "/content/mttransformer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWrp1ofD3ezZ",
        "outputId": "4954d3c3-8d59-4bd9-bdfb-be396e06ec48"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"MT-GAN\")\n",
        "  !python prepro_std.py --gan --apply_balance --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162_task_def.yml\n",
        "else:\n",
        "  print(\"MT-DNN\")\n",
        "  !python prepro_std.py --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir tsv_files/tsv_transformed/ --task_def tsv_files/tsv_transformed/haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162_task_def.yml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MT-GAN\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "2021-07-26 10:05:53.520100: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading english - 1grams ...\n",
            "Reading english - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "07/26/2021 10:06:05 Task haspeede-TW\n",
            "07/26/2021 10:06:05 tsv_files/tsv_transformed/musixmatch_cased/haspeede-TW_train.json\n",
            "labeled\n",
            "200\n",
            "unlabeled\n",
            "2200\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "07/26/2021 10:06:07 tsv_files/tsv_transformed/musixmatch_cased/haspeede-TW_dev.json\n",
            "07/26/2021 10:06:07 tsv_files/tsv_transformed/musixmatch_cased/haspeede-TW_test.json\n",
            "07/26/2021 10:06:08 Task AMI2018A\n",
            "07/26/2021 10:06:08 tsv_files/tsv_transformed/musixmatch_cased/AMI2018A_train.json\n",
            "labeled\n",
            "200\n",
            "unlabeled\n",
            "3000\n",
            "07/26/2021 10:06:10 tsv_files/tsv_transformed/musixmatch_cased/AMI2018A_dev.json\n",
            "07/26/2021 10:06:10 tsv_files/tsv_transformed/musixmatch_cased/AMI2018A_test.json\n",
            "07/26/2021 10:06:10 Task AMI2018B\n",
            "07/26/2021 10:06:10 tsv_files/tsv_transformed/musixmatch_cased/AMI2018B_train.json\n",
            "labeled\n",
            "200\n",
            "unlabeled\n",
            "1186\n",
            "07/26/2021 10:06:11 tsv_files/tsv_transformed/musixmatch_cased/AMI2018B_dev.json\n",
            "07/26/2021 10:06:11 tsv_files/tsv_transformed/musixmatch_cased/AMI2018B_test.json\n",
            "07/26/2021 10:06:11 Task DANKMEMES2020\n",
            "07/26/2021 10:06:11 tsv_files/tsv_transformed/musixmatch_cased/DANKMEMES2020_train.json\n",
            "labeled\n",
            "200\n",
            "unlabeled\n",
            "440\n",
            "07/26/2021 10:06:12 tsv_files/tsv_transformed/musixmatch_cased/DANKMEMES2020_dev.json\n",
            "07/26/2021 10:06:12 tsv_files/tsv_transformed/musixmatch_cased/DANKMEMES2020_test.json\n",
            "07/26/2021 10:06:12 Task SENTIPOLC20161\n",
            "07/26/2021 10:06:12 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20161_train.json\n",
            "labeled\n",
            "200\n",
            "unlabeled\n",
            "5728\n",
            "07/26/2021 10:06:15 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20161_dev.json\n",
            "07/26/2021 10:06:16 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20161_test.json\n",
            "07/26/2021 10:06:17 Task SENTIPOLC20162\n",
            "07/26/2021 10:06:17 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20162_train.json\n",
            "labeled\n",
            "200\n",
            "unlabeled\n",
            "5376\n",
            "07/26/2021 10:06:20 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20162_dev.json\n",
            "07/26/2021 10:06:20 tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20162_test.json\n",
            "\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IBhqIap329V"
      },
      "source": [
        "2. To join your new task with exsting tasks, please append your task and test_set prefix in train.py args : \"--train_datasets EXISTING_TASKS,YOUR_NEW_TASK --test_datasets EXISTING_TASK_TEST_SETS,YOUR_NEW_TASK_SETS\"; if you are looking for single task fine-tuning, please just leave your new task only in the args."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgNUxxYf3fe1",
        "outputId": "1b13d8da-b9f1-4c88-b70b-752aa8827b4b"
      },
      "source": [
        "if apply_gan == True:\n",
        "  print(\"MT-GAN\")\n",
        "  !python train.py --gan --noise_size 100 --epsilon 1e-8 --encoder_type 9 --epochs 25 --task_def tsv_files/tsv_transformed/haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 64 --batch_size_eval 64 --optimizer \"adamW\" --train_datasets haspeede-TW,AMI2018A,AMI2018B,DANKMEMES2020,SENTIPOLC20161,SENTIPOLC20162 --test_datasets haspeede-TW,AMI2018A,AMI2018B,DANKMEMES2020,SENTIPOLC20161,SENTIPOLC20162 --learning_rate \"1e-5\" --multi_gpu_on --grad_accumulation_step 4 #--fp16 --grad_clipping 0 --global_grad_clipping 1\n",
        "else:\n",
        "  print(\"MT-DNN\")\n",
        "  !python train.py --encoder_type 9 --epochs 10 --task_def tsv_files/tsv_transformed/haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --bert_model_type Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 16 --batch_size_eval 16 --optimizer \"adamW\" --train_datasets haspeede-TW,AMI2018A,AMI2018B,DANKMEMES2020,SENTIPOLC20161,SENTIPOLC20162 --test_datasets haspeede-TW,AMI2018A,AMI2018B,DANKMEMES2020,SENTIPOLC20161,SENTIPOLC20162 --learning_rate \"5e-5\" --multi_gpu_on --grad_accumulation_step 4 #--fp16 --grad_clipping 0 --global_grad_clipping 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MT-GAN\n",
            "2021-07-26 10:06:24.256781: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "07/26/2021 10:06:26 Launching the MT-DNN training\n",
            "07/26/2021 10:06:26 Loading tsv_files/tsv_transformed/musixmatch_cased/haspeede-TW_train.json as task 0\n",
            "Loaded 2800 samples out of 2800\n",
            "07/26/2021 10:06:26 Loading tsv_files/tsv_transformed/musixmatch_cased/AMI2018A_train.json as task 1\n",
            "Loaded 3800 samples out of 3800\n",
            "07/26/2021 10:06:26 Loading tsv_files/tsv_transformed/musixmatch_cased/AMI2018B_train.json as task 2\n",
            "Loaded 1586 samples out of 1586\n",
            "07/26/2021 10:06:26 Loading tsv_files/tsv_transformed/musixmatch_cased/DANKMEMES2020_train.json as task 3\n",
            "Loaded 640 samples out of 640\n",
            "07/26/2021 10:06:26 Loading tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20161_train.json as task 4\n",
            "Loaded 6528 samples out of 6528\n",
            "07/26/2021 10:06:26 Loading tsv_files/tsv_transformed/musixmatch_cased/SENTIPOLC20162_train.json as task 5\n",
            "Loaded 6176 samples out of 6176\n",
            "Loaded 600 samples out of 600\n",
            "Loaded 1000 samples out of 1000\n",
            "Loaded 800 samples out of 800\n",
            "Loaded 1000 samples out of 1000\n",
            "Loaded 347 samples out of 347\n",
            "Loaded 446 samples out of 446\n",
            "Loaded 160 samples out of 160\n",
            "Loaded 200 samples out of 200\n",
            "Loaded 1482 samples out of 1482\n",
            "Loaded 2000 samples out of 2000\n",
            "Loaded 1394 samples out of 1394\n",
            "Loaded 1964 samples out of 1964\n",
            "07/26/2021 10:06:27 ####################\n",
            "07/26/2021 10:06:27 {'log_file': 'mt-dnn-train.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'Musixmatch/umberto-commoncrawl-cased-v1', 'data_dir': 'tsv_files/tsv_transformed/musixmatch_cased/', 'data_sort_on': False, 'name': 'farmer', 'task_def': 'tsv_files/tsv_transformed/haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162_task_def.yml', 'train_datasets': ['haspeede-TW', 'AMI2018A', 'AMI2018B', 'DANKMEMES2020', 'SENTIPOLC20161', 'SENTIPOLC20162'], 'test_datasets': ['haspeede-TW', 'AMI2018A', 'AMI2018B', 'DANKMEMES2020', 'SENTIPOLC20161', 'SENTIPOLC20162'], 'glue_format_on': False, 'mkd_opt': 0, 'do_padding': False, 'gan': True, 'num_hidden_layers_d': 1, 'num_hidden_layers_g': 1, 'noise_size': 100, 'epsilon': 1e-08, 'update_bert_opt': 0, 'multi_gpu_on': True, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 10, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 1, 'pooler_actf': 'tanh', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 128, 'init_ratio': 1, 'encoder_type': 9, 'num_hidden_layers': -1, 'bert_model_type': 'Musixmatch/umberto-commoncrawl-cased-v1', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'bin_on': False, 'bin_size': 64, 'bin_grow_ratio': 0.5, 'local_rank': -1, 'world_size': 1, 'master_addr': 'localhost', 'master_port': '6600', 'backend': 'nccl', 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 25, 'batch_size': 64, 'batch_size_eval': 64, 'optimizer': 'adamW', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 1e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'apply_scheduler': False, 'lr_gamma': 0.5, 'scheduler_type': 'ms', 'output_dir': 'checkpoint', 'seed': 2018, 'grad_accumulation_step': 4, 'fp16': False, 'fp16_opt_level': 'O1', 'adv_train': False, 'adv_opt': 0, 'adv_norm_level': 0, 'adv_p_norm': 'inf', 'adv_alpha': 1, 'adv_k': 1, 'adv_step_size': 1e-05, 'adv_noise_var': 1e-05, 'adv_epsilon': 1e-06, 'encode_mode': False, 'debug': False, 'task_def_list': [{'self': '{}', 'label_vocab': 'None', 'n_class': '2', 'data_type': '<DataFormat.Gan: 7>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}, {'self': '{}', 'label_vocab': 'None', 'n_class': '2', 'data_type': '<DataFormat.Gan: 7>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}, {'self': '{}', 'label_vocab': 'None', 'n_class': '3', 'data_type': '<DataFormat.Gan: 7>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}, {'self': '{}', 'label_vocab': 'None', 'n_class': '2', 'data_type': '<DataFormat.Gan: 7>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}, {'self': '{}', 'label_vocab': 'None', 'n_class': '2', 'data_type': '<DataFormat.Gan: 7>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}, {'self': '{}', 'label_vocab': 'None', 'n_class': '3', 'data_type': '<DataFormat.Gan: 7>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}]}\n",
            "07/26/2021 10:06:27 ####################\n",
            "07/26/2021 10:06:27 ############# Gradient Accumulation Info #############\n",
            "07/26/2021 10:06:27 number of step: 8450\n",
            "07/26/2021 10:06:27 number of grad grad_accumulation step: 4\n",
            "07/26/2021 10:06:27 adjusted number of step: 2112\n",
            "07/26/2021 10:06:27 ############# Gradient Accumulation Info #############\n",
            "07/26/2021 10:06:48 \n",
            "############# Model Arch of MT-DNN #############\n",
            "SANBertNetwork(\n",
            "  (dropout_list): ModuleList(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Dropout(p=0.1, inplace=False)\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Dropout(p=0.1, inplace=False)\n",
            "    (5): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (bert): CamembertModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): RobertaPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (scoring_list): ModuleList(\n",
            "    (0): Discriminator(\n",
            "      (input_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (logit): Linear(in_features=768, out_features=3, bias=True)\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "    (1): Discriminator(\n",
            "      (input_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (logit): Linear(in_features=768, out_features=3, bias=True)\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "    (2): Discriminator(\n",
            "      (input_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (logit): Linear(in_features=768, out_features=4, bias=True)\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "    (3): Discriminator(\n",
            "      (input_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (logit): Linear(in_features=768, out_features=3, bias=True)\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "    (4): Discriminator(\n",
            "      (input_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (logit): Linear(in_features=768, out_features=3, bias=True)\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "    (5): Discriminator(\n",
            "      (input_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (logit): Linear(in_features=768, out_features=4, bias=True)\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "07/26/2021 10:06:48 Total number of params: 114180884\n",
            "07/26/2021 10:06:48 At epoch 0\n",
            "07/26/2021 10:06:50 Task [ 4] Train Generator Loss [1.36662] Train Discriminator Loss [0.55887]   remaining [0:07:51]\n",
            "07/26/2021 10:06:51 Task [ 1] Train Generator Loss [1.40758] Train Discriminator Loss [0.55262]   remaining [0:07:51]\n",
            "07/26/2021 10:06:52 Task [ 1] Train Generator Loss [1.39509] Train Discriminator Loss [0.55084]   remaining [0:07:54]\n",
            "07/26/2021 10:06:54 Task [ 4] Train Generator Loss [1.38474] Train Discriminator Loss [0.55012]   remaining [0:07:55]\n",
            "07/26/2021 10:06:55 Task [ 4] Train Generator Loss [1.37500] Train Discriminator Loss [0.54882]   remaining [0:07:56]\n",
            "07/26/2021 10:06:57 Task [ 0] Train Generator Loss [1.40962] Train Discriminator Loss [0.54909]   remaining [0:07:56]\n",
            "07/26/2021 10:06:58 Task [ 4] Train Generator Loss [1.40104] Train Discriminator Loss [0.54710]   remaining [0:07:56]\n",
            "07/26/2021 10:14:29 Evaluation\n",
            "Dev\n",
            "07/26/2021 10:14:33 Task haspeede-TW -- epoch 0 -- Dev F1MAC: 40.060\n",
            "07/26/2021 10:14:33 Task haspeede-TW -- epoch 0 -- Dev ACC: 66.833\n",
            "07/26/2021 10:14:39 Task AMI2018A -- epoch 0 -- Dev F1MAC: 35.065\n",
            "07/26/2021 10:14:39 Task AMI2018A -- epoch 0 -- Dev ACC: 54.000\n",
            "07/26/2021 10:14:41 Task AMI2018B -- epoch 0 -- Dev F1MAC: 17.862\n",
            "07/26/2021 10:14:41 Task AMI2018B -- epoch 0 -- Dev ACC: 36.599\n",
            "07/26/2021 10:14:42 Task DANKMEMES2020 -- epoch 0 -- Dev F1MAC: 32.489\n",
            "07/26/2021 10:14:42 Task DANKMEMES2020 -- epoch 0 -- Dev ACC: 48.125\n",
            "07/26/2021 10:14:53 Task SENTIPOLC20161 -- epoch 0 -- Dev F1MAC: 39.951\n",
            "07/26/2021 10:14:53 Task SENTIPOLC20161 -- epoch 0 -- Dev ACC: 66.532\n",
            "07/26/2021 10:15:03 Task SENTIPOLC20162 -- epoch 0 -- Dev F1MAC: 19.588\n",
            "07/26/2021 10:15:03 Task SENTIPOLC20162 -- epoch 0 -- Dev ACC: 41.607\n",
            "Media F1MAC: 30.836\n",
            "Media ACC: 52.283\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 10:15:03 Evaluation\n",
            "Test\n",
            "07/26/2021 10:15:10 Task haspeede-TW -- epoch 0 -- Test F1MAC: 40.334\n",
            "07/26/2021 10:15:10 Task haspeede-TW -- epoch 0 -- Test ACC: 67.600\n",
            "07/26/2021 10:15:17 Task AMI2018A -- epoch 0 -- Test F1MAC: 32.931\n",
            "07/26/2021 10:15:17 Task AMI2018A -- epoch 0 -- Test ACC: 49.100\n",
            "07/26/2021 10:15:20 Task AMI2018B -- epoch 0 -- Test F1MAC: 12.606\n",
            "07/26/2021 10:15:20 Task AMI2018B -- epoch 0 -- Test ACC: 23.318\n",
            "07/26/2021 10:15:21 Task DANKMEMES2020 -- epoch 0 -- Test F1MAC: 34.426\n",
            "07/26/2021 10:15:21 Task DANKMEMES2020 -- epoch 0 -- Test ACC: 52.500\n",
            "07/26/2021 10:15:36 Task SENTIPOLC20161 -- epoch 0 -- Test F1MAC: 39.486\n",
            "07/26/2021 10:15:36 Task SENTIPOLC20161 -- epoch 0 -- Test ACC: 65.250\n",
            "07/26/2021 10:15:50 Task SENTIPOLC20162 -- epoch 0 -- Test F1MAC: 21.172\n",
            "07/26/2021 10:15:50 Task SENTIPOLC20162 -- epoch 0 -- Test ACC: 46.538\n",
            "Media F1MAC: 30.159\n",
            "Media ACC: 50.718\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 10:15:50 [new test scores at 0 saved.]\n",
            "07/26/2021 10:15:55 At epoch 1\n",
            "07/26/2021 10:23:35 Evaluation\n",
            "Dev\n",
            "07/26/2021 10:23:39 Task haspeede-TW -- epoch 1 -- Dev F1MAC: 40.060\n",
            "07/26/2021 10:23:39 Task haspeede-TW -- epoch 1 -- Dev ACC: 66.833\n",
            "07/26/2021 10:23:45 Task AMI2018A -- epoch 1 -- Dev F1MAC: 53.675\n",
            "07/26/2021 10:23:45 Task AMI2018A -- epoch 1 -- Dev ACC: 61.750\n",
            "07/26/2021 10:23:47 Task AMI2018B -- epoch 1 -- Dev F1MAC: 26.662\n",
            "07/26/2021 10:23:47 Task AMI2018B -- epoch 1 -- Dev ACC: 37.176\n",
            "07/26/2021 10:23:48 Task DANKMEMES2020 -- epoch 1 -- Dev F1MAC: 34.156\n",
            "07/26/2021 10:23:48 Task DANKMEMES2020 -- epoch 1 -- Dev ACC: 51.875\n",
            "07/26/2021 10:23:59 Task SENTIPOLC20161 -- epoch 1 -- Dev F1MAC: 49.535\n",
            "07/26/2021 10:23:59 Task SENTIPOLC20161 -- epoch 1 -- Dev ACC: 68.354\n",
            "07/26/2021 10:24:09 Task SENTIPOLC20162 -- epoch 1 -- Dev F1MAC: 19.588\n",
            "07/26/2021 10:24:09 Task SENTIPOLC20162 -- epoch 1 -- Dev ACC: 41.607\n",
            "Media F1MAC: 37.279\n",
            "Media ACC: 54.599\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 10:24:09 Evaluation\n",
            "Test\n",
            "07/26/2021 10:24:16 Task haspeede-TW -- epoch 1 -- Test F1MAC: 40.334\n",
            "07/26/2021 10:24:16 Task haspeede-TW -- epoch 1 -- Test ACC: 67.600\n",
            "07/26/2021 10:24:23 Task AMI2018A -- epoch 1 -- Test F1MAC: 70.968\n",
            "07/26/2021 10:24:23 Task AMI2018A -- epoch 1 -- Test ACC: 72.200\n",
            "07/26/2021 10:24:26 Task AMI2018B -- epoch 1 -- Test F1MAC: 25.140\n",
            "07/26/2021 10:24:26 Task AMI2018B -- epoch 1 -- Test ACC: 30.269\n",
            "07/26/2021 10:24:28 Task DANKMEMES2020 -- epoch 1 -- Test F1MAC: 32.203\n",
            "07/26/2021 10:24:28 Task DANKMEMES2020 -- epoch 1 -- Test ACC: 47.500\n",
            "07/26/2021 10:24:42 Task SENTIPOLC20161 -- epoch 1 -- Test F1MAC: 42.985\n",
            "07/26/2021 10:24:42 Task SENTIPOLC20161 -- epoch 1 -- Test ACC: 65.650\n",
            "07/26/2021 10:24:56 Task SENTIPOLC20162 -- epoch 1 -- Test F1MAC: 21.172\n",
            "07/26/2021 10:24:56 Task SENTIPOLC20162 -- epoch 1 -- Test ACC: 46.538\n",
            "Media F1MAC: 38.800\n",
            "Media ACC: 54.959\n",
            "07/26/2021 10:24:56 [new test scores at 1 saved.]\n",
            "07/26/2021 10:25:02 At epoch 2\n",
            "07/26/2021 10:32:42 Evaluation\n",
            "Dev\n",
            "07/26/2021 10:32:46 Task haspeede-TW -- epoch 2 -- Dev F1MAC: 40.060\n",
            "07/26/2021 10:32:46 Task haspeede-TW -- epoch 2 -- Dev ACC: 66.833\n",
            "07/26/2021 10:32:52 Task AMI2018A -- epoch 2 -- Dev F1MAC: 63.794\n",
            "07/26/2021 10:32:52 Task AMI2018A -- epoch 2 -- Dev ACC: 64.750\n",
            "07/26/2021 10:32:54 Task AMI2018B -- epoch 2 -- Dev F1MAC: 24.675\n",
            "07/26/2021 10:32:54 Task AMI2018B -- epoch 2 -- Dev ACC: 34.294\n",
            "07/26/2021 10:32:55 Task DANKMEMES2020 -- epoch 2 -- Dev F1MAC: 48.274\n",
            "07/26/2021 10:32:55 Task DANKMEMES2020 -- epoch 2 -- Dev ACC: 53.125\n",
            "07/26/2021 10:33:06 Task SENTIPOLC20161 -- epoch 2 -- Dev F1MAC: 59.797\n",
            "07/26/2021 10:33:06 Task SENTIPOLC20161 -- epoch 2 -- Dev ACC: 67.341\n",
            "07/26/2021 10:33:16 Task SENTIPOLC20162 -- epoch 2 -- Dev F1MAC: 39.870\n",
            "07/26/2021 10:33:16 Task SENTIPOLC20162 -- epoch 2 -- Dev ACC: 47.274\n",
            "Media F1MAC: 46.078\n",
            "Media ACC: 55.603\n",
            "BEST F1MAC EPOCH\n",
            "BEST ACC EPOCH\n",
            "07/26/2021 10:33:16 Evaluation\n",
            "Test\n",
            "07/26/2021 10:33:23 Task haspeede-TW -- epoch 2 -- Test F1MAC: 40.334\n",
            "07/26/2021 10:33:23 Task haspeede-TW -- epoch 2 -- Test ACC: 67.600\n",
            "07/26/2021 10:33:30 Task AMI2018A -- epoch 2 -- Test F1MAC: 70.498\n",
            "07/26/2021 10:33:30 Task AMI2018A -- epoch 2 -- Test ACC: 70.500\n",
            "07/26/2021 10:33:33 Task AMI2018B -- epoch 2 -- Test F1MAC: 29.286\n",
            "07/26/2021 10:33:33 Task AMI2018B -- epoch 2 -- Test ACC: 36.771\n",
            "07/26/2021 10:33:34 Task DANKMEMES2020 -- epoch 2 -- Test F1MAC: 47.638\n",
            "07/26/2021 10:33:34 Task DANKMEMES2020 -- epoch 2 -- Test ACC: 53.000\n",
            "07/26/2021 10:33:49 Task SENTIPOLC20161 -- epoch 2 -- Test F1MAC: 63.283\n",
            "07/26/2021 10:33:49 Task SENTIPOLC20161 -- epoch 2 -- Test ACC: 69.350\n",
            "07/26/2021 10:34:03 Task SENTIPOLC20162 -- epoch 2 -- Test F1MAC: 43.052\n",
            "07/26/2021 10:34:03 Task SENTIPOLC20162 -- epoch 2 -- Test ACC: 52.699\n",
            "Media F1MAC: 49.015\n",
            "Media ACC: 58.320\n",
            "07/26/2021 10:34:03 [new test scores at 2 saved.]\n",
            "07/26/2021 10:34:09 At epoch 3\n",
            "07/26/2021 10:41:50 Evaluation\n",
            "Dev\n",
            "07/26/2021 10:41:54 Task haspeede-TW -- epoch 3 -- Dev F1MAC: 40.224\n",
            "07/26/2021 10:41:54 Task haspeede-TW -- epoch 3 -- Dev ACC: 66.000\n",
            "07/26/2021 10:42:00 Task AMI2018A -- epoch 3 -- Dev F1MAC: 63.584\n",
            "07/26/2021 10:42:00 Task AMI2018A -- epoch 3 -- Dev ACC: 63.625\n",
            "07/26/2021 10:42:02 Task AMI2018B -- epoch 3 -- Dev F1MAC: 28.227\n",
            "07/26/2021 10:42:02 Task AMI2018B -- epoch 3 -- Dev ACC: 34.582\n",
            "07/26/2021 10:42:03 Task DANKMEMES2020 -- epoch 3 -- Dev F1MAC: 34.156\n",
            "07/26/2021 10:42:03 Task DANKMEMES2020 -- epoch 3 -- Dev ACC: 51.875\n",
            "07/26/2021 10:42:14 Task SENTIPOLC20161 -- epoch 3 -- Dev F1MAC: 59.687\n",
            "07/26/2021 10:42:14 Task SENTIPOLC20161 -- epoch 3 -- Dev ACC: 62.551\n",
            "07/26/2021 10:42:24 Task SENTIPOLC20162 -- epoch 3 -- Dev F1MAC: 39.682\n",
            "07/26/2021 10:42:24 Task SENTIPOLC20162 -- epoch 3 -- Dev ACC: 45.768\n",
            "Media F1MAC: 44.260\n",
            "Media ACC: 54.067\n",
            "07/26/2021 10:42:24 Evaluation\n",
            "Test\n",
            "07/26/2021 10:42:31 Task haspeede-TW -- epoch 3 -- Test F1MAC: 40.822\n",
            "07/26/2021 10:42:31 Task haspeede-TW -- epoch 3 -- Test ACC: 66.600\n",
            "07/26/2021 10:42:38 Task AMI2018A -- epoch 3 -- Test F1MAC: 64.963\n",
            "07/26/2021 10:42:38 Task AMI2018A -- epoch 3 -- Test ACC: 65.700\n",
            "07/26/2021 10:42:41 Task AMI2018B -- epoch 3 -- Test F1MAC: 28.705\n",
            "07/26/2021 10:42:41 Task AMI2018B -- epoch 3 -- Test ACC: 40.359\n",
            "07/26/2021 10:42:43 Task DANKMEMES2020 -- epoch 3 -- Test F1MAC: 32.203\n",
            "07/26/2021 10:42:43 Task DANKMEMES2020 -- epoch 3 -- Test ACC: 47.500\n",
            "07/26/2021 10:42:57 Task SENTIPOLC20161 -- epoch 3 -- Test F1MAC: 62.243\n",
            "07/26/2021 10:42:57 Task SENTIPOLC20161 -- epoch 3 -- Test ACC: 64.650\n",
            "07/26/2021 10:43:11 Task SENTIPOLC20162 -- epoch 3 -- Test F1MAC: 42.085\n",
            "07/26/2021 10:43:11 Task SENTIPOLC20162 -- epoch 3 -- Test ACC: 49.644\n",
            "Media F1MAC: 45.170\n",
            "Media ACC: 55.742\n",
            "07/26/2021 10:43:11 [new test scores at 3 saved.]\n",
            "07/26/2021 10:43:16 At epoch 4\n",
            "07/26/2021 10:50:56 Evaluation\n",
            "Dev\n",
            "07/26/2021 10:51:00 Task haspeede-TW -- epoch 4 -- Dev F1MAC: 40.060\n",
            "07/26/2021 10:51:00 Task haspeede-TW -- epoch 4 -- Dev ACC: 66.833\n",
            "07/26/2021 10:51:06 Task AMI2018A -- epoch 4 -- Dev F1MAC: 58.879\n",
            "07/26/2021 10:51:06 Task AMI2018A -- epoch 4 -- Dev ACC: 59.250\n",
            "07/26/2021 10:51:08 Task AMI2018B -- epoch 4 -- Dev F1MAC: 29.438\n",
            "07/26/2021 10:51:08 Task AMI2018B -- epoch 4 -- Dev ACC: 34.870\n",
            "07/26/2021 10:51:09 Task DANKMEMES2020 -- epoch 4 -- Dev F1MAC: 35.229\n",
            "07/26/2021 10:51:09 Task DANKMEMES2020 -- epoch 4 -- Dev ACC: 46.250\n",
            "07/26/2021 10:51:20 Task SENTIPOLC20161 -- epoch 4 -- Dev F1MAC: 49.596\n",
            "07/26/2021 10:51:20 Task SENTIPOLC20161 -- epoch 4 -- Dev ACC: 49.663\n",
            "07/26/2021 10:51:30 Task SENTIPOLC20162 -- epoch 4 -- Dev F1MAC: 37.433\n",
            "07/26/2021 10:51:30 Task SENTIPOLC20162 -- epoch 4 -- Dev ACC: 44.907\n",
            "Media F1MAC: 41.772\n",
            "Media ACC: 50.296\n",
            "07/26/2021 10:51:30 Evaluation\n",
            "Test\n",
            "07/26/2021 10:51:37 Task haspeede-TW -- epoch 4 -- Test F1MAC: 40.334\n",
            "07/26/2021 10:51:37 Task haspeede-TW -- epoch 4 -- Test ACC: 67.600\n",
            "07/26/2021 10:51:44 Task AMI2018A -- epoch 4 -- Test F1MAC: 56.836\n",
            "07/26/2021 10:51:44 Task AMI2018A -- epoch 4 -- Test ACC: 59.100\n",
            "07/26/2021 10:51:47 Task AMI2018B -- epoch 4 -- Test F1MAC: 30.137\n",
            "07/26/2021 10:51:47 Task AMI2018B -- epoch 4 -- Test ACC: 42.825\n",
            "07/26/2021 10:51:49 Task DANKMEMES2020 -- epoch 4 -- Test F1MAC: 38.558\n",
            "07/26/2021 10:51:49 Task DANKMEMES2020 -- epoch 4 -- Test ACC: 51.000\n",
            "07/26/2021 10:52:03 Task SENTIPOLC20161 -- epoch 4 -- Test F1MAC: 50.717\n",
            "07/26/2021 10:52:03 Task SENTIPOLC20161 -- epoch 4 -- Test ACC: 50.900\n",
            "07/26/2021 10:52:17 Task SENTIPOLC20162 -- epoch 4 -- Test F1MAC: 38.529\n",
            "07/26/2021 10:52:17 Task SENTIPOLC20162 -- epoch 4 -- Test ACC: 49.236\n",
            "Media F1MAC: 42.519\n",
            "Media ACC: 53.444\n",
            "07/26/2021 10:52:17 [new test scores at 4 saved.]\n",
            "07/26/2021 10:52:23 At epoch 5\n",
            "07/26/2021 10:59:25 Task [ 2] Train Generator Loss [1.47735] Train Discriminator Loss [0.45899]   remaining [0:00:38]\n",
            "07/26/2021 10:59:26 Task [ 5] Train Generator Loss [1.47796] Train Discriminator Loss [0.45892]   remaining [0:00:36]\n",
            "07/26/2021 10:59:27 Task [ 4] Train Generator Loss [1.48149] Train Discriminator Loss [0.45877]   remaining [0:00:35]\n",
            "07/26/2021 10:59:29 Task [ 4] Train Generator Loss [1.48442] Train Discriminator Loss [0.45863]   remaining [0:00:34]\n",
            "07/26/2021 11:00:02 Evaluation\n",
            "Dev\n",
            "07/26/2021 11:00:07 Task haspeede-TW -- epoch 5 -- Dev F1MAC: 40.060\n",
            "07/26/2021 11:00:07 Task haspeede-TW -- epoch 5 -- Dev ACC: 66.833\n",
            "07/26/2021 11:00:12 Task AMI2018A -- epoch 5 -- Dev F1MAC: 56.103\n",
            "07/26/2021 11:00:12 Task AMI2018A -- epoch 5 -- Dev ACC: 56.375\n",
            "07/26/2021 11:00:15 Task AMI2018B -- epoch 5 -- Dev F1MAC: 27.502\n",
            "07/26/2021 11:00:15 Task AMI2018B -- epoch 5 -- Dev ACC: 34.006\n",
            "07/26/2021 11:00:16 Task DANKMEMES2020 -- epoch 5 -- Dev F1MAC: 44.295\n",
            "07/26/2021 11:00:16 Task DANKMEMES2020 -- epoch 5 -- Dev ACC: 45.000\n",
            "07/26/2021 11:00:26 Task SENTIPOLC20161 -- epoch 5 -- Dev F1MAC: 49.851\n",
            "07/26/2021 11:00:26 Task SENTIPOLC20161 -- epoch 5 -- Dev ACC: 49.865\n",
            "07/26/2021 11:00:36 Task SENTIPOLC20162 -- epoch 5 -- Dev F1MAC: 38.108\n",
            "07/26/2021 11:00:36 Task SENTIPOLC20162 -- epoch 5 -- Dev ACC: 43.615\n",
            "Media F1MAC: 42.653\n",
            "Media ACC: 49.282\n",
            "07/26/2021 11:00:36 Evaluation\n",
            "Test\n",
            "07/26/2021 11:00:43 Task haspeede-TW -- epoch 5 -- Test F1MAC: 40.334\n",
            "07/26/2021 11:00:43 Task haspeede-TW -- epoch 5 -- Test ACC: 67.600\n",
            "07/26/2021 11:00:51 Task AMI2018A -- epoch 5 -- Test F1MAC: 55.823\n",
            "07/26/2021 11:00:51 Task AMI2018A -- epoch 5 -- Test ACC: 58.000\n",
            "07/26/2021 11:00:54 Task AMI2018B -- epoch 5 -- Test F1MAC: 30.643\n",
            "07/26/2021 11:00:54 Task AMI2018B -- epoch 5 -- Test ACC: 40.135\n",
            "07/26/2021 11:00:55 Task DANKMEMES2020 -- epoch 5 -- Test F1MAC: 46.796\n",
            "07/26/2021 11:00:55 Task DANKMEMES2020 -- epoch 5 -- Test ACC: 47.500\n",
            "07/26/2021 11:01:09 Task SENTIPOLC20161 -- epoch 5 -- Test F1MAC: 52.272\n",
            "07/26/2021 11:01:09 Task SENTIPOLC20161 -- epoch 5 -- Test ACC: 52.350\n",
            "07/26/2021 11:01:23 Task SENTIPOLC20162 -- epoch 5 -- Test F1MAC: 38.853\n",
            "07/26/2021 11:01:23 Task SENTIPOLC20162 -- epoch 5 -- Test ACC: 47.709\n",
            "Media F1MAC: 44.120\n",
            "Media ACC: 52.216\n",
            "07/26/2021 11:01:23 [new test scores at 5 saved.]\n",
            "07/26/2021 11:01:29 At epoch 6\n",
            "07/26/2021 11:09:09 Evaluation\n",
            "Dev\n",
            "07/26/2021 11:09:13 Task haspeede-TW -- epoch 6 -- Dev F1MAC: 42.922\n",
            "07/26/2021 11:09:13 Task haspeede-TW -- epoch 6 -- Dev ACC: 55.000\n",
            "07/26/2021 11:09:19 Task AMI2018A -- epoch 6 -- Dev F1MAC: 61.731\n",
            "07/26/2021 11:09:19 Task AMI2018A -- epoch 6 -- Dev ACC: 61.750\n",
            "07/26/2021 11:09:21 Task AMI2018B -- epoch 6 -- Dev F1MAC: 33.545\n",
            "07/26/2021 11:09:21 Task AMI2018B -- epoch 6 -- Dev ACC: 39.193\n",
            "07/26/2021 11:09:22 Task DANKMEMES2020 -- epoch 6 -- Dev F1MAC: 32.489\n",
            "07/26/2021 11:09:22 Task DANKMEMES2020 -- epoch 6 -- Dev ACC: 48.125\n",
            "07/26/2021 11:09:33 Task SENTIPOLC20161 -- epoch 6 -- Dev F1MAC: 54.951\n",
            "07/26/2021 11:09:33 Task SENTIPOLC20161 -- epoch 6 -- Dev ACC: 55.331\n",
            "07/26/2021 11:09:43 Task SENTIPOLC20162 -- epoch 6 -- Dev F1MAC: 43.911\n",
            "07/26/2021 11:09:43 Task SENTIPOLC20162 -- epoch 6 -- Dev ACC: 46.055\n",
            "Media F1MAC: 44.925\n",
            "Media ACC: 50.909\n",
            "07/26/2021 11:09:43 Evaluation\n",
            "Test\n",
            "07/26/2021 11:09:50 Task haspeede-TW -- epoch 6 -- Test F1MAC: 45.065\n",
            "07/26/2021 11:09:50 Task haspeede-TW -- epoch 6 -- Test ACC: 57.200\n",
            "07/26/2021 11:09:57 Task AMI2018A -- epoch 6 -- Test F1MAC: 62.717\n",
            "07/26/2021 11:09:57 Task AMI2018A -- epoch 6 -- Test ACC: 63.300\n",
            "07/26/2021 11:10:00 Task AMI2018B -- epoch 6 -- Test F1MAC: 31.675\n",
            "07/26/2021 11:10:00 Task AMI2018B -- epoch 6 -- Test ACC: 42.377\n",
            "07/26/2021 11:10:02 Task DANKMEMES2020 -- epoch 6 -- Test F1MAC: 34.426\n",
            "07/26/2021 11:10:02 Task DANKMEMES2020 -- epoch 6 -- Test ACC: 52.500\n",
            "07/26/2021 11:10:16 Task SENTIPOLC20161 -- epoch 6 -- Test F1MAC: 57.477\n",
            "07/26/2021 11:10:16 Task SENTIPOLC20161 -- epoch 6 -- Test ACC: 57.950\n",
            "07/26/2021 11:10:30 Task SENTIPOLC20162 -- epoch 6 -- Test F1MAC: 44.821\n",
            "07/26/2021 11:10:30 Task SENTIPOLC20162 -- epoch 6 -- Test ACC: 49.542\n",
            "Media F1MAC: 46.030\n",
            "Media ACC: 53.811\n",
            "07/26/2021 11:10:30 [new test scores at 6 saved.]\n",
            "07/26/2021 11:10:36 At epoch 7\n",
            "07/26/2021 11:18:15 Evaluation\n",
            "Dev\n",
            "07/26/2021 11:18:20 Task haspeede-TW -- epoch 7 -- Dev F1MAC: 49.966\n",
            "07/26/2021 11:18:20 Task haspeede-TW -- epoch 7 -- Dev ACC: 50.833\n",
            "07/26/2021 11:18:25 Task AMI2018A -- epoch 7 -- Dev F1MAC: 57.998\n",
            "07/26/2021 11:18:25 Task AMI2018A -- epoch 7 -- Dev ACC: 58.000\n",
            "07/26/2021 11:18:28 Task AMI2018B -- epoch 7 -- Dev F1MAC: 32.547\n",
            "07/26/2021 11:18:28 Task AMI2018B -- epoch 7 -- Dev ACC: 36.599\n",
            "07/26/2021 11:18:29 Task DANKMEMES2020 -- epoch 7 -- Dev F1MAC: 51.407\n",
            "07/26/2021 11:18:29 Task DANKMEMES2020 -- epoch 7 -- Dev ACC: 52.500\n",
            "07/26/2021 11:18:40 Task SENTIPOLC20161 -- epoch 7 -- Dev F1MAC: 53.183\n",
            "07/26/2021 11:18:40 Task SENTIPOLC20161 -- epoch 7 -- Dev ACC: 53.239\n",
            "07/26/2021 11:18:50 Task SENTIPOLC20162 -- epoch 7 -- Dev F1MAC: 40.582\n",
            "07/26/2021 11:18:50 Task SENTIPOLC20162 -- epoch 7 -- Dev ACC: 47.991\n",
            "Media F1MAC: 47.614\n",
            "Media ACC: 49.861\n",
            "BEST F1MAC EPOCH\n",
            "07/26/2021 11:18:50 Evaluation\n",
            "Test\n",
            "07/26/2021 11:18:57 Task haspeede-TW -- epoch 7 -- Test F1MAC: 48.304\n",
            "07/26/2021 11:18:57 Task haspeede-TW -- epoch 7 -- Test ACC: 49.000\n",
            "07/26/2021 11:19:04 Task AMI2018A -- epoch 7 -- Test F1MAC: 60.244\n",
            "07/26/2021 11:19:04 Task AMI2018A -- epoch 7 -- Test ACC: 61.300\n",
            "07/26/2021 11:19:07 Task AMI2018B -- epoch 7 -- Test F1MAC: 32.435\n",
            "07/26/2021 11:19:07 Task AMI2018B -- epoch 7 -- Test ACC: 41.928\n",
            "07/26/2021 11:19:08 Task DANKMEMES2020 -- epoch 7 -- Test F1MAC: 51.880\n",
            "07/26/2021 11:19:08 Task DANKMEMES2020 -- epoch 7 -- Test ACC: 52.000\n",
            "07/26/2021 11:19:23 Task SENTIPOLC20161 -- epoch 7 -- Test F1MAC: 54.290\n",
            "07/26/2021 11:19:23 Task SENTIPOLC20161 -- epoch 7 -- Test ACC: 54.300\n",
            "07/26/2021 11:19:37 Task SENTIPOLC20162 -- epoch 7 -- Test F1MAC: 42.193\n",
            "07/26/2021 11:19:37 Task SENTIPOLC20162 -- epoch 7 -- Test ACC: 53.208\n",
            "Media F1MAC: 48.224\n",
            "Media ACC: 51.956\n",
            "07/26/2021 11:19:37 [new test scores at 7 saved.]\n",
            "07/26/2021 11:19:43 At epoch 8\n",
            "07/26/2021 11:27:22 Evaluation\n",
            "Dev\n",
            "07/26/2021 11:27:27 Task haspeede-TW -- epoch 8 -- Dev F1MAC: 45.881\n",
            "07/26/2021 11:27:27 Task haspeede-TW -- epoch 8 -- Dev ACC: 57.667\n",
            "07/26/2021 11:27:33 Task AMI2018A -- epoch 8 -- Dev F1MAC: 62.627\n",
            "07/26/2021 11:27:33 Task AMI2018A -- epoch 8 -- Dev ACC: 64.375\n",
            "07/26/2021 11:27:35 Task AMI2018B -- epoch 8 -- Dev F1MAC: 32.903\n",
            "07/26/2021 11:27:35 Task AMI2018B -- epoch 8 -- Dev ACC: 38.040\n",
            "07/26/2021 11:27:36 Task DANKMEMES2020 -- epoch 8 -- Dev F1MAC: 50.297\n",
            "07/26/2021 11:27:36 Task DANKMEMES2020 -- epoch 8 -- Dev ACC: 50.625\n",
            "07/26/2021 11:27:47 Task SENTIPOLC20161 -- epoch 8 -- Dev F1MAC: 60.563\n",
            "07/26/2021 11:27:47 Task SENTIPOLC20161 -- epoch 8 -- Dev ACC: 64.103\n",
            "07/26/2021 11:27:57 Task SENTIPOLC20162 -- epoch 8 -- Dev F1MAC: 41.161\n",
            "07/26/2021 11:27:57 Task SENTIPOLC20162 -- epoch 8 -- Dev ACC: 47.991\n",
            "Media F1MAC: 48.905\n",
            "Media ACC: 53.800\n",
            "BEST F1MAC EPOCH\n",
            "07/26/2021 11:27:57 Evaluation\n",
            "Test\n",
            "07/26/2021 11:28:04 Task haspeede-TW -- epoch 8 -- Test F1MAC: 45.179\n",
            "07/26/2021 11:28:04 Task haspeede-TW -- epoch 8 -- Test ACC: 57.600\n",
            "07/26/2021 11:28:11 Task AMI2018A -- epoch 8 -- Test F1MAC: 71.378\n",
            "07/26/2021 11:28:11 Task AMI2018A -- epoch 8 -- Test ACC: 71.400\n",
            "07/26/2021 11:28:14 Task AMI2018B -- epoch 8 -- Test F1MAC: 31.177\n",
            "07/26/2021 11:28:14 Task AMI2018B -- epoch 8 -- Test ACC: 38.789\n",
            "07/26/2021 11:28:16 Task DANKMEMES2020 -- epoch 8 -- Test F1MAC: 47.000\n",
            "07/26/2021 11:28:16 Task DANKMEMES2020 -- epoch 8 -- Test ACC: 47.000\n",
            "07/26/2021 11:28:30 Task SENTIPOLC20161 -- epoch 8 -- Test F1MAC: 60.947\n",
            "07/26/2021 11:28:30 Task SENTIPOLC20161 -- epoch 8 -- Test ACC: 64.300\n",
            "07/26/2021 11:28:44 Task SENTIPOLC20162 -- epoch 8 -- Test F1MAC: 41.746\n",
            "07/26/2021 11:28:44 Task SENTIPOLC20162 -- epoch 8 -- Test ACC: 51.527\n",
            "Media F1MAC: 49.571\n",
            "Media ACC: 55.103\n",
            "07/26/2021 11:28:44 [new test scores at 8 saved.]\n",
            "07/26/2021 11:28:49 At epoch 9\n",
            "07/26/2021 11:36:28 Evaluation\n",
            "Dev\n",
            "07/26/2021 11:36:33 Task haspeede-TW -- epoch 9 -- Dev F1MAC: 50.742\n",
            "07/26/2021 11:36:33 Task haspeede-TW -- epoch 9 -- Dev ACC: 54.333\n",
            "07/26/2021 11:36:38 Task AMI2018A -- epoch 9 -- Dev F1MAC: 58.286\n",
            "07/26/2021 11:36:38 Task AMI2018A -- epoch 9 -- Dev ACC: 58.375\n",
            "07/26/2021 11:36:41 Task AMI2018B -- epoch 9 -- Dev F1MAC: 30.477\n",
            "07/26/2021 11:36:41 Task AMI2018B -- epoch 9 -- Dev ACC: 35.447\n",
            "07/26/2021 11:36:42 Task DANKMEMES2020 -- epoch 9 -- Dev F1MAC: 53.722\n",
            "07/26/2021 11:36:42 Task DANKMEMES2020 -- epoch 9 -- Dev ACC: 54.375\n",
            "07/26/2021 11:36:52 Task SENTIPOLC20161 -- epoch 9 -- Dev F1MAC: 51.360\n",
            "07/26/2021 11:36:52 Task SENTIPOLC20161 -- epoch 9 -- Dev ACC: 51.484\n",
            "07/26/2021 11:37:02 Task SENTIPOLC20162 -- epoch 9 -- Dev F1MAC: 43.248\n",
            "07/26/2021 11:37:02 Task SENTIPOLC20162 -- epoch 9 -- Dev ACC: 44.476\n",
            "Media F1MAC: 47.973\n",
            "Media ACC: 49.748\n",
            "07/26/2021 11:37:02 Evaluation\n",
            "Test\n",
            "07/26/2021 11:37:10 Task haspeede-TW -- epoch 9 -- Test F1MAC: 48.056\n",
            "07/26/2021 11:37:10 Task haspeede-TW -- epoch 9 -- Test ACC: 52.700\n",
            "07/26/2021 11:37:17 Task AMI2018A -- epoch 9 -- Test F1MAC: 59.619\n",
            "07/26/2021 11:37:17 Task AMI2018A -- epoch 9 -- Test ACC: 61.400\n",
            "07/26/2021 11:37:20 Task AMI2018B -- epoch 9 -- Test F1MAC: 30.113\n",
            "07/26/2021 11:37:20 Task AMI2018B -- epoch 9 -- Test ACC: 41.928\n",
            "07/26/2021 11:37:21 Task DANKMEMES2020 -- epoch 9 -- Test F1MAC: 48.958\n",
            "07/26/2021 11:37:21 Task DANKMEMES2020 -- epoch 9 -- Test ACC: 51.000\n",
            "07/26/2021 11:37:36 Task SENTIPOLC20161 -- epoch 9 -- Test F1MAC: 53.710\n",
            "07/26/2021 11:37:36 Task SENTIPOLC20161 -- epoch 9 -- Test ACC: 53.800\n",
            "07/26/2021 11:37:50 Task SENTIPOLC20162 -- epoch 9 -- Test F1MAC: 42.282\n",
            "07/26/2021 11:37:50 Task SENTIPOLC20162 -- epoch 9 -- Test ACC: 44.603\n",
            "Media F1MAC: 47.123\n",
            "Media ACC: 50.905\n",
            "07/26/2021 11:37:50 [new test scores at 9 saved.]\n",
            "07/26/2021 11:37:56 At epoch 10\n",
            "07/26/2021 11:45:35 Evaluation\n",
            "Dev\n",
            "07/26/2021 11:45:39 Task haspeede-TW -- epoch 10 -- Dev F1MAC: 49.245\n",
            "07/26/2021 11:45:39 Task haspeede-TW -- epoch 10 -- Dev ACC: 52.333\n",
            "07/26/2021 11:45:45 Task AMI2018A -- epoch 10 -- Dev F1MAC: 58.054\n",
            "07/26/2021 11:45:45 Task AMI2018A -- epoch 10 -- Dev ACC: 58.125\n",
            "07/26/2021 11:45:48 Task AMI2018B -- epoch 10 -- Dev F1MAC: 40.326\n",
            "07/26/2021 11:45:48 Task AMI2018B -- epoch 10 -- Dev ACC: 40.634\n",
            "07/26/2021 11:45:49 Task DANKMEMES2020 -- epoch 10 -- Dev F1MAC: 59.487\n",
            "07/26/2021 11:45:49 Task DANKMEMES2020 -- epoch 10 -- Dev ACC: 60.000\n",
            "07/26/2021 11:45:59 Task SENTIPOLC20161 -- epoch 10 -- Dev F1MAC: 54.281\n",
            "07/26/2021 11:45:59 Task SENTIPOLC20161 -- epoch 10 -- Dev ACC: 54.926\n",
            "07/26/2021 11:46:09 Task SENTIPOLC20162 -- epoch 10 -- Dev F1MAC: 43.066\n",
            "07/26/2021 11:46:09 Task SENTIPOLC20162 -- epoch 10 -- Dev ACC: 44.261\n",
            "Media F1MAC: 50.743\n",
            "Media ACC: 51.713\n",
            "BEST F1MAC EPOCH\n",
            "07/26/2021 11:46:09 Evaluation\n",
            "Test\n",
            "07/26/2021 11:46:16 Task haspeede-TW -- epoch 10 -- Test F1MAC: 46.896\n",
            "07/26/2021 11:46:16 Task haspeede-TW -- epoch 10 -- Test ACC: 51.000\n",
            "07/26/2021 11:46:23 Task AMI2018A -- epoch 10 -- Test F1MAC: 61.845\n",
            "07/26/2021 11:46:23 Task AMI2018A -- epoch 10 -- Test ACC: 62.500\n",
            "07/26/2021 11:46:26 Task AMI2018B -- epoch 10 -- Test F1MAC: 37.096\n",
            "07/26/2021 11:46:26 Task AMI2018B -- epoch 10 -- Test ACC: 41.704\n",
            "07/26/2021 11:46:28 Task DANKMEMES2020 -- epoch 10 -- Test F1MAC: 49.583\n",
            "07/26/2021 11:46:28 Task DANKMEMES2020 -- epoch 10 -- Test ACC: 51.500\n",
            "07/26/2021 11:46:42 Task SENTIPOLC20161 -- epoch 10 -- Test F1MAC: 55.866\n",
            "07/26/2021 11:46:42 Task SENTIPOLC20161 -- epoch 10 -- Test ACC: 56.050\n",
            "07/26/2021 11:46:56 Task SENTIPOLC20162 -- epoch 10 -- Test F1MAC: 42.686\n",
            "07/26/2021 11:46:56 Task SENTIPOLC20162 -- epoch 10 -- Test ACC: 44.399\n",
            "Media F1MAC: 48.995\n",
            "Media ACC: 51.192\n",
            "07/26/2021 11:46:56 [new test scores at 10 saved.]\n",
            "07/26/2021 11:47:01 At epoch 11\n",
            "07/26/2021 11:53:25 Task [ 1] Train Generator Loss [3.11159] Train Discriminator Loss [0.35146]   remaining [0:01:16]\n",
            "07/26/2021 11:53:26 Task [ 5] Train Generator Loss [3.11202] Train Discriminator Loss [0.35142]   remaining [0:01:14]\n",
            "07/26/2021 11:53:27 Task [ 0] Train Generator Loss [3.11250] Train Discriminator Loss [0.35140]   remaining [0:01:13]\n",
            "07/26/2021 11:53:29 Task [ 1] Train Generator Loss [3.11347] Train Discriminator Loss [0.35134]   remaining [0:01:12]\n",
            "07/26/2021 11:54:40 Evaluation\n",
            "Dev\n",
            "07/26/2021 11:54:45 Task haspeede-TW -- epoch 11 -- Dev F1MAC: 51.813\n",
            "07/26/2021 11:54:45 Task haspeede-TW -- epoch 11 -- Dev ACC: 54.667\n",
            "07/26/2021 11:54:50 Task AMI2018A -- epoch 11 -- Dev F1MAC: 61.925\n",
            "07/26/2021 11:54:50 Task AMI2018A -- epoch 11 -- Dev ACC: 62.375\n",
            "07/26/2021 11:54:53 Task AMI2018B -- epoch 11 -- Dev F1MAC: 40.885\n",
            "07/26/2021 11:54:53 Task AMI2018B -- epoch 11 -- Dev ACC: 41.499\n",
            "07/26/2021 11:54:54 Task DANKMEMES2020 -- epoch 11 -- Dev F1MAC: 52.902\n",
            "07/26/2021 11:54:54 Task DANKMEMES2020 -- epoch 11 -- Dev ACC: 53.125\n",
            "07/26/2021 11:55:04 Task SENTIPOLC20161 -- epoch 11 -- Dev F1MAC: 55.708\n",
            "07/26/2021 11:55:04 Task SENTIPOLC20161 -- epoch 11 -- Dev ACC: 56.748\n",
            "07/26/2021 11:55:14 Task SENTIPOLC20162 -- epoch 11 -- Dev F1MAC: 44.634\n",
            "07/26/2021 11:55:14 Task SENTIPOLC20162 -- epoch 11 -- Dev ACC: 47.561\n",
            "Media F1MAC: 51.311\n",
            "Media ACC: 52.662\n",
            "BEST F1MAC EPOCH\n",
            "07/26/2021 11:55:14 Evaluation\n",
            "Test\n",
            "07/26/2021 11:55:21 Task haspeede-TW -- epoch 11 -- Test F1MAC: 46.956\n",
            "07/26/2021 11:55:21 Task haspeede-TW -- epoch 11 -- Test ACC: 49.500\n",
            "07/26/2021 11:55:29 Task AMI2018A -- epoch 11 -- Test F1MAC: 66.637\n",
            "07/26/2021 11:55:29 Task AMI2018A -- epoch 11 -- Test ACC: 66.800\n",
            "07/26/2021 11:55:32 Task AMI2018B -- epoch 11 -- Test F1MAC: 38.169\n",
            "07/26/2021 11:55:32 Task AMI2018B -- epoch 11 -- Test ACC: 43.498\n",
            "07/26/2021 11:55:33 Task DANKMEMES2020 -- epoch 11 -- Test F1MAC: 49.495\n",
            "07/26/2021 11:55:33 Task DANKMEMES2020 -- epoch 11 -- Test ACC: 50.000\n",
            "07/26/2021 11:55:47 Task SENTIPOLC20161 -- epoch 11 -- Test F1MAC: 56.732\n",
            "07/26/2021 11:55:47 Task SENTIPOLC20161 -- epoch 11 -- Test ACC: 57.550\n",
            "07/26/2021 11:56:01 Task SENTIPOLC20162 -- epoch 11 -- Test F1MAC: 43.209\n",
            "07/26/2021 11:56:01 Task SENTIPOLC20162 -- epoch 11 -- Test ACC: 47.556\n",
            "Media F1MAC: 50.200\n",
            "Media ACC: 52.484\n",
            "07/26/2021 11:56:01 [new test scores at 11 saved.]\n",
            "07/26/2021 11:56:07 At epoch 12\n",
            "07/26/2021 12:03:47 Evaluation\n",
            "Dev\n",
            "07/26/2021 12:03:51 Task haspeede-TW -- epoch 12 -- Dev F1MAC: 52.536\n",
            "07/26/2021 12:03:51 Task haspeede-TW -- epoch 12 -- Dev ACC: 57.000\n",
            "07/26/2021 12:03:57 Task AMI2018A -- epoch 12 -- Dev F1MAC: 58.745\n",
            "07/26/2021 12:03:57 Task AMI2018A -- epoch 12 -- Dev ACC: 60.375\n",
            "07/26/2021 12:04:00 Task AMI2018B -- epoch 12 -- Dev F1MAC: 36.956\n",
            "07/26/2021 12:04:00 Task AMI2018B -- epoch 12 -- Dev ACC: 42.651\n",
            "07/26/2021 12:04:01 Task DANKMEMES2020 -- epoch 12 -- Dev F1MAC: 53.171\n",
            "07/26/2021 12:04:01 Task DANKMEMES2020 -- epoch 12 -- Dev ACC: 56.875\n",
            "07/26/2021 12:04:11 Task SENTIPOLC20161 -- epoch 12 -- Dev F1MAC: 55.983\n",
            "07/26/2021 12:04:11 Task SENTIPOLC20161 -- epoch 12 -- Dev ACC: 58.300\n",
            "07/26/2021 12:04:21 Task SENTIPOLC20162 -- epoch 12 -- Dev F1MAC: 42.298\n",
            "07/26/2021 12:04:21 Task SENTIPOLC20162 -- epoch 12 -- Dev ACC: 45.624\n",
            "Media F1MAC: 49.948\n",
            "Media ACC: 53.471\n",
            "07/26/2021 12:04:21 Evaluation\n",
            "Test\n",
            "07/26/2021 12:04:28 Task haspeede-TW -- epoch 12 -- Test F1MAC: 50.207\n",
            "07/26/2021 12:04:28 Task haspeede-TW -- epoch 12 -- Test ACC: 54.900\n",
            "07/26/2021 12:04:35 Task AMI2018A -- epoch 12 -- Test F1MAC: 67.098\n",
            "07/26/2021 12:04:35 Task AMI2018A -- epoch 12 -- Test ACC: 67.100\n",
            "07/26/2021 12:04:38 Task AMI2018B -- epoch 12 -- Test F1MAC: 38.393\n",
            "07/26/2021 12:04:38 Task AMI2018B -- epoch 12 -- Test ACC: 44.619\n",
            "07/26/2021 12:04:40 Task DANKMEMES2020 -- epoch 12 -- Test F1MAC: 45.355\n",
            "07/26/2021 12:04:40 Task DANKMEMES2020 -- epoch 12 -- Test ACC: 48.000\n",
            "07/26/2021 12:04:54 Task SENTIPOLC20161 -- epoch 12 -- Test F1MAC: 56.759\n",
            "07/26/2021 12:04:54 Task SENTIPOLC20161 -- epoch 12 -- Test ACC: 58.550\n",
            "07/26/2021 12:05:08 Task SENTIPOLC20162 -- epoch 12 -- Test F1MAC: 42.308\n",
            "07/26/2021 12:05:08 Task SENTIPOLC20162 -- epoch 12 -- Test ACC: 47.862\n",
            "Media F1MAC: 50.020\n",
            "Media ACC: 53.505\n",
            "07/26/2021 12:05:08 [new test scores at 12 saved.]\n",
            "07/26/2021 12:05:14 At epoch 13\n",
            "07/26/2021 12:12:53 Evaluation\n",
            "Dev\n",
            "07/26/2021 12:12:57 Task haspeede-TW -- epoch 13 -- Dev F1MAC: 53.200\n",
            "07/26/2021 12:12:57 Task haspeede-TW -- epoch 13 -- Dev ACC: 55.167\n",
            "07/26/2021 12:13:03 Task AMI2018A -- epoch 13 -- Dev F1MAC: 59.556\n",
            "07/26/2021 12:13:03 Task AMI2018A -- epoch 13 -- Dev ACC: 59.625\n",
            "07/26/2021 12:13:06 Task AMI2018B -- epoch 13 -- Dev F1MAC: 37.979\n",
            "07/26/2021 12:13:06 Task AMI2018B -- epoch 13 -- Dev ACC: 40.922\n",
            "07/26/2021 12:13:07 Task DANKMEMES2020 -- epoch 13 -- Dev F1MAC: 55.808\n",
            "07/26/2021 12:13:07 Task DANKMEMES2020 -- epoch 13 -- Dev ACC: 56.250\n",
            "07/26/2021 12:13:17 Task SENTIPOLC20161 -- epoch 13 -- Dev F1MAC: 53.131\n",
            "07/26/2021 12:13:17 Task SENTIPOLC20161 -- epoch 13 -- Dev ACC: 53.644\n",
            "07/26/2021 12:13:27 Task SENTIPOLC20162 -- epoch 13 -- Dev F1MAC: 44.336\n",
            "07/26/2021 12:13:27 Task SENTIPOLC20162 -- epoch 13 -- Dev ACC: 47.418\n",
            "Media F1MAC: 50.668\n",
            "Media ACC: 52.171\n",
            "07/26/2021 12:13:27 Evaluation\n",
            "Test\n",
            "07/26/2021 12:13:34 Task haspeede-TW -- epoch 13 -- Test F1MAC: 49.312\n",
            "07/26/2021 12:13:34 Task haspeede-TW -- epoch 13 -- Test ACC: 50.900\n",
            "07/26/2021 12:13:41 Task AMI2018A -- epoch 13 -- Test F1MAC: 65.009\n",
            "07/26/2021 12:13:41 Task AMI2018A -- epoch 13 -- Test ACC: 65.600\n",
            "07/26/2021 12:13:45 Task AMI2018B -- epoch 13 -- Test F1MAC: 36.588\n",
            "07/26/2021 12:13:45 Task AMI2018B -- epoch 13 -- Test ACC: 42.825\n",
            "07/26/2021 12:13:46 Task DANKMEMES2020 -- epoch 13 -- Test F1MAC: 52.031\n",
            "07/26/2021 12:13:46 Task DANKMEMES2020 -- epoch 13 -- Test ACC: 53.500\n",
            "07/26/2021 12:14:00 Task SENTIPOLC20161 -- epoch 13 -- Test F1MAC: 53.441\n",
            "07/26/2021 12:14:00 Task SENTIPOLC20161 -- epoch 13 -- Test ACC: 53.750\n",
            "07/26/2021 12:14:14 Task SENTIPOLC20162 -- epoch 13 -- Test F1MAC: 43.500\n",
            "07/26/2021 12:14:14 Task SENTIPOLC20162 -- epoch 13 -- Test ACC: 48.931\n",
            "Media F1MAC: 49.980\n",
            "Media ACC: 52.584\n",
            "07/26/2021 12:14:14 [new test scores at 13 saved.]\n",
            "07/26/2021 12:14:20 At epoch 14\n",
            "07/26/2021 12:22:00 Evaluation\n",
            "Dev\n",
            "07/26/2021 12:22:05 Task haspeede-TW -- epoch 14 -- Dev F1MAC: 51.713\n",
            "07/26/2021 12:22:05 Task haspeede-TW -- epoch 14 -- Dev ACC: 52.333\n",
            "07/26/2021 12:22:10 Task AMI2018A -- epoch 14 -- Dev F1MAC: 59.102\n",
            "07/26/2021 12:22:10 Task AMI2018A -- epoch 14 -- Dev ACC: 59.125\n",
            "07/26/2021 12:22:13 Task AMI2018B -- epoch 14 -- Dev F1MAC: 34.920\n",
            "07/26/2021 12:22:13 Task AMI2018B -- epoch 14 -- Dev ACC: 38.329\n",
            "07/26/2021 12:22:14 Task DANKMEMES2020 -- epoch 14 -- Dev F1MAC: 57.493\n",
            "07/26/2021 12:22:14 Task DANKMEMES2020 -- epoch 14 -- Dev ACC: 57.500\n",
            "07/26/2021 12:22:24 Task SENTIPOLC20161 -- epoch 14 -- Dev F1MAC: 53.397\n",
            "07/26/2021 12:22:24 Task SENTIPOLC20161 -- epoch 14 -- Dev ACC: 53.576\n",
            "07/26/2021 12:22:34 Task SENTIPOLC20162 -- epoch 14 -- Dev F1MAC: 43.103\n",
            "07/26/2021 12:22:34 Task SENTIPOLC20162 -- epoch 14 -- Dev ACC: 47.633\n",
            "Media F1MAC: 49.955\n",
            "Media ACC: 51.416\n",
            "07/26/2021 12:22:34 Evaluation\n",
            "Test\n",
            "07/26/2021 12:22:41 Task haspeede-TW -- epoch 14 -- Test F1MAC: 48.554\n",
            "07/26/2021 12:22:41 Task haspeede-TW -- epoch 14 -- Test ACC: 49.100\n",
            "07/26/2021 12:22:49 Task AMI2018A -- epoch 14 -- Test F1MAC: 62.056\n",
            "07/26/2021 12:22:49 Task AMI2018A -- epoch 14 -- Test ACC: 62.800\n",
            "07/26/2021 12:22:52 Task AMI2018B -- epoch 14 -- Test F1MAC: 35.155\n",
            "07/26/2021 12:22:52 Task AMI2018B -- epoch 14 -- Test ACC: 43.498\n",
            "07/26/2021 12:22:53 Task DANKMEMES2020 -- epoch 14 -- Test F1MAC: 56.254\n",
            "07/26/2021 12:22:53 Task DANKMEMES2020 -- epoch 14 -- Test ACC: 56.500\n",
            "07/26/2021 12:23:07 Task SENTIPOLC20161 -- epoch 14 -- Test F1MAC: 53.845\n",
            "07/26/2021 12:23:07 Task SENTIPOLC20161 -- epoch 14 -- Test ACC: 54.000\n",
            "07/26/2021 12:23:21 Task SENTIPOLC20162 -- epoch 14 -- Test F1MAC: 43.657\n",
            "07/26/2021 12:23:21 Task SENTIPOLC20162 -- epoch 14 -- Test ACC: 50.356\n",
            "Media F1MAC: 49.920\n",
            "Media ACC: 52.709\n",
            "07/26/2021 12:23:21 [new test scores at 14 saved.]\n",
            "07/26/2021 12:23:27 At epoch 15\n",
            "07/26/2021 12:31:06 Evaluation\n",
            "Dev\n",
            "07/26/2021 12:31:11 Task haspeede-TW -- epoch 15 -- Dev F1MAC: 53.553\n",
            "07/26/2021 12:31:11 Task haspeede-TW -- epoch 15 -- Dev ACC: 55.667\n",
            "07/26/2021 12:31:16 Task AMI2018A -- epoch 15 -- Dev F1MAC: 59.052\n",
            "07/26/2021 12:31:16 Task AMI2018A -- epoch 15 -- Dev ACC: 59.375\n",
            "07/26/2021 12:31:19 Task AMI2018B -- epoch 15 -- Dev F1MAC: 35.646\n",
            "07/26/2021 12:31:19 Task AMI2018B -- epoch 15 -- Dev ACC: 39.481\n",
            "07/26/2021 12:31:20 Task DANKMEMES2020 -- epoch 15 -- Dev F1MAC: 51.560\n",
            "07/26/2021 12:31:20 Task DANKMEMES2020 -- epoch 15 -- Dev ACC: 56.875\n",
            "07/26/2021 12:31:31 Task SENTIPOLC20161 -- epoch 15 -- Dev F1MAC: 53.918\n",
            "07/26/2021 12:31:31 Task SENTIPOLC20161 -- epoch 15 -- Dev ACC: 54.791\n",
            "07/26/2021 12:31:40 Task SENTIPOLC20162 -- epoch 15 -- Dev F1MAC: 43.053\n",
            "07/26/2021 12:31:40 Task SENTIPOLC20162 -- epoch 15 -- Dev ACC: 46.915\n",
            "Media F1MAC: 49.464\n",
            "Media ACC: 52.184\n",
            "07/26/2021 12:31:40 Evaluation\n",
            "Test\n",
            "07/26/2021 12:31:48 Task haspeede-TW -- epoch 15 -- Test F1MAC: 50.976\n",
            "07/26/2021 12:31:48 Task haspeede-TW -- epoch 15 -- Test ACC: 52.600\n",
            "07/26/2021 12:31:55 Task AMI2018A -- epoch 15 -- Test F1MAC: 64.890\n",
            "07/26/2021 12:31:55 Task AMI2018A -- epoch 15 -- Test ACC: 65.200\n",
            "07/26/2021 12:31:58 Task AMI2018B -- epoch 15 -- Test F1MAC: 38.134\n",
            "07/26/2021 12:31:58 Task AMI2018B -- epoch 15 -- Test ACC: 43.722\n",
            "07/26/2021 12:31:59 Task DANKMEMES2020 -- epoch 15 -- Test F1MAC: 48.101\n",
            "07/26/2021 12:31:59 Task DANKMEMES2020 -- epoch 15 -- Test ACC: 50.500\n",
            "07/26/2021 12:32:14 Task SENTIPOLC20161 -- epoch 15 -- Test F1MAC: 53.996\n",
            "07/26/2021 12:32:14 Task SENTIPOLC20161 -- epoch 15 -- Test ACC: 55.800\n",
            "07/26/2021 12:32:28 Task SENTIPOLC20162 -- epoch 15 -- Test F1MAC: 40.678\n",
            "07/26/2021 12:32:28 Task SENTIPOLC20162 -- epoch 15 -- Test ACC: 46.640\n",
            "Media F1MAC: 49.463\n",
            "Media ACC: 52.410\n",
            "07/26/2021 12:32:28 [new test scores at 15 saved.]\n",
            "07/26/2021 12:32:33 At epoch 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBOPQSIeOeDI"
      },
      "source": [
        "### Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "407mGwIE4Q1B"
      },
      "source": [
        "**Use model resulting from previous training!**\n",
        "\n",
        "> For upload the best model of training MTL:\n",
        "\n",
        "<pre>--init_checkpoint .../model_0.pt</pre>\n",
        "\n",
        "the model is located in the checkpoint folder\n",
        "\n",
        "> To specify which task to perform finetuning for:\n",
        "\n",
        "<pre>--task 0</pre>\n",
        "\n",
        "0 is the first task!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRfDgU1B--w8",
        "outputId": "60713551-93bb-4d51-edd4-9ce628830871"
      },
      "source": [
        "#finetuning\n",
        "!python finetuning.py --finetuning --task 0 --epochs 5 --string haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162 --task_def tsv_files/tsv_transformed/haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162_task_def.yml --data_dir tsv_files/tsv_transformed/musixmatch_cased/ --init_checkpoint checkpoint/model_0.pt --max_seq_len 128 --batch_size 16 --batch_size_eval 16 --optimizer \"adamW\" --train_datasets haspeede-TW --test_datasets haspeede-TW --learning_rate \"5e-5\" --multi_gpu_on --grad_accumulation_step 4 #--fp16 --grad_clipping 0 --global_grad_clipping 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-28 12:50:16.665257: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "05/28/2021 12:50:18 Launching the MT-DNN training\n",
            "05/28/2021 12:50:18 Loading tsv_files/tsv_transformed/musixmatch_cased/haspeede-TW_train.json as task 0\n",
            "Loaded 2700 samples out of 2700\n",
            "Loaded 600 samples out of 600\n",
            "Loaded 1000 samples out of 1000\n",
            "05/28/2021 12:50:18 ####################\n",
            "05/28/2021 12:50:18 {'log_file': 'mt-dnn-train.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'checkpoint/model_0.pt', 'data_dir': 'tsv_files/tsv_transformed/musixmatch_cased/', 'data_sort_on': False, 'name': 'farmer', 'task_def': 'tsv_files/tsv_transformed/haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162_task_def.yml', 'train_datasets': ['haspeede-TW'], 'test_datasets': ['haspeede-TW'], 'glue_format_on': False, 'mkd_opt': 0, 'do_padding': False, 'finetuning': True, 'string': 'haspeede-TW_AMI2018A_AMI2018B_DANKMEMES2020_SENTIPOLC20161_SENTIPOLC20162', 'task': 0, 'gan': False, 'num_hidden_layers_d': 1, 'num_hidden_layers_g': 1, 'noise_size': 100, 'epsilon': 1e-08, 'update_bert_opt': 0, 'multi_gpu_on': True, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 10, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 1, 'pooler_actf': 'tanh', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 128, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'num_hidden_layers': -1, 'bert_model_type': 'bert-base-uncased', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'bin_on': False, 'bin_size': 64, 'bin_grow_ratio': 0.5, 'local_rank': -1, 'world_size': 1, 'master_addr': 'localhost', 'master_port': '6600', 'backend': 'nccl', 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 5, 'batch_size': 16, 'batch_size_eval': 16, 'optimizer': 'adamW', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'apply_scheduler': False, 'lr_gamma': 0.5, 'scheduler_type': 'ms', 'output_dir': 'checkpoint', 'seed': 2018, 'grad_accumulation_step': 4, 'fp16': False, 'fp16_opt_level': 'O1', 'adv_train': False, 'adv_opt': 0, 'adv_norm_level': 0, 'adv_p_norm': 'inf', 'adv_alpha': 1, 'adv_k': 1, 'adv_step_size': 1e-05, 'adv_noise_var': 1e-05, 'adv_epsilon': 1e-06, 'encode_mode': False, 'debug': False, 'task_def_list': [{'self': '{}', 'label_vocab': 'None', 'n_class': '2', 'data_type': '<DataFormat.Gan: 7>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'split_names': \"['train', 'dev', 'test']\", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': 'None', 'adv_loss': 'None', '__class__': \"<class 'experiments.exp_def.TaskDef'>\"}]}\n",
            "05/28/2021 12:50:18 ####################\n",
            "05/28/2021 12:50:18 ############# Gradient Accumulation Info #############\n",
            "05/28/2021 12:50:18 number of step: 845\n",
            "05/28/2021 12:50:18 number of grad grad_accumulation step: 4\n",
            "05/28/2021 12:50:18 adjusted number of step: 211\n",
            "05/28/2021 12:50:18 ############# Gradient Accumulation Info #############\n",
            "05/28/2021 12:50:29 \n",
            "############# Model Arch of MT-DNN #############\n",
            "SANBertNetwork(\n",
            "  (dropout_list): ModuleList(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (bert): CamembertModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): RobertaPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (scoring_list): ModuleList(\n",
            "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "05/28/2021 12:50:29 Total number of params: 110623490\n",
            "05/28/2021 12:50:29 At epoch 0\n",
            "Traceback (most recent call last):\n",
            "  File \"finetuning.py\", line 558, in <module>\n",
            "    main()\n",
            "  File \"finetuning.py\", line 508, in main\n",
            "    for i, (batch_meta, batch_data) in enumerate(multi_task_train_data):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 475, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
            "    return self.collate_fn(data)\n",
            "  File \"/content/mttransformer/mt_dnn/batcher.py\", line 430, in collate_fn\n",
            "    batch_info, batch_data = self._prepare_model_input(batch, data_type)\n",
            "  File \"/content/mttransformer/mt_dnn/batcher.py\", line 571, in _prepare_model_input\n",
            "    type_ids[i, :select_len] = torch.LongTensor(sample['type_id'][:select_len])\n",
            "KeyError: 'type_id'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}