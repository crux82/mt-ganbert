{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0.BERT-based_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNW3bAVSbR4t1yCRFYsIMrx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crux82/mt-ganbert/blob/main/0_BERT_based_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kROPcbt9WhWe"
      },
      "source": [
        "# BERT-based model in Pytorch\n",
        "This notebook shows how to train a simple model without Multi-task and Generative adversarial learning. We will train a simple transformer, Italian Bert-base model, in particular UmBERTo (https://github.com/musixmatchresearch/umberto), on one six tasks considered in our work, used for the recognition of abusive linguistic behaviors. The task are:\n",
        "\n",
        "1.   HaSpeeDe: Hate Spech Recognition\n",
        "2.   AMI A: Automatic Misogyny Identification (misogyny, not mysogyny)\n",
        "3.   AMI B: Automatic Misogyny Identification (misogyny_category: stereotype, sexual_harassment, discredit)\n",
        "4.   DANKMEMEs: Hate Spech Recognition in MEMEs sentences\n",
        "5.   SENTIPOLC 1: Sentiment Polarity Classification (objective, subjective)\n",
        "6.   SENTIPOLC 2: Sentiment Polarity Classification (polarity: positive, negative, neutral)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XxvrajGueuv"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZs976xHuMtV"
      },
      "source": [
        "#--------------------------------\n",
        "#  Retrieve the github directory\n",
        "#--------------------------------\n",
        "!git clone https://github.com/crux82/mt-ganbert\n",
        "%cd mt-ganbert/mttransformer/\n",
        "\n",
        "#installation of necessary packages\n",
        "!pip install -r requirements.txt\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install ekphrasis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-eirkwdwoee"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LULw5oINt9KW"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9_f70mM4ms0"
      },
      "source": [
        "## Run training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9TaSrUauaQt"
      },
      "source": [
        "For each dataset, with a dedicated script (\"script_tsv.py\"), are created 4 files:\n",
        "\n",
        "1.   taskName_task_def.yml, a config file about the task\n",
        "2.   taskName_train.tsv, file tsv of task train set \n",
        "3.   taskName_test.tsv, file tsv of task test set \n",
        "4.   taskName_dev.tsv, file tsv of task dev set \n",
        "\n",
        "\n",
        "The number of examples of train can consist of:\n",
        "\n",
        "*   All train dataset\n",
        "*   100 examples of oringinal train dataset\n",
        "*   200 examples of oringinal train dataset\n",
        "*   500 examples of oringinal train dataset\n",
        "\n",
        "\n",
        "To access to the .tsv files and config file of each task, based on the cutting of examples of the train set you want to use, these can be the paths:\n",
        "\n",
        "*   data/0/taskName_file\n",
        "*   data/100/no_gan/taskName_file\n",
        "*   data/200/no_gan/taskName_file\n",
        "*   data/500/no_gan/taskName_file\n",
        "\n",
        "\"no_gan\" means that you want to use BERT_based model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfGXddUX2A0U"
      },
      "source": [
        "###**Tokenization and Convert to Json**\n",
        "\n",
        "The training code reads tokenized data in json format, so \"prepro_std.py\" (modified script of work https://github.com/namisan/mt-dnn) is used to do tokenization and convert data of .tsv files into json format.\n",
        "\n",
        "The args used in the script invocation are:\n",
        "\n",
        "* --model: the model used to tokenize input sentences\n",
        "* --root_dir: the folder from which to get the .tsv files\n",
        "* --task_def: the task_def file of the task, which contains useful information for converting to .json files\n",
        "\n",
        "The script is run for single task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktE5q93Y18UV"
      },
      "source": [
        "#edit --root_dir and --task_def depending on the task and train set\n",
        "!python prepro_std.py --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir data/\"0\"/ --task_def data/0/haspeede-TW_task_def.yml  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3RCD0vM2MH4"
      },
      "source": [
        "###**Onboard your task into training!**\n",
        "\n",
        "To run the training is used the script \"train.py\" (modified script of work https://github.com/namisan/mt-dnn).\n",
        "The args used in the script invocation are:\n",
        "\n",
        "\n",
        "*   --encoder_type: it means which transformer is used to encode the sentences. In this case is equals to \"9\", that matches to UmBERTo\n",
        "*   --epochs: number of epochs that you want to use in the training\n",
        "*   --task_def: the task_def file of the task\n",
        "*   --data_dir: the folder from which to get the .json files\n",
        "*   --init_checkpoint: the name of the transformer to be loaded, in this case \"Musixmatch/umberto-commoncrawl-cased-v1\"\n",
        "*   --max_seq_len: the maximum length of a sequence that the BERT model can handle\n",
        "*   --batch_size: the number of training examples in one forward/backward pass\n",
        "*   --batch_size_eval: the batch size used for validation and test\n",
        "*   --optimizer: the name of optimizer that you want to use\n",
        "*   --train_datasets: the name of task without train file extension\n",
        "*   --test_datasets: the name of task without test file extension\n",
        "*   --learning_rate: the learning rate that you want to use\n",
        "\n",
        "The script is run for single task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb4mcYjw2TyA"
      },
      "source": [
        "#edit --task_def, --data_dir, --train_datasets and test_datasets depending on the task and train set\n",
        "!python train.py --encoder_type 9 --epochs 10 --task_def data/0/haspeede-TW_task_def.yml --data_dir data/0/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 16 --batch_size_eval 16 --optimizer \"adamW\" --train_datasets haspeede-TW --test_datasets haspeede-TW --learning_rate \"5e-5\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}