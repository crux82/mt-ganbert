{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.GANBERT_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNpBH/g0ld0grbTjWbLPFKB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crux82/mt-ganbert/blob/main/2_GANBERT_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kROPcbt9WhWe"
      },
      "source": [
        "# GANBERT model in Pytorch\n",
        "This notebook shows how to train the GANBERT model (https://github.com/crux82/ganbert). The model consists of a transformer, Italian Bert-base model, UmBERTo (https://github.com/musixmatchresearch/umberto), and it is trained on one six tasks considered in our work, used for the recognition of abusive linguistic behaviors. The task are:\n",
        "\n",
        "1.   HaSpeeDe: Hate Spech Recognition\n",
        "2.   AMI A: Automatic Misogyny Identification (misogyny, not mysogyny)\n",
        "3.   AMI B: Automatic Misogyny Identification (misogyny_category: stereotype, sexual_harassment, discredit)\n",
        "4.   DANKMEMEs: Hate Spech Recognition in MEMEs sentences\n",
        "5.   SENTIPOLC 1: Sentiment Polarity Classification (objective, subjective)\n",
        "6.   SENTIPOLC 2: Sentiment Polarity Classification (polarity: positive, negative, neutral)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF3rn3TX3UHb"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZs976xHuMtV"
      },
      "source": [
        "#--------------------------------\n",
        "#  Retrieve the github directory\n",
        "#--------------------------------\n",
        "!git clone https://github.com/crux82/mt-ganbert\n",
        "%cd mt-ganbert/mttransformer/\n",
        "\n",
        "#installation of necessary packages\n",
        "!pip install -r requirements.txt\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install ekphrasis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JQH0mFl3aJe"
      },
      "source": [
        "##Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LULw5oINt9KW"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K-vOY1Gasqd"
      },
      "source": [
        "## Run training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9TaSrUauaQt"
      },
      "source": [
        "For each dataset, with a dedicated script (\"script_tsv.py\"), are created 4 files:\n",
        "\n",
        "1.   taskName_task_def.yml, a config file about the task\n",
        "2.   taskName_train.tsv, file tsv of task train set \n",
        "3.   taskName_test.tsv, file tsv of task test set \n",
        "4.   taskName_dev.tsv, file tsv of task dev set \n",
        "\n",
        "\n",
        "The number of examples of train can consist of:\n",
        "\n",
        "*   All train dataset\n",
        "*   100 examples of oringinal train dataset\n",
        "*   200 examples of oringinal train dataset\n",
        "*   500 examples of oringinal train dataset\n",
        "\n",
        "\n",
        "To access to the .tsv files and config file of each task, based on the cutting of examples of the train set you want to use, these can be the paths:\n",
        "\n",
        "*   data/0/taskName_file\n",
        "*   data/100/gan/taskName_file\n",
        "*   data/200/gan/taskName_file\n",
        "*   data/500/gan/taskName_file\n",
        "\n",
        "\"gan\" means that you want to use GANBERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfGXddUX2A0U"
      },
      "source": [
        "###**Tokenization and Convert to Json**\n",
        "\n",
        "The training code reads tokenized data in json format, so \"prepro_std.py\" (modified script of work https://github.com/namisan/mt-dnn) is used to do tokenization and convert data of .tsv files into json format.\n",
        "\n",
        "The args used in the script invocation are:\n",
        "\n",
        "* --gan: it's a flag which means we want to use a model that contains adversarial learning, in this case GANBERT\n",
        "* --apply_balance: it's a flag which means that we want activate the balancing that the GANBERT model performs\n",
        "* --model: the model used to tokenize input sentences\n",
        "* --root_dir: the folder from which to get the .tsv files\n",
        "* --task_def: the task_def file of the task, which contains useful information for converting to .json files\n",
        "\n",
        "The script is run for single task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktE5q93Y18UV"
      },
      "source": [
        "#edit --root_dir and --task_def depending on the task and train set\n",
        "!python prepro_std.py --gan --apply_balance --model Musixmatch/umberto-commoncrawl-cased-v1 --root_dir data/\"100\"/gan/ --task_def data/100/gan/haspeede-TW_task_def.yml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3RCD0vM2MH4"
      },
      "source": [
        "###**Onboard your task into training!**\n",
        "\n",
        "To run the training is used the script \"train.py\" (modified script of work https://github.com/namisan/mt-dnn).\n",
        "The args used in the script invocation are:\n",
        "\n",
        "*   --gan: it's a flag which means we want to use a model that contains adversarial learning, in this case GANBERT\n",
        "*   --num_hidden_layers_g: number of hidden layer of mlp Generator, that in our work is 3\n",
        "*   --num_hidden_layers_d: number of hidden layer of mlp Discriminator, that in our work is 0\n",
        "*   --noise_size: the size of the noise that the vectors in input to the Generator have. In our work is 100 \n",
        "*   --epsilon: epsilon for the training of Generator and Discriminator\n",
        "*   --encoder_type: it means which transformer is used to encode the sentences. In this case is equals to \"9\", that matches to UmBERTo\n",
        "*   --epochs: number of epochs that you want to use in the training\n",
        "*   --task_def: the task_def file of the task\n",
        "*   --data_dir: the folder from which to get the .json files\n",
        "*   --init_checkpoint: the name of the transformer to be loaded, in this case \"Musixmatch/umberto-commoncrawl-cased-v1\"\n",
        "*   --max_seq_len: the maximum length of a sequence that the BERT model can handle\n",
        "*   --batch_size: the number of training examples in one forward/backward pass\n",
        "*   --batch_size_eval: the batch size used for validation and test\n",
        "*   --optimizer: the name of optimizer that you want to use\n",
        "*   --train_datasets: the name of task without train file extension\n",
        "*   --test_datasets: the name of task without test file extension\n",
        "*   --learning_rate: the learning rate that you want to use\n",
        "\n",
        "The script is run for single task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb4mcYjw2TyA"
      },
      "source": [
        "#edit --task_def, --data_dir, --train_datasets and test_datasets  depending on the task and train set\n",
        "!python train.py --gan --num_hidden_layers_g 3 --num_hidden_layers_d 0 --noise_size 100 --epsilon 1e-8 --encoder_type 9 --epochs 25 --task_def data/100/gan/haspeede-TW_task_def.yml --data_dir data/100/gan/musixmatch_cased/ --init_checkpoint Musixmatch/umberto-commoncrawl-cased-v1 --max_seq_len 128 --batch_size 64 --batch_size_eval 64 --optimizer \"adamW\" --train_datasets haspeede-TW --test_datasets haspeede-TW --learning_rate \"1e-5\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}